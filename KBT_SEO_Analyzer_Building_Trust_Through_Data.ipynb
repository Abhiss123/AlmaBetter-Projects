{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abhiss123/AlmaBetter-Projects/blob/main/KBT_SEO_Analyzer_Building_Trust_Through_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name: KBT-SEO Analyzer: Building Trust Through Data**\n",
        "\n",
        "---\n",
        "# **What Is This Project About?**\n",
        "\n",
        "The **KBT-SEO Analyzer: Building Trust Through Data** is a tool designed to help website owners, digital marketers, and SEO professionals analyze the quality of their website content.\n",
        "\n",
        "Its goal is to **evaluate and improve trustworthiness, readability, and SEO performance** using advanced technology. This project focuses on applying **Knowledge-Based Trust (KBT)** principles, which means determining how credible and trustworthy a webpage's content is based on measurable factors like grammar, sentiment, and citations.\n",
        "\n",
        "---\n",
        "\n",
        "### **Purpose of This Project**\n",
        "The primary purpose of this project is to help **website owners** and **content creators** ensure their content is:\n",
        "\n",
        "1. **Trustworthy**:\n",
        "   - It checks if the content has references, is well-written, and avoids misleading or low-quality language.\n",
        "\n",
        "2. **Readable**:\n",
        "   - The tool ensures sentences are clear and easy to understand.\n",
        "   - It flags long and complex sentences, making content more engaging for readers.\n",
        "\n",
        "3. **SEO-Optimized**:\n",
        "   - It evaluates keyword density to ensure the content is neither under-optimized nor overstuffed with keywords.\n",
        "   - The tool helps balance keywords in a way that improves search engine rankings without penalization.\n",
        "\n",
        "4. **Actionable**:\n",
        "   - After analyzing the content, it provides clear suggestions and improvements, helping the user take specific actions to enhance the quality of their webpage.\n",
        "\n",
        "---\n",
        "\n",
        "### **Why Was This Project Created?**\n",
        "\n",
        "1. **Problem It Solves**:\n",
        "   - Website content often fails to rank on search engines because it lacks credibility or contains poorly written language.\n",
        "   - Many webpages overuse keywords (keyword stuffing), which reduces their quality and can lead to penalties from search engines.\n",
        "   - A lack of proper citations or references makes content appear less trustworthy to readers and search engines.\n",
        "\n",
        "2. **How It Helps**:\n",
        "   - The KBT-SEO Analyzer identifies these problems in your content and provides actionable insights to fix them.\n",
        "   - It enhances the trust factor of your content, which is critical for building a loyal audience and ranking higher on search engines.\n",
        "\n",
        "---\n",
        "\n",
        "### **Who Is This Project For?**\n",
        "\n",
        "1. **Website Owners**:\n",
        "   - Ensures that their website content builds trust and ranks higher in search engine results.\n",
        "\n",
        "2. **SEO Professionals**:\n",
        "   - Helps them optimize their clients’ content for both readability and search engine trust.\n",
        "\n",
        "3. **Content Creators**:\n",
        "   - Offers insights into how to improve their writing for clarity, grammar, and sentiment.\n",
        "\n",
        "4. **Digital Marketers**:\n",
        "   - Provides detailed feedback on how content can be aligned with Knowledge-Based Trust principles to engage audiences effectively.\n",
        "\n",
        "---\n",
        "\n",
        "### **What Does This Project Analyze?**\n",
        "Here are the key features of the **KBT-SEO Analyzer**:\n",
        "\n",
        "1. **Sentiment Analysis**:\n",
        "   - Analyzes the tone of the content (positive, negative, or neutral).\n",
        "   - Helps improve the tone to make it more engaging for readers.\n",
        "\n",
        "2. **Keyword Density Analysis**:\n",
        "   - Checks how often specific keywords appear in the content.\n",
        "   - Flags keyword stuffing, ensuring that the content is SEO-friendly.\n",
        "\n",
        "3. **Grammar and Sentence Structure**:\n",
        "   - Identifies sentences that are too long or use passive voice, making content harder to understand.\n",
        "   - Recommends rewriting for better readability.\n",
        "\n",
        "4. **Citation Count**:\n",
        "   - Counts references and citations in the content.\n",
        "   - Flags content that lacks proper citations, helping improve trustworthiness.\n",
        "\n",
        "5. **Suggestions for Improvement**:\n",
        "   - Provides actionable suggestions, such as simplifying sentences, improving tone, or reducing overuse of specific keywords.\n",
        "\n",
        "---\n",
        "\n",
        "### **How Is It Beneficial?**\n",
        "The **KBT-SEO Analyzer** ensures your content is ready for both readers and search engines. Here's how it benefits you:\n",
        "\n",
        "1. **Improves Search Rankings**:\n",
        "   - By optimizing your content for keywords, grammar, and readability, it becomes more likely to rank higher on Google and other search engines.\n",
        "\n",
        "2. **Builds Trust**:\n",
        "   - Content with proper citations, positive tone, and clear language establishes trust with your audience.\n",
        "\n",
        "3. **Enhances User Experience**:\n",
        "   - Simplifies complex sentences and reduces errors, making content enjoyable to read.\n",
        "\n",
        "4. **Saves Time**:\n",
        "   - Instead of manually checking your content, the tool quickly analyzes it and provides suggestions.\n",
        "\n",
        "---\n",
        "\n",
        "### **What Should You Do After Getting This Output?**\n",
        "\n",
        "1. **Review the Issues**:\n",
        "   - Look at the flagged issues in the **Issues** section. For example, check for sentences that are too long or for keywords that are overused.\n",
        "\n",
        "2. **Follow the Suggestions**:\n",
        "   - Use the actionable suggestions to rewrite and improve your content.\n",
        "\n",
        "3. **Optimize Keyword Usage**:\n",
        "   - Ensure keywords are used naturally and avoid overusing them.\n",
        "\n",
        "4. **Check Tone and Sentiment**:\n",
        "   - If the sentiment is flagged as \"neutral\" or \"negative,\" rewrite sections to make them more engaging.\n",
        "\n",
        "5. **Add Citations**:\n",
        "   - If the citation count is low, include proper references to build credibility.\n",
        "\n",
        "6. **Repeat the Process**:\n",
        "   - After making changes, re-run the tool to ensure all issues are resolved.\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary**\n",
        "The **KBT-SEO Analyzer: Building Trust Through Data** is a powerful tool designed to help website owners and content creators optimize their content. By analyzing trustworthiness, readability, and SEO effectiveness, it ensures that your content engages readers and ranks higher on search engines.\n"
      ],
      "metadata": {
        "id": "fiVNeqJxW5AZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# **What is Knowledge-Based Trust (KBT) in SEO?**\n",
        "Knowledge-Based Trust (KBT) is an algorithm developed by Google to measure how trustworthy and accurate the information on a website is. This trust score is based on:\n",
        "- How well the facts presented on the website match publicly available, verified knowledge sources.\n",
        "- Whether the information is free from misleading, false, or incomplete claims.\n",
        "\n",
        "This is important because Google uses KBT to prioritize websites that provide reliable and accurate information when ranking them in search results.\n",
        "\n",
        "---\n",
        "\n",
        "### Use Cases of KBT in SEO:\n",
        "Here are the use cases of KBT in the context of a **website**:\n",
        "1. **Improving Search Rankings**: Websites that present accurate, fact-based information are more likely to rank higher on Google.\n",
        "2. **Building User Trust**: Users trust websites with reliable information, leading to higher engagement and lower bounce rates.\n",
        "3. **Avoiding Penalties**: Misinformation or inaccuracies can lead to lower rankings or penalties by Google.\n",
        "4. **Boosting Brand Credibility**: A website that aligns with Google's trust algorithms strengthens its brand image as a reliable source of information.\n",
        "\n",
        "---\n",
        "\n",
        "### Real-Life Implementations of KBT in SEO for Websites:\n",
        "1. **News Websites**: They use KBT to ensure the facts they present are verified and match reputable sources. For example, Google News prioritizes trustworthy content.\n",
        "2. **E-Commerce Websites**: Product descriptions, reviews, and specifications must be accurate to ensure trustworthiness.\n",
        "3. **Educational Websites**: They cross-check their facts with known knowledge bases (e.g., Wikipedia, research papers) to ensure credibility.\n",
        "4. **Healthcare Websites**: Medical websites ensure their content is fact-checked against reliable medical sources like PubMed or WHO guidelines.\n",
        "\n",
        "---\n",
        "\n",
        "### Data Required by a KBT Model:\n",
        "A KBT model requires input data to analyze and determine the trustworthiness of the content. The data can be provided in two main formats:\n",
        "1. **Website URLs**:  \n",
        "   - The URLs of the web pages are fed into the model.  \n",
        "   - The model crawls these pages to extract the text content for analysis.  \n",
        "   - This approach is ideal for analyzing live content on a website.\n",
        "\n",
        "2. **Structured Data in CSV Format**:  \n",
        "   - This is used when the text content (e.g., page titles, descriptions, and main content) is already exported into a CSV file.  \n",
        "   - Each row in the CSV file represents a web page, with columns for title, content, metadata, etc.\n",
        "\n",
        "---\n",
        "\n",
        "### How Does the KBT Model Process Data?\n",
        "1. **Text Preprocessing**:  \n",
        "   - The text is extracted from URLs or CSV files.  \n",
        "   - The text is cleaned by removing HTML tags, special characters, and redundant formatting.\n",
        "   \n",
        "2. **Fact Checking**:  \n",
        "   - The extracted content is cross-checked against trusted knowledge sources (e.g., Google Knowledge Graph, Wikipedia, medical journals).  \n",
        "   - The model looks for factual inconsistencies or unverifiable claims.\n",
        "\n",
        "3. **Trust Score Calculation**:  \n",
        "   - Based on the accuracy and alignment of the content with known facts, the model assigns a trust score to each page.\n",
        "\n",
        "---\n",
        "\n",
        "### Expected Output of a KBT Model:\n",
        "Here’s what the KBT model provides as output:\n",
        "1. **Trust Scores for Web Pages**:  \n",
        "   - A numerical score (e.g., 0-100) indicating the trustworthiness of each page.\n",
        "2. **Highlighted Issues**:  \n",
        "   - Specific sections of content that may be misleading or unverified.  \n",
        "   - Suggestions for improving factual accuracy.\n",
        "3. **Recommendations**:  \n",
        "   - Tips for aligning content with reliable sources to improve trustworthiness.  \n",
        "   - Identifying missing citations or verifications.\n",
        "4. **Insights on Metadata**:  \n",
        "   - Suggestions for optimizing meta titles, descriptions, and schema markup to align with KBT principles.\n",
        "\n",
        "---\n",
        "\n",
        "### How is This Useful for Optimizing Website Content?\n",
        "1. **Content Review**:  \n",
        "   - Website owners can identify inaccurate or weak content and update it with verified information.\n",
        "2. **Citation Management**:  \n",
        "   - Add proper citations and references to back up claims on the website.\n",
        "3. **Improved Rankings**:  \n",
        "   - Higher trust scores translate into better SEO rankings as Google prioritizes accurate content.\n",
        "4. **User Retention**:  \n",
        "   - Users are more likely to stay on and trust websites with accurate information, leading to better engagement metrics.\n",
        "\n",
        "---\n",
        "\n",
        "### Non-Tech Guide to Implementing KBT in SEO:\n",
        "1. **Data Preparation**:  \n",
        "   - Either provide the URLs of your website or export your content into a structured CSV file.\n",
        "   \n",
        "2. **Running the KBT Model**:  \n",
        "   - Use a tool or script (often in Python) to analyze the data.\n",
        "   - The model will preprocess the content, cross-check with verified knowledge bases, and calculate trust scores.\n",
        "\n",
        "3. **Interpreting the Output**:  \n",
        "   - Look at the trust scores and recommendations provided by the model.  \n",
        "   - Update your website content based on the insights to align with KBT principles.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "tPXVZj56XRaJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JazNklEmh1zj"
      },
      "source": [
        "---\n",
        "# **Part 1: Web Scraping Code**\n",
        "**Purpose**: To fetch, clean, and save raw webpage content from specified URLs for further processing.\n",
        "\n",
        "#### **Steps and Functionality**:\n",
        "1. **`fetch_content(url)`**:  \n",
        "   - Fetches raw HTML content from a webpage using the given URL.\n",
        "   - **Why**: This function retrieves the initial webpage data which is essential for analysis.\n",
        "\n",
        "2. **`extract_text_from_html(html_content)`**:  \n",
        "   - Cleans the raw HTML content to extract readable text while removing unwanted elements like scripts and styles.\n",
        "   - **Why**: Ensures only relevant information is passed to the next steps.\n",
        "\n",
        "3. **`scrape_webpages(urls)`**:  \n",
        "   - Iterates through a list of URLs, fetches the content, and cleans it using the above functions.\n",
        "   - **Why**: Gathers all webpage data into a structured format for later use.\n",
        "\n",
        "4. **`save_to_csv(data, filename)`**:  \n",
        "   - Saves the scraped data (URL and cleaned text) into a CSV file.\n",
        "   - **Why**: Provides a structured file format for further processing.\n",
        "\n",
        "5. **`preview_data(data)`**:  \n",
        "   - Displays a preview of the scraped data to verify its accuracy.\n",
        "   - **Why**: Ensures that the scraped content is accurate before moving to the next step.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJi4felddmYy",
        "outputId": "4afbb142-a596-49c3-f792-6da20cbef559"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping URL: https://thatware.co/software-development-services/\n",
            "Scraping URL: https://thatware.co/business-intelligence-services/\n",
            "Scraping URL: https://thatware.co/competitor-keyword-analysis/\n",
            "Data successfully saved to webpage_content.csv.\n",
            "\n",
            "Preview of Scraped Data:\n",
            "\n",
            "                                                 URL  \\\n",
            "0  https://thatware.co/software-development-servi...   \n",
            "1  https://thatware.co/business-intelligence-serv...   \n",
            "2   https://thatware.co/competitor-keyword-analysis/   \n",
            "\n",
            "                                             Content  \n",
            "0  Custom Software Development Services - Softwar...  \n",
            "1  Business Intelligence Services - Competitive A...  \n",
            "2  SEO Competitor Keyword Analysis - Competitor R...  \n"
          ]
        }
      ],
      "source": [
        "# Importing required libraries\n",
        "import requests  # To send HTTP requests and fetch webpage content\n",
        "from bs4 import BeautifulSoup  # To parse HTML and extract readable text\n",
        "import csv  # To save the scraped data in a CSV file\n",
        "import pandas as pd  # To display a preview of the data\n",
        "\n",
        "# List of URLs to scrape (provided by the user)\n",
        "urls = [\n",
        "    'https://thatware.co/software-development-services/',\n",
        "    'https://thatware.co/business-intelligence-services/',\n",
        "    'https://thatware.co/competitor-keyword-analysis/'\n",
        "]\n",
        "\n",
        "# Step 1: Function to fetch webpage content from a given URL\n",
        "def fetch_content(url):\n",
        "    \"\"\"\n",
        "    Fetch HTML content from a webpage.\n",
        "    - Purpose: This function sends an HTTP request to the given URL to fetch the raw HTML content of the page.\n",
        "    - Why: Without this step, we wouldn't have any data to process or analyze.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)  # Sending a GET request with a 10-second timeout\n",
        "        response.raise_for_status()  # Ensures the request was successful; raises an error otherwise\n",
        "        return response.text  # Return the raw HTML content of the page\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        # Print an error message if the request fails\n",
        "        print(f\"Error fetching URL {url}: {e}\")\n",
        "        return None  # Return None so the process can continue even if one URL fails\n",
        "\n",
        "# Step 2: Function to extract meaningful text from HTML content\n",
        "def extract_text_from_html(html_content):\n",
        "    \"\"\"\n",
        "    Extract visible text from HTML content.\n",
        "    - Purpose: This function removes unnecessary elements like scripts, styles, and hidden text from the HTML.\n",
        "    - Why: It ensures that only the main content of the webpage is extracted for analysis.\n",
        "    \"\"\"\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')  # Parse the HTML content using BeautifulSoup\n",
        "    # Extract visible text by removing unnecessary elements and combining all visible text\n",
        "    return soup.get_text(separator=' ', strip=True)\n",
        "\n",
        "# Step 3: Function to scrape all URLs and store their content\n",
        "def scrape_webpages(urls):\n",
        "    \"\"\"\n",
        "    Scrape webpage content for multiple URLs.\n",
        "    - Purpose: This function loops through each URL, fetches its content, and cleans it.\n",
        "    - Why: It structures the process of collecting and organizing webpage data for easy analysis later.\n",
        "    \"\"\"\n",
        "    webpage_data = []  # List to store scraped data for each URL\n",
        "    for url in urls:\n",
        "        print(f\"Scraping URL: {url}\")  # Notify the user of the current URL being processed\n",
        "        html_content = fetch_content(url)  # Step 1: Fetch HTML content\n",
        "        if html_content:  # Ensure we have valid content before proceeding\n",
        "            text_content = extract_text_from_html(html_content)  # Step 2: Extract clean text\n",
        "            # Store the URL and cleaned content in a dictionary\n",
        "            webpage_data.append({'URL': url, 'Content': text_content})\n",
        "    return webpage_data  # Return the scraped data as a list of dictionaries\n",
        "\n",
        "# Step 4: Save the scraped data to a CSV file\n",
        "def save_to_csv(data, filename='webpage_content.csv'):\n",
        "    \"\"\"\n",
        "    Save scraped data to a CSV file.\n",
        "    - Purpose: This function saves the collected webpage data into a structured format (CSV).\n",
        "    - Why: The CSV format is easy to open, share, and analyze using tools like Excel or Python.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Open the CSV file in write mode with UTF-8 encoding to handle special characters\n",
        "        with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
        "            # Create a CSV writer object and define the columns\n",
        "            writer = csv.DictWriter(file, fieldnames=['URL', 'Content'])\n",
        "            writer.writeheader()  # Write the column headers to the file\n",
        "            writer.writerows(data)  # Write each row of data\n",
        "        print(f\"Data successfully saved to {filename}.\")  # Confirm success to the user\n",
        "    except Exception as e:\n",
        "        # Handle any file-saving issues\n",
        "        print(f\"Error saving data to CSV: {e}\")\n",
        "\n",
        "# Step 5: Display a preview of the scraped data\n",
        "def preview_data(data, num_rows=5):\n",
        "    \"\"\"\n",
        "    Display a preview of the scraped data in tabular format.\n",
        "    - Purpose: Show a quick preview of the data to ensure it's correctly scraped before moving forward.\n",
        "    - Why: This helps validate the data and catch issues early on.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df = pd.DataFrame(data)  # Convert the scraped data into a Pandas DataFrame for tabular representation\n",
        "        print(\"\\nPreview of Scraped Data:\\n\")\n",
        "        print(df.head(num_rows))  # Display the first few rows of the data\n",
        "    except Exception as e:\n",
        "        # Handle issues with data preview\n",
        "        print(f\"Error displaying preview: {e}\")\n",
        "\n",
        "# Main process: This is where all the functions come together\n",
        "if __name__ == \"__main__\":\n",
        "    # Step 3: Scrape the URLs and get the data\n",
        "    scraped_data = scrape_webpages(urls)\n",
        "\n",
        "    # Step 4: Save the scraped data into a CSV file\n",
        "    save_to_csv(scraped_data)\n",
        "\n",
        "    # Step 5: Display a preview of the scraped data\n",
        "    preview_data(scraped_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# **Explanation of the Output**\n",
        "\n",
        "This output represents the **scraped data from the webpage**. It is a structured representation of information that was collected from certain URLs (webpage links). Let's break it down step by step:\n",
        "\n",
        "#### **Columns in the Data**\n",
        "1. **URL**  \n",
        "   - The `URL` column contains the web address of the pages from which the content was scraped.\n",
        "   - These URLs are like digital addresses that point to specific webpages on the internet.\n",
        "   - Example: `https://thatware.co/software-development-services/`  \n",
        "     This URL is for a page about custom software development services.\n",
        "\n",
        "2. **Content**  \n",
        "   - The `Content` column contains the textual content found on each webpage.\n",
        "   - This is the information that was visible on the webpage, such as titles, descriptions, and any other text.\n",
        "   - Example:\n",
        "     - \"Custom Software Development Services - Software tailored to your needs.\"\n",
        "     - This content is what users see when they visit the corresponding URL.\n",
        "\n",
        "#### **Rows in the Data**\n",
        "Each row in the output corresponds to one webpage:\n",
        "- Row 1 (Index `0`):\n",
        "  - URL: A webpage about software development services.\n",
        "  - Content: Text from that page, which includes descriptions or promotional content about those services.\n",
        "- Row 2 (Index `1`):\n",
        "  - URL: A webpage about business intelligence services.\n",
        "  - Content: Text from that page discussing competitive analysis and strategies.\n",
        "- Row 3 (Index `2`):\n",
        "  - URL: A webpage about competitor keyword analysis.\n",
        "  - Content: Text describing SEO services related to competitor research.\n",
        "\n",
        "---\n",
        "\n",
        "### **Purpose of the Data**\n",
        "This data serves as the **input for further analysis** in your Knowledge-Based Trust (KBT) SEO Model. Here's what it does:\n",
        "1. **Extracts Information:**\n",
        "   - Gathers textual content from specific URLs to understand what the page is about.\n",
        "\n",
        "2. **Prepares for Analysis:**\n",
        "   - This content will later be analyzed for issues like tone, grammar, and keyword density to improve the quality of the webpage.\n",
        "\n",
        "3. **Client-Friendly View:**\n",
        "   - This output shows the client what information has been collected from their webpages for review or further processing.\n",
        "\n",
        "---\n",
        "\n",
        "### **Why This Data Matters**\n",
        "1. **SEO Insights:**\n",
        "   - Helps analyze how webpages are written and whether they align with SEO best practices.\n",
        "2. **Quality Assurance:**\n",
        "   - Ensures that the content on the webpages is engaging, grammatically correct, and optimized for keywords.\n",
        "3. **Data Transparency:**\n",
        "   - Shows exactly what data was extracted from the webpages, ensuring there are no surprises for the client.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "This output represents the starting point of the KBT model. It captures the **webpage URLs** and their **content** to provide transparency and set the foundation for analysis. It ensures the client knows exactly what is being analyzed and why.\n",
        "\n"
      ],
      "metadata": {
        "id": "686-rmZPKFgw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZkhL3ZolL5h"
      },
      "source": [
        "---\n",
        "# **Part 2: Data Enhancement and Preprocessing**\n",
        "**Purpose**: To clean, preprocess, and enrich the webpage content with NLP (Natural Language Processing) features.\n",
        "\n",
        "#### **Steps and Functionality**:\n",
        "1. **`initialize_nltk_resources()`**:  \n",
        "   - Downloads necessary NLTK resources such as stopwords and tokenizers.\n",
        "   - **Why**: Prepares the environment for advanced text preprocessing tasks.\n",
        "\n",
        "2. **`initialize_spacy_model()`**:  \n",
        "   - Loads SpaCy’s pre-trained language model for grammar and sentence analysis.\n",
        "   - **Why**: Enables advanced NLP tasks like sentence segmentation and keyword extraction.\n",
        "\n",
        "3. **`preprocess_text(text, nlp)`**:  \n",
        "   - Cleans the text by removing special characters, converting to lowercase, and filtering out stopwords.\n",
        "   - **Why**: Prepares the text for accurate NLP analysis.\n",
        "\n",
        "4. **`extract_keywords(content, nlp)`**:  \n",
        "   - Extracts keywords dynamically using SpaCy’s part-of-speech tagging.\n",
        "   - **Why**: Identifies the most relevant terms in the text.\n",
        "\n",
        "5. **`calculate_sentiment(content)`**:  \n",
        "   - Analyzes the sentiment polarity (positive, neutral, or negative) of the content.\n",
        "   - **Why**: Determines the emotional tone of the text, which is crucial for trust analysis.\n",
        "\n",
        "6. **`sentence_metadata(content, nlp)`**:  \n",
        "   - Provides metadata for each sentence, including its length and whether it uses passive voice.\n",
        "   - **Why**: Identifies structural issues in the text.\n",
        "\n",
        "7. **`count_citations(content)`**:  \n",
        "   - Counts references and citations dynamically based on specific keywords like \"source\" or \"report.\"\n",
        "   - **Why**: Measures the credibility of the content.\n",
        "\n",
        "8. **`process_data(input_file, output_file)`**:  \n",
        "   - Applies all preprocessing steps to the webpage content and saves the enhanced data to a CSV file.\n",
        "   - **Why**: Enhances the raw data with NLP insights for final analysis.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries for text processing\n",
        "import re  # To clean and normalize text by removing special characters\n",
        "import pandas as pd  # To handle tabular data in a structured format\n",
        "import nltk  # Natural Language Toolkit for language-related tasks\n",
        "from nltk.corpus import stopwords  # For removing common stopwords (e.g., 'the', 'is', etc.)\n",
        "from textblob import TextBlob  # For sentiment analysis\n",
        "import spacy  # For advanced natural language processing\n",
        "from collections import Counter  # For counting keyword frequencies\n",
        "\n",
        "# Step 1: Download and ensure all necessary NLP resources are available\n",
        "def initialize_nltk_resources():\n",
        "    \"\"\"\n",
        "    Ensures all required NLTK resources are available for the program.\n",
        "    - Purpose: Downloads tokenization resources, stopwords, and WordNet.\n",
        "    - Why: These resources are essential for text cleaning and processing tasks.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        nltk.download('punkt', force=True)  # For breaking sentences into words\n",
        "        nltk.download('stopwords', force=True)  # To filter out common stopwords\n",
        "        nltk.download('wordnet', force=True)  # For word synonym and lexical analysis\n",
        "        nltk.download('omw-1.4', force=True)  # Additional support for synonyms\n",
        "        print(\"All necessary NLTK resources downloaded successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading NLTK resources: {e}\")\n",
        "        raise e\n",
        "\n",
        "# Step 2: Load SpaCy's language model\n",
        "def initialize_spacy_model():\n",
        "    \"\"\"\n",
        "    Loads SpaCy's English language model for advanced NLP tasks.\n",
        "    - Purpose: Provides features like tokenization, named entity recognition, and more.\n",
        "    - Why: SpaCy handles advanced linguistic features that enhance text processing.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return spacy.load(\"en_core_web_sm\")  # Load the SpaCy English model\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading SpaCy model: {e}\")\n",
        "        raise e\n",
        "\n",
        "# Step 3: Preprocess text by cleaning and removing noise\n",
        "def preprocess_text(text, nlp):\n",
        "    \"\"\"\n",
        "    Cleans and preprocesses text data by:\n",
        "    - Removing special characters.\n",
        "    - Converting text to lowercase.\n",
        "    - Removing stopwords using NLTK and SpaCy.\n",
        "    - Purpose: Ensures the text is clean and ready for analysis.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)  # Remove special characters like punctuation\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()  # Normalize spaces\n",
        "        text = text.lower()  # Convert to lowercase for consistency\n",
        "        doc = nlp(text)  # Use SpaCy to tokenize text\n",
        "        stop_words = set(stopwords.words('english'))  # Load stopwords\n",
        "        filtered_tokens = [token.text for token in doc if token.text not in stop_words]\n",
        "        return ' '.join(filtered_tokens)  # Return cleaned text\n",
        "    except Exception as e:\n",
        "        print(f\"Error during text preprocessing: {e}\")\n",
        "        raise e\n",
        "\n",
        "# Step 4: Extract keywords from the text dynamically\n",
        "def extract_keywords(content, nlp):\n",
        "    \"\"\"\n",
        "    Identifies keywords dynamically based on parts of speech (NOUN, PROPN).\n",
        "    - Purpose: Highlight important concepts from the text.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        doc = nlp(content)  # Analyze the text using SpaCy\n",
        "        keywords = [token.text.lower() for token in doc if token.pos_ in [\"NOUN\", \"PROPN\"]]\n",
        "        return dict(Counter(keywords))  # Count keyword frequencies and return as a dictionary\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting keywords: {e}\")\n",
        "        raise e\n",
        "\n",
        "# Step 5: Perform sentiment analysis on the text\n",
        "def calculate_sentiment(content):\n",
        "    \"\"\"\n",
        "    Calculates the sentiment polarity of the text.\n",
        "    - Purpose: Determines whether the text is positive, negative, or neutral.\n",
        "    - Output: Polarity score between -1 (negative) and 1 (positive).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        blob = TextBlob(content)  # Use TextBlob for sentiment analysis\n",
        "        return blob.sentiment.polarity  # Return the polarity score\n",
        "    except Exception as e:\n",
        "        print(f\"Error calculating sentiment: {e}\")\n",
        "        raise e\n",
        "\n",
        "# Step 6: Generate sentence-level metadata\n",
        "def sentence_metadata(content, nlp):\n",
        "    \"\"\"\n",
        "    Provides metadata about each sentence, such as:\n",
        "    - Sentence length.\n",
        "    - Whether the sentence is in passive voice.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        doc = nlp(content)  # Tokenize and analyze the text\n",
        "        metadata = []  # Store metadata for each sentence\n",
        "        for sent in doc.sents:\n",
        "            # Check if the sentence uses passive voice\n",
        "            is_passive = any([token.tag_ == \"VBN\" and token.dep_ == \"auxpass\" for token in sent])\n",
        "            metadata.append({\n",
        "                \"sentence\": sent.text,\n",
        "                \"length\": len(sent.text.split()),  # Word count in the sentence\n",
        "                \"is_passive\": is_passive\n",
        "            })\n",
        "        return metadata\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating sentence metadata: {e}\")\n",
        "        raise e\n",
        "\n",
        "# Step 7: Count citations or references in the text\n",
        "def count_citations(content):\n",
        "    \"\"\"\n",
        "    Counts the number of references or citations in the text.\n",
        "    - Purpose: Identify if the content provides sufficient references.\n",
        "    \"\"\"\n",
        "    citation_keywords = [\"source\", \"reference\", \"citation\", \"study\", \"report\"]\n",
        "    return sum(content.lower().count(keyword) for keyword in citation_keywords)\n",
        "\n",
        "# Step 8: Load the CSV input file\n",
        "def load_input_data(filename):\n",
        "    \"\"\"\n",
        "    Loads the input CSV file containing webpage content.\n",
        "    - Purpose: Prepares the data for processing.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        data = pd.read_csv(filename)  # Load data into a Pandas DataFrame\n",
        "        print(f\"\\nLoaded data from '{filename}'. Preview:\")\n",
        "        print(data.head())  # Display the first few rows\n",
        "        return data\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error: File '{filename}' not found.\")\n",
        "        raise e\n",
        "\n",
        "# Step 9: Process the data for analysis\n",
        "def process_data(input_file, output_file):\n",
        "    \"\"\"\n",
        "    Enhances data with advanced NLP features like:\n",
        "    - Cleaned content.\n",
        "    - Keyword counts.\n",
        "    - Sentiment scores.\n",
        "    - Sentence metadata.\n",
        "    - Citation counts.\n",
        "    \"\"\"\n",
        "    initialize_nltk_resources()  # Step 1: Initialize NLP resources\n",
        "    nlp = initialize_spacy_model()  # Step 2: Load SpaCy model\n",
        "    data = load_input_data(input_file)  # Step 8: Load input data\n",
        "\n",
        "    try:\n",
        "        data.rename(columns={\"Content\": \"Original_Content\"}, inplace=True)  # Ensure consistent column names\n",
        "        # Apply preprocessing and analysis functions\n",
        "        data[\"Cleaned_Content\"] = data[\"Original_Content\"].apply(lambda x: preprocess_text(x, nlp))\n",
        "        data[\"Keyword_Counts\"] = data[\"Original_Content\"].apply(lambda x: extract_keywords(x, nlp))\n",
        "        data[\"Sentiment_Score\"] = data[\"Original_Content\"].apply(calculate_sentiment)\n",
        "        data[\"Citations_Count\"] = data[\"Original_Content\"].apply(count_citations)\n",
        "        data[\"Sentence_Metadata\"] = data[\"Original_Content\"].apply(lambda x: sentence_metadata(x, nlp))\n",
        "        # Add flags based on sentiment and citation thresholds\n",
        "        data[\"Sentiment_Flag\"] = data[\"Sentiment_Score\"].apply(\n",
        "            lambda x: \"Negative\" if x < 0 else \"Positive\" if x > 0 else \"Neutral\"\n",
        "        )\n",
        "        data[\"Citation_Flag\"] = data[\"Citations_Count\"].apply(\n",
        "            lambda x: \"Low Citations\" if x < 5 else \"Sufficient Citations\"\n",
        "        )\n",
        "        # Save the enhanced data\n",
        "        data.to_csv(output_file, index=False)\n",
        "        print(f\"\\nEnhanced data saved to '{output_file}'.\")\n",
        "        print(\"\\nPreview of Enhanced Data:\")\n",
        "        print(data.head())  # Display the processed data\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing data: {e}\")\n",
        "        raise e\n",
        "\n",
        "# Step 10: Run the workflow\n",
        "if __name__ == \"__main__\":\n",
        "    input_file = \"webpage_content.csv\"  # Input CSV file\n",
        "    output_file = \"enhanced_webpage_content.csv\"  # Output CSV file\n",
        "    process_data(input_file, output_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjDGwbZ90v3G",
        "outputId": "3aeca5be-2805-4547-a047-77b352cf1a1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All necessary NLTK resources downloaded successfully.\n",
            "\n",
            "Loaded data from 'webpage_content.csv'. Preview:\n",
            "                                                 URL  \\\n",
            "0  https://thatware.co/software-development-servi...   \n",
            "1  https://thatware.co/business-intelligence-serv...   \n",
            "2   https://thatware.co/competitor-keyword-analysis/   \n",
            "\n",
            "                                             Content  \n",
            "0  Custom Software Development Services - Softwar...  \n",
            "1  Business Intelligence Services - Competitive A...  \n",
            "2  SEO Competitor Keyword Analysis - Competitor R...  \n",
            "\n",
            "Enhanced data saved to 'enhanced_webpage_content.csv'.\n",
            "\n",
            "Preview of Enhanced Data:\n",
            "                                                 URL  \\\n",
            "0  https://thatware.co/software-development-servi...   \n",
            "1  https://thatware.co/business-intelligence-serv...   \n",
            "2   https://thatware.co/competitor-keyword-analysis/   \n",
            "\n",
            "                                    Original_Content  \\\n",
            "0  Custom Software Development Services - Softwar...   \n",
            "1  Business Intelligence Services - Competitive A...   \n",
            "2  SEO Competitor Keyword Analysis - Competitor R...   \n",
            "\n",
            "                                     Cleaned_Content  \\\n",
            "0  custom software development services software ...   \n",
            "1  business intelligence services competitive ana...   \n",
            "2  seo competitor keyword analysis competitor res...   \n",
            "\n",
            "                                      Keyword_Counts  Sentiment_Score  \\\n",
            "0  {'custom': 32, 'software': 105, 'development':...         0.147029   \n",
            "1  {'business': 35, 'intelligence': 20, 'services...         0.155916   \n",
            "2  {'seo': 108, 'competitor': 34, 'keyword': 55, ...         0.199583   \n",
            "\n",
            "   Citations_Count                                  Sentence_Metadata  \\\n",
            "0               12  [{'sentence': 'Custom Software Development Ser...   \n",
            "1               11  [{'sentence': 'Business Intelligence Services ...   \n",
            "2                7  [{'sentence': 'SEO Competitor Keyword Analysis...   \n",
            "\n",
            "  Sentiment_Flag         Citation_Flag  \n",
            "0       Positive  Sufficient Citations  \n",
            "1       Positive  Sufficient Citations  \n",
            "2       Positive  Sufficient Citations  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "#  **Explanation of the Output**\n",
        "This output represents **processed and enhanced data** from various webpages. The raw webpage content has been cleaned and analyzed to provide insights into its quality, tone, readability, and keyword usage. This enhanced data is saved in a CSV file called `'enhanced_webpage_content.csv'`.\n",
        "\n",
        "It contains **several columns**, each representing a specific aspect of the analyzed content.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Column-by-Column Explanation**\n",
        "\n",
        "#### **Column: `URL`**\n",
        "- **What it is**:\n",
        "  - This column lists the address of the webpage from which the data was collected.\n",
        "- **Why it’s important**:\n",
        "  - It tells us the source of the content, so we can trace each piece of information back to its webpage.\n",
        "- **Example**:\n",
        "  - `https://thatware.co/software-development-services/`\n",
        "  - This URL links to a page about software development services.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Column: `Original_Content`**\n",
        "- **What it is**:\n",
        "  - The exact text or content extracted from the webpage. This is how the content appears on the website.\n",
        "- **Why it’s important**:\n",
        "  - It provides the unaltered raw text for reference before any cleaning or processing.\n",
        "- **Example**:\n",
        "  - `\"Custom Software Development Services - Software tailored to your needs.\"`\n",
        "  - This is the raw content from the website’s page.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Column: `Cleaned_Content`**\n",
        "- **What it is**:\n",
        "  - A cleaned and processed version of the `Original_Content`. This version removes unnecessary characters, punctuation, stop words, and formatting to make it easier to analyze.\n",
        "- **Why it’s important**:\n",
        "  - Cleaning the content ensures accurate analysis, especially for tasks like keyword density or sentiment analysis.\n",
        "- **Example**:\n",
        "  - `\"custom software development services software tailored needs\"`\n",
        "  - This processed content is ready for deeper analysis.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Column: `Keyword_Counts`**\n",
        "- **What it is**:\n",
        "  - A breakdown of how often specific words (keywords) appear in the cleaned content.\n",
        "- **Why it’s important**:\n",
        "  - It identifies the focus of the content and flags potential overuse of specific words (keyword stuffing), which can harm SEO rankings.\n",
        "- **Example**:\n",
        "  - `{ 'custom': 32, 'software': 105, 'development': 45 }`\n",
        "  - This tells us the word `software` appears 105 times, which could be excessive.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Column: `Sentiment_Score`**\n",
        "- **What it is**:\n",
        "  - A numerical score that represents the emotional tone of the content. It is calculated using advanced algorithms.\n",
        "- **Why it’s important**:\n",
        "  - Content with a neutral or negative sentiment may not engage users effectively, while a positive tone is more appealing.\n",
        "- **Example**:\n",
        "  - `0.147029`\n",
        "  - A low score like this suggests a neutral or slightly positive tone.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Column: `Citations_Count`**\n",
        "- **What it is**:\n",
        "  - The total number of references, links, or citations found in the content.\n",
        "- **Why it’s important**:\n",
        "  - Citations establish the credibility and trustworthiness of the content. Pages with more citations are often considered more authoritative.\n",
        "- **Example**:\n",
        "  - `12`\n",
        "  - This indicates there are 12 citations or references in the content.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Column: `Sentence_Metadata`**\n",
        "- **What it is**:\n",
        "  - A detailed analysis of each sentence in the content, including:\n",
        "    - Sentence length\n",
        "    - Whether it’s written in passive voice\n",
        "    - Other grammatical details\n",
        "- **Why it’s important**:\n",
        "  - Helps improve readability by identifying overly complex or passive sentences.\n",
        "- **Example**:\n",
        "  ```\n",
        "  [{'sentence': 'Custom Software Development Services - Software tailored to your needs.',\n",
        "    'length': 8,\n",
        "    'is_passive': False}]\n",
        "  ```\n",
        "  - This metadata tells us the first sentence is 8 words long and not written in passive voice.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Column: `Sentiment_Flag`**\n",
        "- **What it is**:\n",
        "  - A simple label indicating whether the sentiment of the content is positive, neutral, or negative.\n",
        "- **Why it’s important**:\n",
        "  - Provides a quick overview of the emotional tone of the content.\n",
        "- **Example**:\n",
        "  - `\"Positive\"`\n",
        "  - This means the content has a generally positive tone.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Column: `Citation_Flag`**\n",
        "- **What it is**:\n",
        "  - A label indicating whether the content includes a sufficient number of citations.\n",
        "- **Why it’s important**:\n",
        "  - It ensures the content meets credibility standards, especially for professional or informational webpages.\n",
        "- **Example**:\n",
        "  - `\"Sufficient Citations\"`\n",
        "  - This means the content includes enough citations to be considered credible.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Why Is This Data Useful?**\n",
        "\n",
        "This enhanced data helps in several ways:\n",
        "\n",
        "#### **A. Improving Content Quality**\n",
        "- Identifies overly complex sentences, passive voice, and excessive keywords, allowing you to rewrite the content for better readability and user engagement.\n",
        "\n",
        "#### **B. SEO Optimization**\n",
        "- Highlights keyword usage patterns to ensure the content is optimized for search engines without being penalized for keyword stuffing.\n",
        "\n",
        "#### **C. Sentiment Analysis**\n",
        "- Ensures the tone of the content aligns with the target audience’s expectations. Positive sentiment is crucial for engaging users.\n",
        "\n",
        "#### **D. Credibility Check**\n",
        "- Assesses whether the content includes sufficient citations to establish trustworthiness.\n",
        "\n",
        "#### **E. Actionable Suggestions**\n",
        "- The data provides actionable feedback (e.g., reduce certain keywords, simplify sentences), making it easier to improve the content.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "IkvHQT3DLykb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1l_MjoDUrgW"
      },
      "source": [
        "---\n",
        "# **Part 3: Knowledge-Based Trust (KBT) Analysis**\n",
        "**Purpose**: To analyze the enhanced data for trustworthiness, tone, keyword usage, and readability.\n",
        "\n",
        "#### **Steps and Functionality**:\n",
        "1. **`load_enhanced_data(filename)`**:  \n",
        "   - Loads the enhanced CSV data into a structured format (DataFrame).\n",
        "   - **Why**: Prepares the input data for analysis.\n",
        "\n",
        "2. **`initialize_models()`**:  \n",
        "   - Initializes the tokenizer, tone analysis model, and SpaCy grammar model.\n",
        "   - **Why**: Provides the tools needed for detailed analysis of each webpage.\n",
        "\n",
        "3. **`chunk_text_dynamically(text, tokenizer, max_tokens=512)`**:  \n",
        "   - Splits large text into manageable chunks based on token limits.\n",
        "   - **Why**: Ensures the text fits within the limitations of the NLP models.\n",
        "\n",
        "4. **`analyze_chunk(chunk, tone_model, spacy_model)`**:  \n",
        "   - Analyzes each chunk for:\n",
        "     - Tone issues.\n",
        "     - Grammar problems.\n",
        "     - Keyword density.\n",
        "   - **Why**: Provides actionable insights on the content's quality.\n",
        "\n",
        "5. **`dynamic_analysis(row, tokenizer, tone_model, spacy_model)`**:  \n",
        "   - Aggregates the analysis results from all chunks of a webpage.\n",
        "   - **Why**: Consolidates findings into a single report for easier interpretation.\n",
        "\n",
        "6. **`save_results(data, csv_file, json_file)`**:  \n",
        "   - Saves the analysis results into both CSV and JSON formats.\n",
        "   - **Why**: Makes the output accessible for different use cases.\n",
        "\n",
        "7. **`process_trust_scores(input_file, csv_output_file, json_output_file)`**:  \n",
        "   - Executes the full KBT analysis pipeline, combining all previous steps.\n",
        "   - **Why**: Produces a final report highlighting key insights like issues, suggestions, and severity scores.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install textstat\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RSr8HexCuR9",
        "outputId": "fbf32cef-beeb-47ed-e7e4-fcc3e34a93d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textstat in /usr/local/lib/python3.10/dist-packages (0.7.4)\n",
            "Requirement already satisfied: pyphen in /usr/local/lib/python3.10/dist-packages (from textstat) (0.17.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from textstat) (75.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import pandas as pd  # For handling tabular data and reading/writing CSV files\n",
        "from transformers import AutoTokenizer, pipeline  # For advanced NLP tasks such as tone analysis\n",
        "from collections import Counter  # To count and manage issues and keyword densities\n",
        "import spacy  # For advanced natural language processing tasks like grammar analysis\n",
        "import json  # For saving the results in JSON format\n",
        "\n",
        "# Step 1: Load Enhanced Data\n",
        "def load_enhanced_data(filename):\n",
        "    \"\"\"\n",
        "    Loads the input CSV file containing cleaned webpage content.\n",
        "    Purpose:\n",
        "        - Converts the CSV into a structured format (DataFrame) for analysis.\n",
        "    Why:\n",
        "        - Structured input ensures efficient processing during analysis.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Reading the input CSV file into a Pandas DataFrame\n",
        "        data = pd.read_csv(filename)\n",
        "        print(f\"Data loaded successfully from '{filename}'.\")\n",
        "        print(\"Preview of the data:\\n\", data.head())  # Display the first few rows to verify content\n",
        "        return data\n",
        "    except FileNotFoundError:\n",
        "        raise Exception(f\"Error: File '{filename}' not found.\")  # Raise an error if the file is missing\n",
        "\n",
        "# Step 2: Initialize NLP Models\n",
        "def initialize_models():\n",
        "    \"\"\"\n",
        "    Initializes the required NLP models:\n",
        "    - Tokenizer: Splits large text into manageable parts.\n",
        "    - Tone Model: Determines the emotional tone of the content.\n",
        "    - SpaCy Model: Analyzes grammar and sentence structure.\n",
        "    Purpose:\n",
        "        - Provides tools for chunking, analyzing tone, and identifying grammatical structures.\n",
        "    \"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")  # Initialize the tokenizer\n",
        "    tone_model = pipeline(\"text-classification\", model=\"bhadresh-savani/distilbert-base-uncased-emotion\")  # Load tone model\n",
        "    spacy_model = spacy.load(\"en_core_web_sm\")  # Load SpaCy's English language model\n",
        "    return tokenizer, tone_model, spacy_model\n",
        "\n",
        "# Step 3: Chunk Text Dynamically with Token Limit Handling\n",
        "def chunk_text_dynamically(text, tokenizer, max_tokens=512):\n",
        "    \"\"\"\n",
        "    Splits long text into smaller chunks that fit within the model's token limit.\n",
        "    Purpose:\n",
        "        - Prevents errors when processing text that exceeds the model's token limit.\n",
        "    \"\"\"\n",
        "    tokens = tokenizer.tokenize(text)  # Tokenize the text into smaller units\n",
        "    chunks = []  # Store the resulting chunks\n",
        "    for i in range(0, len(tokens), max_tokens - 50):  # Split into overlapping chunks to preserve context\n",
        "        chunk_tokens = tokens[i:i + max_tokens - 50]\n",
        "        chunks.append(tokenizer.convert_tokens_to_string(chunk_tokens))  # Convert tokens back into text\n",
        "    return chunks\n",
        "\n",
        "# Step 4: Analyze Each Chunk\n",
        "def analyze_chunk(chunk, tone_model, spacy_model):\n",
        "    \"\"\"\n",
        "    Analyzes a single chunk of text for:\n",
        "    - Tone issues.\n",
        "    - Grammatical structure and sentence complexity.\n",
        "    - Keyword density and potential keyword stuffing.\n",
        "    Purpose:\n",
        "        - Provides detailed insights for each chunk of content.\n",
        "    \"\"\"\n",
        "    issues = Counter()  # Store detected issues\n",
        "    suggestions = set()  # Store actionable suggestions\n",
        "    severity_score = 0  # Overall score indicating the severity of issues in this chunk\n",
        "\n",
        "    # Tone Analysis\n",
        "    try:\n",
        "        truncated_chunk = chunk[:512]  # Ensure the chunk doesn't exceed the model's input limit\n",
        "        tone = tone_model(truncated_chunk)[0]['label'].lower()  # Perform tone analysis\n",
        "        if tone in [\"negative\", \"neutral\"]:  # Identify undesirable tones\n",
        "            issues[\"Negative/neutral tone detected.\"] += 1\n",
        "            suggestions.add(\"Improve the tone to make it more engaging.\")\n",
        "            severity_score += 2  # Assign higher weight to tone issues\n",
        "    except Exception as e:\n",
        "        print(f\"Tone analysis failed: {e}\")  # Handle tone analysis errors gracefully\n",
        "\n",
        "    # Grammar and Sentence Analysis\n",
        "    doc = spacy_model(chunk)  # Analyze text using SpaCy\n",
        "    processed_sentences = set()  # Avoid duplicate sentence processing\n",
        "    for sent in doc.sents:\n",
        "        if sent.text not in processed_sentences:\n",
        "            processed_sentences.add(sent.text)\n",
        "            # Check for passive voice usage\n",
        "            passive_voice_count = sum(1 for token in sent if token.dep_ == \"auxpass\")\n",
        "            if passive_voice_count > 0:\n",
        "                issues[\"Passive voice detected.\"] += passive_voice_count\n",
        "                suggestions.add(\"Rewrite sentences in active voice.\")\n",
        "                severity_score += passive_voice_count  # Weight based on the count of passive constructions\n",
        "            # Check for sentence complexity\n",
        "            if len(sent.text.split()) > 20:\n",
        "                issues[\"Contains long/complex sentences.\"] += 1\n",
        "                suggestions.add(\"Simplify long sentences for better readability.\")\n",
        "                severity_score += 1\n",
        "\n",
        "    # Keyword Density Analysis\n",
        "    words = chunk.split()  # Split the chunk into individual words\n",
        "    keyword_counts = Counter(words)  # Count occurrences of each word\n",
        "    flagged_keywords = {}\n",
        "    for keyword, count in keyword_counts.items():\n",
        "        density = (count / len(words)) * 100  # Calculate keyword density as a percentage\n",
        "        if density > 3:  # Flag keywords with high density\n",
        "            issues[f\"Keyword stuffing detected: {keyword}\"] += 1\n",
        "            flagged_keywords[keyword] = density\n",
        "            suggestions.add(f\"Reduce usage of keyword '{keyword}'.\")\n",
        "            severity_score += 1\n",
        "\n",
        "    return issues, list(suggestions), flagged_keywords, severity_score\n",
        "\n",
        "# Step 5: Aggregate Results\n",
        "def dynamic_analysis(row, tokenizer, tone_model, spacy_model):\n",
        "    \"\"\"\n",
        "    Aggregates analysis results across all chunks of a webpage's content.\n",
        "    Purpose:\n",
        "        - Combines insights to provide a holistic view of the content's quality.\n",
        "    \"\"\"\n",
        "    aggregated_issues = Counter()  # Consolidate issues\n",
        "    aggregated_suggestions = set()  # Consolidate suggestions\n",
        "    keyword_densities = {}  # Store keyword densities\n",
        "    severity_scores = []  # Track severity scores for each chunk\n",
        "\n",
        "    text_chunks = chunk_text_dynamically(row[\"Cleaned_Content\"], tokenizer)\n",
        "    for chunk in text_chunks:\n",
        "        chunk_issues, chunk_suggestions, flagged_keywords, chunk_severity_score = analyze_chunk(chunk, tone_model, spacy_model)\n",
        "        aggregated_issues.update(chunk_issues)\n",
        "        aggregated_suggestions.update(chunk_suggestions)\n",
        "        keyword_densities.update(flagged_keywords)\n",
        "        severity_scores.append(chunk_severity_score)\n",
        "\n",
        "    # Print a summary for the aggregated results\n",
        "    summary = {\n",
        "        \"Total Issues\": sum(aggregated_issues.values()),\n",
        "        \"Severity Score\": sum(severity_scores),\n",
        "        \"Top Issues\": aggregated_issues.most_common(3),\n",
        "        \"Top Suggestions\": list(aggregated_suggestions)[:3]\n",
        "    }\n",
        "    print(\"Summary for Content Analysis:\", summary)\n",
        "\n",
        "    return aggregated_issues, list(aggregated_suggestions), keyword_densities, sum(severity_scores)\n",
        "\n",
        "# Step 6: Save Results\n",
        "def save_results(data, csv_file, json_file):\n",
        "    \"\"\"\n",
        "    Saves the analysis results into CSV and JSON formats.\n",
        "    Purpose:\n",
        "        - Enables the client to access results in user-friendly formats.\n",
        "    \"\"\"\n",
        "    data.to_csv(csv_file, index=False)  # Save to CSV\n",
        "    print(f\"Results saved to CSV file: {csv_file}\")\n",
        "\n",
        "    json_output = data.to_dict(orient=\"records\")  # Convert data to a JSON-compatible structure\n",
        "    with open(json_file, \"w\") as json_file_obj:\n",
        "        json.dump(json_output, json_file_obj, indent=4)  # Save as a JSON file\n",
        "    print(f\"Results saved to JSON file: {json_file_obj.name}\")\n",
        "\n",
        "# Step 7: Process and Save Analysis Results\n",
        "def process_trust_scores(input_file, csv_output_file, json_output_file):\n",
        "    \"\"\"\n",
        "    Processes input content, performs analysis, and saves the results.\n",
        "    Purpose:\n",
        "        - Executes the complete analysis pipeline.\n",
        "    \"\"\"\n",
        "    data = load_enhanced_data(input_file)  # Step 1: Load data\n",
        "    tokenizer, tone_model, spacy_model = initialize_models()  # Step 2: Initialize models\n",
        "\n",
        "    # Apply analysis to each row of content\n",
        "    data[\"Issues\"], data[\"Suggestions\"], data[\"Keyword_Densities\"], data[\"Severity_Score\"] = zip(\n",
        "        *data.apply(lambda row: dynamic_analysis(row, tokenizer, tone_model, spacy_model), axis=1)\n",
        "    )\n",
        "\n",
        "    print(\"\\nPreview of Analysis Results:\")\n",
        "    print(data[[\"Issues\", \"Suggestions\", \"Keyword_Densities\", \"Severity_Score\"]].head())  # Show a preview\n",
        "\n",
        "    save_results(data, csv_output_file, json_output_file)  # Save results\n",
        "\n",
        "# Step 8: Execute Workflow\n",
        "if __name__ == \"__main__\":\n",
        "    input_file = \"enhanced_webpage_content.csv\"  # Input data\n",
        "    csv_output_file = \"kbt_refined_output.csv\"  # CSV output file\n",
        "    json_output_file = \"kbt_refined_output.json\"  # JSON output file\n",
        "    process_trust_scores(input_file, csv_output_file, json_output_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpK8RATNFTy1",
        "outputId": "09e91870-c649-4a93-b620-dfbf59bbffd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded successfully from 'enhanced_webpage_content.csv'.\n",
            "Preview of the data:\n",
            "                                                  URL  \\\n",
            "0  https://thatware.co/software-development-servi...   \n",
            "1  https://thatware.co/business-intelligence-serv...   \n",
            "2   https://thatware.co/competitor-keyword-analysis/   \n",
            "\n",
            "                                    Original_Content  \\\n",
            "0  Custom Software Development Services - Softwar...   \n",
            "1  Business Intelligence Services - Competitive A...   \n",
            "2  SEO Competitor Keyword Analysis - Competitor R...   \n",
            "\n",
            "                                     Cleaned_Content  \\\n",
            "0  custom software development services software ...   \n",
            "1  business intelligence services competitive ana...   \n",
            "2  seo competitor keyword analysis competitor res...   \n",
            "\n",
            "                                      Keyword_Counts  Sentiment_Score  \\\n",
            "0  {'custom': 32, 'software': 105, 'development':...         0.147029   \n",
            "1  {'business': 35, 'intelligence': 20, 'services...         0.155916   \n",
            "2  {'seo': 108, 'competitor': 34, 'keyword': 55, ...         0.199583   \n",
            "\n",
            "   Citations_Count                                  Sentence_Metadata  \\\n",
            "0               12  [{'sentence': 'Custom Software Development Ser...   \n",
            "1               11  [{'sentence': 'Business Intelligence Services ...   \n",
            "2                7  [{'sentence': 'SEO Competitor Keyword Analysis...   \n",
            "\n",
            "  Sentiment_Flag         Citation_Flag  \n",
            "0       Positive  Sufficient Citations  \n",
            "1       Positive  Sufficient Citations  \n",
            "2       Positive  Sufficient Citations  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (3473 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary for Content Analysis: {'Total Issues': 29, 'Severity Score': 29, 'Top Issues': [('Contains long/complex sentences.', 11), ('Keyword stuffing detected: software', 4), ('Keyword stuffing detected: development', 3)], 'Top Suggestions': [\"Reduce usage of keyword 'services'.\", 'Simplify long sentences for better readability.', \"Reduce usage of keyword 'saas'.\"]}\n",
            "Summary for Content Analysis: {'Total Issues': 21, 'Severity Score': 21, 'Top Issues': [('Contains long/complex sentences.', 9), ('Keyword stuffing detected: services', 3), ('Keyword stuffing detected: analysis', 2)], 'Top Suggestions': [\"Reduce usage of keyword 'services'.\", 'Simplify long sentences for better readability.', \"Reduce usage of keyword 'upto'.\"]}\n",
            "Summary for Content Analysis: {'Total Issues': 23, 'Severity Score': 23, 'Top Issues': [('Contains long/complex sentences.', 8), ('Keyword stuffing detected: seo', 3), ('Keyword stuffing detected: services', 3)], 'Top Suggestions': [\"Reduce usage of keyword 'services'.\", \"Reduce usage of keyword 'keyword'.\", 'Simplify long sentences for better readability.']}\n",
            "\n",
            "Preview of Analysis Results:\n",
            "                                              Issues  \\\n",
            "0  {'Contains long/complex sentences.': 11, 'Keyw...   \n",
            "1  {'Contains long/complex sentences.': 9, 'Keywo...   \n",
            "2  {'Contains long/complex sentences.': 8, 'Keywo...   \n",
            "\n",
            "                                         Suggestions  \\\n",
            "0  [Reduce usage of keyword 'services'., Simplify...   \n",
            "1  [Reduce usage of keyword 'services'., Simplify...   \n",
            "2  [Reduce usage of keyword 'services'., Reduce u...   \n",
            "\n",
            "                                   Keyword_Densities  Severity_Score  \n",
            "0  {'custom': 3.132530120481928, 'software': 3.98...              29  \n",
            "1  {'business': 3.5629453681710213, 'services': 3...              21  \n",
            "2  {'seo': 10.043668122270741, 'competitor': 3.90...              23  \n",
            "Results saved to CSV file: kbt_refined_output.csv\n",
            "Results saved to JSON file: kbt_refined_output.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# **Explanation of the Output**\n",
        "\n",
        "This output is the result of analyzing webpage content for **quality and SEO performance**. It provides:\n",
        "1. A **summary of issues** identified in the content.\n",
        "2. A list of **actionable suggestions** to improve the content.\n",
        "3. Insights into **keyword usage patterns**.\n",
        "4. A **severity score** to indicate how urgent the issues are.\n",
        "\n",
        "This analysis helps improve the readability, relevance, and SEO ranking of the content on your webpages.\n",
        "\n",
        "---\n",
        "\n",
        "### **Output Breakdown**\n",
        "\n",
        "#### **1. Summary for Content Analysis**\n",
        "Each row in this section summarizes the issues, suggestions, and keyword analysis for a specific webpage.\n",
        "\n",
        "##### **Example Row:**\n",
        "```\n",
        "{'Total Issues': 29,\n",
        " 'Severity Score': 29,\n",
        " 'Top Issues': [('Contains long/complex sentences.', 11),\n",
        "                ('Keyword stuffing detected: software', 4),\n",
        "                ('Keyword stuffing detected: development', 3)],\n",
        " 'Top Suggestions': [\"Reduce usage of keyword 'services'.\",\n",
        "                      'Simplify long sentences for better readability.',\n",
        "                      \"Reduce usage of keyword 'saas'.\"]\n",
        "}\n",
        "```\n",
        "\n",
        "##### **Explanation:**\n",
        "1. **Total Issues**:\n",
        "   - This shows the total number of problems identified in the content.\n",
        "   - Example: `29` issues found.\n",
        "\n",
        "2. **Severity Score**:\n",
        "   - This indicates how serious the problems are. A higher score means more significant issues.\n",
        "   - Example: A severity score of `29` means the content has critical issues that need attention.\n",
        "\n",
        "3. **Top Issues**:\n",
        "   - This lists the most frequent problems.\n",
        "   - Example:\n",
        "     - `Contains long/complex sentences (11 times)`: The content has 11 sentences that are too long or difficult to read.\n",
        "     - `Keyword stuffing detected: software (4 times)`: The word \"software\" appears too frequently, which may harm SEO rankings.\n",
        "     - `Keyword stuffing detected: development (3 times)`: The word \"development\" is also overused.\n",
        "\n",
        "4. **Top Suggestions**:\n",
        "   - These are actionable steps to fix the issues.\n",
        "   - Example:\n",
        "     - `\"Reduce usage of keyword 'services'.\": This suggests cutting down on the overuse of the word \"services.\"\n",
        "     - `\"Simplify long sentences for better readability.\": Break complex sentences into shorter ones.\n",
        "     - `\"Reduce usage of keyword 'saas'.\": Lower the frequency of the word \"saas\" to avoid keyword stuffing.\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. Preview of Analysis Results**\n",
        "This section shows detailed results for each analyzed webpage.\n",
        "\n",
        "##### **Example Columns:**\n",
        "\n",
        "1. **Issues**:\n",
        "   - Contains detailed counts of problems in the content.\n",
        "   - Example:\n",
        "     ```\n",
        "     {'Contains long/complex sentences.': 11,\n",
        "      'Keyword stuffing detected: software': 4,\n",
        "      'Keyword stuffing detected: development': 3}\n",
        "     ```\n",
        "     - Long sentences are a common issue (`11` instances).\n",
        "     - Overuse of the keywords \"software\" and \"development.\"\n",
        "\n",
        "2. **Suggestions**:\n",
        "   - Lists recommendations to improve the content.\n",
        "   - Example:\n",
        "     ```\n",
        "     [Reduce usage of keyword 'services'.,\n",
        "      Simplify long sentences for better readability.,\n",
        "      Reduce usage of keyword 'saas'.]\n",
        "     ```\n",
        "     - These suggestions align with the identified issues.\n",
        "\n",
        "3. **Keyword_Densities**:\n",
        "   - Shows how frequently each keyword appears as a percentage of the total content.\n",
        "   - Example:\n",
        "     ```\n",
        "     {'custom': 3.13,\n",
        "      'software': 3.98,\n",
        "      'development': 3.50}\n",
        "     ```\n",
        "     - The keyword \"software\" appears in `3.98%` of the content, which may be too high.\n",
        "\n",
        "4. **Severity_Score**:\n",
        "   - Indicates how serious the issues are for each webpage.\n",
        "   - Example:\n",
        "     - `29` is a high severity score, suggesting urgent fixes are required.\n",
        "\n",
        "---\n",
        "\n",
        "### **How to Present This to a Client**\n",
        "\n",
        "#### **Key Points to Emphasize:**\n",
        "1. **Purpose**:\n",
        "   - \"This analysis highlights the strengths and weaknesses of your webpage content. It helps you understand where improvements are needed for better readability, SEO, and user engagement.\"\n",
        "\n",
        "2. **Explanation**:\n",
        "   - **Total Issues**:\n",
        "     - \"The number of problems found in the content, such as long sentences or overused keywords.\"\n",
        "   - **Severity Score**:\n",
        "     - \"A measure of how urgent the fixes are. A higher score means the content needs more attention.\"\n",
        "   - **Top Issues**:\n",
        "     - \"The most frequent problems, like keyword stuffing or overly complex sentences.\"\n",
        "   - **Top Suggestions**:\n",
        "     - \"Specific recommendations to improve the content.\"\n",
        "\n",
        "3. **Value**:\n",
        "   - \"By addressing these issues, your content will be more engaging, SEO-friendly, and easier to read.\"\n",
        "\n",
        "---\n",
        "\n",
        "### **Final Summary**\n",
        "This output provides a comprehensive evaluation of webpage content. It identifies issues, prioritizes them by severity, and offers actionable suggestions. Using this data, you can make informed decisions to optimize your content for better user engagement and search engine rankings."
      ],
      "metadata": {
        "id": "VprlVpISMunH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing required libraries\n",
        "import requests  # To send HTTP requests and fetch webpage content\n",
        "from bs4 import BeautifulSoup  # To parse HTML and extract readable text\n",
        "import csv  # To save the scraped data in a CSV file\n",
        "import pandas as pd  # To display a preview of the data\n",
        "\n",
        "# List of URLs to scrape (provided by the user)\n",
        "urls = [\n",
        "    'https://thatware.co/software-development-services/',\n",
        "    'https://thatware.co/business-intelligence-services/',\n",
        "    'https://thatware.co/competitor-keyword-analysis/'\n",
        "]\n",
        "\n",
        "# Step 1: Function to fetch webpage content from a given URL\n",
        "def fetch_content(url):\n",
        "    \"\"\"\n",
        "    Fetch HTML content from a webpage.\n",
        "    - Purpose: This function sends an HTTP request to the given URL to fetch the raw HTML content of the page.\n",
        "    - Why: Without this step, we wouldn't have any data to process or analyze.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)  # Sending a GET request with a 10-second timeout\n",
        "        response.raise_for_status()  # Ensures the request was successful; raises an error otherwise\n",
        "        return response.text  # Return the raw HTML content of the page\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        # Print an error message if the request fails\n",
        "        print(f\"Error fetching URL {url}: {e}\")\n",
        "        return None  # Return None so the process can continue even if one URL fails\n",
        "\n",
        "# Step 2: Function to extract meaningful text from HTML content\n",
        "def extract_text_from_html(html_content):\n",
        "    \"\"\"\n",
        "    Extract visible text from HTML content.\n",
        "    - Purpose: This function removes unnecessary elements like scripts, styles, and hidden text from the HTML.\n",
        "    - Why: It ensures that only the main content of the webpage is extracted for analysis.\n",
        "    \"\"\"\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')  # Parse the HTML content using BeautifulSoup\n",
        "    # Extract visible text by removing unnecessary elements and combining all visible text\n",
        "    return soup.get_text(separator=' ', strip=True)\n",
        "\n",
        "# Step 3: Function to scrape all URLs and store their content\n",
        "def scrape_webpages(urls):\n",
        "    \"\"\"\n",
        "    Scrape webpage content for multiple URLs.\n",
        "    - Purpose: This function loops through each URL, fetches its content, and cleans it.\n",
        "    - Why: It structures the process of collecting and organizing webpage data for easy analysis later.\n",
        "    \"\"\"\n",
        "    webpage_data = []  # List to store scraped data for each URL\n",
        "    for url in urls:\n",
        "        print(f\"Scraping URL: {url}\")  # Notify the user of the current URL being processed\n",
        "        html_content = fetch_content(url)  # Step 1: Fetch HTML content\n",
        "        if html_content:  # Ensure we have valid content before proceeding\n",
        "            text_content = extract_text_from_html(html_content)  # Step 2: Extract clean text\n",
        "            # Store the URL and cleaned content in a dictionary\n",
        "            webpage_data.append({'URL': url, 'Content': text_content})\n",
        "    return webpage_data  # Return the scraped data as a list of dictionaries\n",
        "\n",
        "# Step 4: Save the scraped data to a CSV file\n",
        "def save_to_csv(data, filename='webpage_content.csv'):\n",
        "    \"\"\"\n",
        "    Save scraped data to a CSV file.\n",
        "    - Purpose: This function saves the collected webpage data into a structured format (CSV).\n",
        "    - Why: The CSV format is easy to open, share, and analyze using tools like Excel or Python.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Open the CSV file in write mode with UTF-8 encoding to handle special characters\n",
        "        with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
        "            # Create a CSV writer object and define the columns\n",
        "            writer = csv.DictWriter(file, fieldnames=['URL', 'Content'])\n",
        "            writer.writeheader()  # Write the column headers to the file\n",
        "            writer.writerows(data)  # Write each row of data\n",
        "        print(f\"Data successfully saved to {filename}.\")  # Confirm success to the user\n",
        "    except Exception as e:\n",
        "        # Handle any file-saving issues\n",
        "        print(f\"Error saving data to CSV: {e}\")\n",
        "\n",
        "# Step 5: Display a preview of the scraped data\n",
        "def preview_data(data, num_rows=5):\n",
        "    \"\"\"\n",
        "    Display a preview of the scraped data in tabular format.\n",
        "    - Purpose: Show a quick preview of the data to ensure it's correctly scraped before moving forward.\n",
        "    - Why: This helps validate the data and catch issues early on.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df = pd.DataFrame(data)  # Convert the scraped data into a Pandas DataFrame for tabular representation\n",
        "        print(\"\\nPreview of Scraped Data:\\n\")\n",
        "        print(df.head(num_rows))  # Display the first few rows of the data\n",
        "    except Exception as e:\n",
        "        # Handle issues with data preview\n",
        "        print(f\"Error displaying preview: {e}\")\n",
        "\n",
        "# Main process: This is where all the functions come together\n",
        "if __name__ == \"__main__\":\n",
        "    # Step 3: Scrape the URLs and get the data\n",
        "    scraped_data = scrape_webpages(urls)\n",
        "\n",
        "    # Step 4: Save the scraped data into a CSV file\n",
        "    save_to_csv(scraped_data)\n",
        "\n",
        "    # Step 5: Display a preview of the scraped data\n",
        "    preview_data(scraped_data)\n",
        "\n",
        "\n",
        "# Importing necessary libraries for text processing\n",
        "import re  # To clean and normalize text by removing special characters\n",
        "import pandas as pd  # To handle tabular data in a structured format\n",
        "import nltk  # Natural Language Toolkit for language-related tasks\n",
        "from nltk.corpus import stopwords  # For removing common stopwords (e.g., 'the', 'is', etc.)\n",
        "from textblob import TextBlob  # For sentiment analysis\n",
        "import spacy  # For advanced natural language processing\n",
        "from collections import Counter  # For counting keyword frequencies\n",
        "\n",
        "# Step 1: Download and ensure all necessary NLP resources are available\n",
        "def initialize_nltk_resources():\n",
        "    \"\"\"\n",
        "    Ensures all required NLTK resources are available for the program.\n",
        "    - Purpose: Downloads tokenization resources, stopwords, and WordNet.\n",
        "    - Why: These resources are essential for text cleaning and processing tasks.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        nltk.download('punkt', force=True)  # For breaking sentences into words\n",
        "        nltk.download('stopwords', force=True)  # To filter out common stopwords\n",
        "        nltk.download('wordnet', force=True)  # For word synonym and lexical analysis\n",
        "        nltk.download('omw-1.4', force=True)  # Additional support for synonyms\n",
        "        print(\"All necessary NLTK resources downloaded successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading NLTK resources: {e}\")\n",
        "        raise e\n",
        "\n",
        "# Step 2: Load SpaCy's language model\n",
        "def initialize_spacy_model():\n",
        "    \"\"\"\n",
        "    Loads SpaCy's English language model for advanced NLP tasks.\n",
        "    - Purpose: Provides features like tokenization, named entity recognition, and more.\n",
        "    - Why: SpaCy handles advanced linguistic features that enhance text processing.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return spacy.load(\"en_core_web_sm\")  # Load the SpaCy English model\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading SpaCy model: {e}\")\n",
        "        raise e\n",
        "\n",
        "# Step 3: Preprocess text by cleaning and removing noise\n",
        "def preprocess_text(text, nlp):\n",
        "    \"\"\"\n",
        "    Cleans and preprocesses text data by:\n",
        "    - Removing special characters.\n",
        "    - Converting text to lowercase.\n",
        "    - Removing stopwords using NLTK and SpaCy.\n",
        "    - Purpose: Ensures the text is clean and ready for analysis.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)  # Remove special characters like punctuation\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()  # Normalize spaces\n",
        "        text = text.lower()  # Convert to lowercase for consistency\n",
        "        doc = nlp(text)  # Use SpaCy to tokenize text\n",
        "        stop_words = set(stopwords.words('english'))  # Load stopwords\n",
        "        filtered_tokens = [token.text for token in doc if token.text not in stop_words]\n",
        "        return ' '.join(filtered_tokens)  # Return cleaned text\n",
        "    except Exception as e:\n",
        "        print(f\"Error during text preprocessing: {e}\")\n",
        "        raise e\n",
        "\n",
        "# Step 4: Extract keywords from the text dynamically\n",
        "def extract_keywords(content, nlp):\n",
        "    \"\"\"\n",
        "    Identifies keywords dynamically based on parts of speech (NOUN, PROPN).\n",
        "    - Purpose: Highlight important concepts from the text.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        doc = nlp(content)  # Analyze the text using SpaCy\n",
        "        keywords = [token.text.lower() for token in doc if token.pos_ in [\"NOUN\", \"PROPN\"]]\n",
        "        return dict(Counter(keywords))  # Count keyword frequencies and return as a dictionary\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting keywords: {e}\")\n",
        "        raise e\n",
        "\n",
        "# Step 5: Perform sentiment analysis on the text\n",
        "def calculate_sentiment(content):\n",
        "    \"\"\"\n",
        "    Calculates the sentiment polarity of the text.\n",
        "    - Purpose: Determines whether the text is positive, negative, or neutral.\n",
        "    - Output: Polarity score between -1 (negative) and 1 (positive).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        blob = TextBlob(content)  # Use TextBlob for sentiment analysis\n",
        "        return blob.sentiment.polarity  # Return the polarity score\n",
        "    except Exception as e:\n",
        "        print(f\"Error calculating sentiment: {e}\")\n",
        "        raise e\n",
        "\n",
        "# Step 6: Generate sentence-level metadata\n",
        "def sentence_metadata(content, nlp):\n",
        "    \"\"\"\n",
        "    Provides metadata about each sentence, such as:\n",
        "    - Sentence length.\n",
        "    - Whether the sentence is in passive voice.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        doc = nlp(content)  # Tokenize and analyze the text\n",
        "        metadata = []  # Store metadata for each sentence\n",
        "        for sent in doc.sents:\n",
        "            # Check if the sentence uses passive voice\n",
        "            is_passive = any([token.tag_ == \"VBN\" and token.dep_ == \"auxpass\" for token in sent])\n",
        "            metadata.append({\n",
        "                \"sentence\": sent.text,\n",
        "                \"length\": len(sent.text.split()),  # Word count in the sentence\n",
        "                \"is_passive\": is_passive\n",
        "            })\n",
        "        return metadata\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating sentence metadata: {e}\")\n",
        "        raise e\n",
        "\n",
        "# Step 7: Count citations or references in the text\n",
        "def count_citations(content):\n",
        "    \"\"\"\n",
        "    Counts the number of references or citations in the text.\n",
        "    - Purpose: Identify if the content provides sufficient references.\n",
        "    \"\"\"\n",
        "    citation_keywords = [\"source\", \"reference\", \"citation\", \"study\", \"report\"]\n",
        "    return sum(content.lower().count(keyword) for keyword in citation_keywords)\n",
        "\n",
        "# Step 8: Load the CSV input file\n",
        "def load_input_data(filename):\n",
        "    \"\"\"\n",
        "    Loads the input CSV file containing webpage content.\n",
        "    - Purpose: Prepares the data for processing.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        data = pd.read_csv(filename)  # Load data into a Pandas DataFrame\n",
        "        print(f\"\\nLoaded data from '{filename}'. Preview:\")\n",
        "        print(data.head())  # Display the first few rows\n",
        "        return data\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error: File '{filename}' not found.\")\n",
        "        raise e\n",
        "\n",
        "# Step 9: Process the data for analysis\n",
        "def process_data(input_file, output_file):\n",
        "    \"\"\"\n",
        "    Enhances data with advanced NLP features like:\n",
        "    - Cleaned content.\n",
        "    - Keyword counts.\n",
        "    - Sentiment scores.\n",
        "    - Sentence metadata.\n",
        "    - Citation counts.\n",
        "    \"\"\"\n",
        "    initialize_nltk_resources()  # Step 1: Initialize NLP resources\n",
        "    nlp = initialize_spacy_model()  # Step 2: Load SpaCy model\n",
        "    data = load_input_data(input_file)  # Step 8: Load input data\n",
        "\n",
        "    try:\n",
        "        data.rename(columns={\"Content\": \"Original_Content\"}, inplace=True)  # Ensure consistent column names\n",
        "        # Apply preprocessing and analysis functions\n",
        "        data[\"Cleaned_Content\"] = data[\"Original_Content\"].apply(lambda x: preprocess_text(x, nlp))\n",
        "        data[\"Keyword_Counts\"] = data[\"Original_Content\"].apply(lambda x: extract_keywords(x, nlp))\n",
        "        data[\"Sentiment_Score\"] = data[\"Original_Content\"].apply(calculate_sentiment)\n",
        "        data[\"Citations_Count\"] = data[\"Original_Content\"].apply(count_citations)\n",
        "        data[\"Sentence_Metadata\"] = data[\"Original_Content\"].apply(lambda x: sentence_metadata(x, nlp))\n",
        "        # Add flags based on sentiment and citation thresholds\n",
        "        data[\"Sentiment_Flag\"] = data[\"Sentiment_Score\"].apply(\n",
        "            lambda x: \"Negative\" if x < 0 else \"Positive\" if x > 0 else \"Neutral\"\n",
        "        )\n",
        "        data[\"Citation_Flag\"] = data[\"Citations_Count\"].apply(\n",
        "            lambda x: \"Low Citations\" if x < 5 else \"Sufficient Citations\"\n",
        "        )\n",
        "        # Save the enhanced data\n",
        "        data.to_csv(output_file, index=False)\n",
        "        print(f\"\\nEnhanced data saved to '{output_file}'.\")\n",
        "        print(\"\\nPreview of Enhanced Data:\")\n",
        "        print(data.head())  # Display the processed data\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing data: {e}\")\n",
        "        raise e\n",
        "\n",
        "# Step 10: Run the workflow\n",
        "if __name__ == \"__main__\":\n",
        "    input_file = \"webpage_content.csv\"  # Input CSV file\n",
        "    output_file = \"enhanced_webpage_content.csv\"  # Output CSV file\n",
        "    process_data(input_file, output_file)\n",
        "\n",
        "\n",
        "\n",
        "# Importing necessary libraries\n",
        "import pandas as pd  # For handling tabular data and reading/writing CSV files\n",
        "from transformers import AutoTokenizer, pipeline  # For advanced NLP tasks such as tone analysis\n",
        "from collections import Counter  # To count and manage issues and keyword densities\n",
        "import spacy  # For advanced natural language processing tasks like grammar analysis\n",
        "import json  # For saving the results in JSON format\n",
        "\n",
        "# Step 1: Load Enhanced Data\n",
        "def load_enhanced_data(filename):\n",
        "    \"\"\"\n",
        "    Loads the input CSV file containing cleaned webpage content.\n",
        "    Purpose:\n",
        "        - Converts the CSV into a structured format (DataFrame) for analysis.\n",
        "    Why:\n",
        "        - Structured input ensures efficient processing during analysis.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Reading the input CSV file into a Pandas DataFrame\n",
        "        data = pd.read_csv(filename)\n",
        "        print(f\"Data loaded successfully from '{filename}'.\")\n",
        "        print(\"Preview of the data:\\n\", data.head())  # Display the first few rows to verify content\n",
        "        return data\n",
        "    except FileNotFoundError:\n",
        "        raise Exception(f\"Error: File '{filename}' not found.\")  # Raise an error if the file is missing\n",
        "\n",
        "# Step 2: Initialize NLP Models\n",
        "def initialize_models():\n",
        "    \"\"\"\n",
        "    Initializes the required NLP models:\n",
        "    - Tokenizer: Splits large text into manageable parts.\n",
        "    - Tone Model: Determines the emotional tone of the content.\n",
        "    - SpaCy Model: Analyzes grammar and sentence structure.\n",
        "    Purpose:\n",
        "        - Provides tools for chunking, analyzing tone, and identifying grammatical structures.\n",
        "    \"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")  # Initialize the tokenizer\n",
        "    tone_model = pipeline(\"text-classification\", model=\"bhadresh-savani/distilbert-base-uncased-emotion\")  # Load tone model\n",
        "    spacy_model = spacy.load(\"en_core_web_sm\")  # Load SpaCy's English language model\n",
        "    return tokenizer, tone_model, spacy_model\n",
        "\n",
        "# Step 3: Chunk Text Dynamically with Token Limit Handling\n",
        "def chunk_text_dynamically(text, tokenizer, max_tokens=512):\n",
        "    \"\"\"\n",
        "    Splits long text into smaller chunks that fit within the model's token limit.\n",
        "    Purpose:\n",
        "        - Prevents errors when processing text that exceeds the model's token limit.\n",
        "    \"\"\"\n",
        "    tokens = tokenizer.tokenize(text)  # Tokenize the text into smaller units\n",
        "    chunks = []  # Store the resulting chunks\n",
        "    for i in range(0, len(tokens), max_tokens - 50):  # Split into overlapping chunks to preserve context\n",
        "        chunk_tokens = tokens[i:i + max_tokens - 50]\n",
        "        chunks.append(tokenizer.convert_tokens_to_string(chunk_tokens))  # Convert tokens back into text\n",
        "    return chunks\n",
        "\n",
        "# Step 4: Analyze Each Chunk\n",
        "def analyze_chunk(chunk, tone_model, spacy_model):\n",
        "    \"\"\"\n",
        "    Analyzes a single chunk of text for:\n",
        "    - Tone issues.\n",
        "    - Grammatical structure and sentence complexity.\n",
        "    - Keyword density and potential keyword stuffing.\n",
        "    Purpose:\n",
        "        - Provides detailed insights for each chunk of content.\n",
        "    \"\"\"\n",
        "    issues = Counter()  # Store detected issues\n",
        "    suggestions = set()  # Store actionable suggestions\n",
        "    severity_score = 0  # Overall score indicating the severity of issues in this chunk\n",
        "\n",
        "    # Tone Analysis\n",
        "    try:\n",
        "        truncated_chunk = chunk[:512]  # Ensure the chunk doesn't exceed the model's input limit\n",
        "        tone = tone_model(truncated_chunk)[0]['label'].lower()  # Perform tone analysis\n",
        "        if tone in [\"negative\", \"neutral\"]:  # Identify undesirable tones\n",
        "            issues[\"Negative/neutral tone detected.\"] += 1\n",
        "            suggestions.add(\"Improve the tone to make it more engaging.\")\n",
        "            severity_score += 2  # Assign higher weight to tone issues\n",
        "    except Exception as e:\n",
        "        print(f\"Tone analysis failed: {e}\")  # Handle tone analysis errors gracefully\n",
        "\n",
        "    # Grammar and Sentence Analysis\n",
        "    doc = spacy_model(chunk)  # Analyze text using SpaCy\n",
        "    processed_sentences = set()  # Avoid duplicate sentence processing\n",
        "    for sent in doc.sents:\n",
        "        if sent.text not in processed_sentences:\n",
        "            processed_sentences.add(sent.text)\n",
        "            # Check for passive voice usage\n",
        "            passive_voice_count = sum(1 for token in sent if token.dep_ == \"auxpass\")\n",
        "            if passive_voice_count > 0:\n",
        "                issues[\"Passive voice detected.\"] += passive_voice_count\n",
        "                suggestions.add(\"Rewrite sentences in active voice.\")\n",
        "                severity_score += passive_voice_count  # Weight based on the count of passive constructions\n",
        "            # Check for sentence complexity\n",
        "            if len(sent.text.split()) > 20:\n",
        "                issues[\"Contains long/complex sentences.\"] += 1\n",
        "                suggestions.add(\"Simplify long sentences for better readability.\")\n",
        "                severity_score += 1\n",
        "\n",
        "    # Keyword Density Analysis\n",
        "    words = chunk.split()  # Split the chunk into individual words\n",
        "    keyword_counts = Counter(words)  # Count occurrences of each word\n",
        "    flagged_keywords = {}\n",
        "    for keyword, count in keyword_counts.items():\n",
        "        density = (count / len(words)) * 100  # Calculate keyword density as a percentage\n",
        "        if density > 3:  # Flag keywords with high density\n",
        "            issues[f\"Keyword stuffing detected: {keyword}\"] += 1\n",
        "            flagged_keywords[keyword] = density\n",
        "            suggestions.add(f\"Reduce usage of keyword '{keyword}'.\")\n",
        "            severity_score += 1\n",
        "\n",
        "    return issues, list(suggestions), flagged_keywords, severity_score\n",
        "\n",
        "# Step 5: Aggregate Results\n",
        "def dynamic_analysis(row, tokenizer, tone_model, spacy_model):\n",
        "    \"\"\"\n",
        "    Aggregates analysis results across all chunks of a webpage's content.\n",
        "    Purpose:\n",
        "        - Combines insights to provide a holistic view of the content's quality.\n",
        "    \"\"\"\n",
        "    aggregated_issues = Counter()  # Consolidate issues\n",
        "    aggregated_suggestions = set()  # Consolidate suggestions\n",
        "    keyword_densities = {}  # Store keyword densities\n",
        "    severity_scores = []  # Track severity scores for each chunk\n",
        "\n",
        "    text_chunks = chunk_text_dynamically(row[\"Cleaned_Content\"], tokenizer)\n",
        "    for chunk in text_chunks:\n",
        "        chunk_issues, chunk_suggestions, flagged_keywords, chunk_severity_score = analyze_chunk(chunk, tone_model, spacy_model)\n",
        "        aggregated_issues.update(chunk_issues)\n",
        "        aggregated_suggestions.update(chunk_suggestions)\n",
        "        keyword_densities.update(flagged_keywords)\n",
        "        severity_scores.append(chunk_severity_score)\n",
        "\n",
        "    # Print a summary for the aggregated results\n",
        "    summary = {\n",
        "        \"Total Issues\": sum(aggregated_issues.values()),\n",
        "        \"Severity Score\": sum(severity_scores),\n",
        "        \"Top Issues\": aggregated_issues.most_common(3),\n",
        "        \"Top Suggestions\": list(aggregated_suggestions)[:3]\n",
        "    }\n",
        "    print(\"Summary for Content Analysis:\", summary)\n",
        "\n",
        "    return aggregated_issues, list(aggregated_suggestions), keyword_densities, sum(severity_scores)\n",
        "\n",
        "# Step 6: Save Results\n",
        "def save_results(data, csv_file, json_file):\n",
        "    \"\"\"\n",
        "    Saves the analysis results into CSV and JSON formats.\n",
        "    Purpose:\n",
        "        - Enables the client to access results in user-friendly formats.\n",
        "    \"\"\"\n",
        "    data.to_csv(csv_file, index=False)  # Save to CSV\n",
        "    print(f\"Results saved to CSV file: {csv_file}\")\n",
        "\n",
        "    json_output = data.to_dict(orient=\"records\")  # Convert data to a JSON-compatible structure\n",
        "    with open(json_file, \"w\") as json_file_obj:\n",
        "        json.dump(json_output, json_file_obj, indent=4)  # Save as a JSON file\n",
        "    print(f\"Results saved to JSON file: {json_file_obj.name}\")\n",
        "\n",
        "# Step 7: Process and Save Analysis Results\n",
        "def process_trust_scores(input_file, csv_output_file, json_output_file):\n",
        "    \"\"\"\n",
        "    Processes input content, performs analysis, and saves the results.\n",
        "    Purpose:\n",
        "        - Executes the complete analysis pipeline.\n",
        "    \"\"\"\n",
        "    data = load_enhanced_data(input_file)  # Step 1: Load data\n",
        "    tokenizer, tone_model, spacy_model = initialize_models()  # Step 2: Initialize models\n",
        "\n",
        "    # Apply analysis to each row of content\n",
        "    data[\"Issues\"], data[\"Suggestions\"], data[\"Keyword_Densities\"], data[\"Severity_Score\"] = zip(\n",
        "        *data.apply(lambda row: dynamic_analysis(row, tokenizer, tone_model, spacy_model), axis=1)\n",
        "    )\n",
        "\n",
        "    print(\"\\nPreview of Analysis Results:\")\n",
        "    print(data[[\"Issues\", \"Suggestions\", \"Keyword_Densities\", \"Severity_Score\"]].head())  # Show a preview\n",
        "\n",
        "    save_results(data, csv_output_file, json_output_file)  # Save results\n",
        "\n",
        "# Step 8: Execute Workflow\n",
        "if __name__ == \"__main__\":\n",
        "    input_file = \"enhanced_webpage_content.csv\"  # Input data\n",
        "    csv_output_file = \"kbt_refined_output.csv\"  # CSV output file\n",
        "    json_output_file = \"kbt_refined_output.json\"  # JSON output file\n",
        "    process_trust_scores(input_file, csv_output_file, json_output_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNAblPynHyyT",
        "outputId": "2464ea1c-eda2-490c-844e-c25e2a9ce0ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping URL: https://thatware.co/software-development-services/\n",
            "Scraping URL: https://thatware.co/business-intelligence-services/\n",
            "Scraping URL: https://thatware.co/competitor-keyword-analysis/\n",
            "Data successfully saved to webpage_content.csv.\n",
            "\n",
            "Preview of Scraped Data:\n",
            "\n",
            "                                                 URL  \\\n",
            "0  https://thatware.co/software-development-servi...   \n",
            "1  https://thatware.co/business-intelligence-serv...   \n",
            "2   https://thatware.co/competitor-keyword-analysis/   \n",
            "\n",
            "                                             Content  \n",
            "0  Custom Software Development Services - Softwar...  \n",
            "1  Business Intelligence Services - Competitive A...  \n",
            "2  SEO Competitor Keyword Analysis - Competitor R...  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All necessary NLTK resources downloaded successfully.\n",
            "\n",
            "Loaded data from 'webpage_content.csv'. Preview:\n",
            "                                                 URL  \\\n",
            "0  https://thatware.co/software-development-servi...   \n",
            "1  https://thatware.co/business-intelligence-serv...   \n",
            "2   https://thatware.co/competitor-keyword-analysis/   \n",
            "\n",
            "                                             Content  \n",
            "0  Custom Software Development Services - Softwar...  \n",
            "1  Business Intelligence Services - Competitive A...  \n",
            "2  SEO Competitor Keyword Analysis - Competitor R...  \n",
            "\n",
            "Enhanced data saved to 'enhanced_webpage_content.csv'.\n",
            "\n",
            "Preview of Enhanced Data:\n",
            "                                                 URL  \\\n",
            "0  https://thatware.co/software-development-servi...   \n",
            "1  https://thatware.co/business-intelligence-serv...   \n",
            "2   https://thatware.co/competitor-keyword-analysis/   \n",
            "\n",
            "                                    Original_Content  \\\n",
            "0  Custom Software Development Services - Softwar...   \n",
            "1  Business Intelligence Services - Competitive A...   \n",
            "2  SEO Competitor Keyword Analysis - Competitor R...   \n",
            "\n",
            "                                     Cleaned_Content  \\\n",
            "0  custom software development services software ...   \n",
            "1  business intelligence services competitive ana...   \n",
            "2  seo competitor keyword analysis competitor res...   \n",
            "\n",
            "                                      Keyword_Counts  Sentiment_Score  \\\n",
            "0  {'custom': 32, 'software': 105, 'development':...         0.147029   \n",
            "1  {'business': 35, 'intelligence': 20, 'services...         0.155916   \n",
            "2  {'seo': 108, 'competitor': 34, 'keyword': 55, ...         0.199583   \n",
            "\n",
            "   Citations_Count                                  Sentence_Metadata  \\\n",
            "0               12  [{'sentence': 'Custom Software Development Ser...   \n",
            "1               11  [{'sentence': 'Business Intelligence Services ...   \n",
            "2                7  [{'sentence': 'SEO Competitor Keyword Analysis...   \n",
            "\n",
            "  Sentiment_Flag         Citation_Flag  \n",
            "0       Positive  Sufficient Citations  \n",
            "1       Positive  Sufficient Citations  \n",
            "2       Positive  Sufficient Citations  \n",
            "Data loaded successfully from 'enhanced_webpage_content.csv'.\n",
            "Preview of the data:\n",
            "                                                  URL  \\\n",
            "0  https://thatware.co/software-development-servi...   \n",
            "1  https://thatware.co/business-intelligence-serv...   \n",
            "2   https://thatware.co/competitor-keyword-analysis/   \n",
            "\n",
            "                                    Original_Content  \\\n",
            "0  Custom Software Development Services - Softwar...   \n",
            "1  Business Intelligence Services - Competitive A...   \n",
            "2  SEO Competitor Keyword Analysis - Competitor R...   \n",
            "\n",
            "                                     Cleaned_Content  \\\n",
            "0  custom software development services software ...   \n",
            "1  business intelligence services competitive ana...   \n",
            "2  seo competitor keyword analysis competitor res...   \n",
            "\n",
            "                                      Keyword_Counts  Sentiment_Score  \\\n",
            "0  {'custom': 32, 'software': 105, 'development':...         0.147029   \n",
            "1  {'business': 35, 'intelligence': 20, 'services...         0.155916   \n",
            "2  {'seo': 108, 'competitor': 34, 'keyword': 55, ...         0.199583   \n",
            "\n",
            "   Citations_Count                                  Sentence_Metadata  \\\n",
            "0               12  [{'sentence': 'Custom Software Development Ser...   \n",
            "1               11  [{'sentence': 'Business Intelligence Services ...   \n",
            "2                7  [{'sentence': 'SEO Competitor Keyword Analysis...   \n",
            "\n",
            "  Sentiment_Flag         Citation_Flag  \n",
            "0       Positive  Sufficient Citations  \n",
            "1       Positive  Sufficient Citations  \n",
            "2       Positive  Sufficient Citations  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (3473 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary for Content Analysis: {'Total Issues': 29, 'Severity Score': 29, 'Top Issues': [('Contains long/complex sentences.', 11), ('Keyword stuffing detected: software', 4), ('Keyword stuffing detected: development', 3)], 'Top Suggestions': [\"Reduce usage of keyword 'services'.\", 'Simplify long sentences for better readability.', \"Reduce usage of keyword 'saas'.\"]}\n",
            "Summary for Content Analysis: {'Total Issues': 21, 'Severity Score': 21, 'Top Issues': [('Contains long/complex sentences.', 9), ('Keyword stuffing detected: services', 3), ('Keyword stuffing detected: analysis', 2)], 'Top Suggestions': [\"Reduce usage of keyword 'services'.\", 'Simplify long sentences for better readability.', \"Reduce usage of keyword 'upto'.\"]}\n",
            "Summary for Content Analysis: {'Total Issues': 23, 'Severity Score': 23, 'Top Issues': [('Contains long/complex sentences.', 8), ('Keyword stuffing detected: seo', 3), ('Keyword stuffing detected: services', 3)], 'Top Suggestions': [\"Reduce usage of keyword 'services'.\", \"Reduce usage of keyword 'keyword'.\", 'Simplify long sentences for better readability.']}\n",
            "\n",
            "Preview of Analysis Results:\n",
            "                                              Issues  \\\n",
            "0  {'Contains long/complex sentences.': 11, 'Keyw...   \n",
            "1  {'Contains long/complex sentences.': 9, 'Keywo...   \n",
            "2  {'Contains long/complex sentences.': 8, 'Keywo...   \n",
            "\n",
            "                                         Suggestions  \\\n",
            "0  [Reduce usage of keyword 'services'., Simplify...   \n",
            "1  [Reduce usage of keyword 'services'., Simplify...   \n",
            "2  [Reduce usage of keyword 'services'., Reduce u...   \n",
            "\n",
            "                                   Keyword_Densities  Severity_Score  \n",
            "0  {'custom': 3.132530120481928, 'software': 3.98...              29  \n",
            "1  {'business': 3.5629453681710213, 'services': 3...              21  \n",
            "2  {'seo': 10.043668122270741, 'competitor': 3.90...              23  \n",
            "Results saved to CSV file: kbt_refined_output.csv\n",
            "Results saved to JSON file: kbt_refined_output.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This output provides an **analysis of webpage content** that evaluates various factors to assess its quality, sentiment, readability, and credibility. Here's a step-by-step explanation of each column in the output and how this data can benefit the website owner.\n",
        "\n",
        "---\n",
        "\n",
        "### **Column-by-Column Explanation**\n",
        "\n",
        "1. **`Keyword_Counts`**:\n",
        "   - **What it is**:  \n",
        "     This column shows a dictionary of the most frequently used keywords in the webpage's content. For example:\n",
        "     - Row 0: Keywords like \"custom\" appear 32 times, \"software\" appears 105 times, and \"development\" appears frequently as well.\n",
        "     - Row 1: Keywords like \"business\" (35 times) and \"intelligence\" (20 times) dominate the content.\n",
        "     - Row 2: Keywords like \"SEO\" (108 times), \"competitor\" (34 times), and \"keyword\" (55 times) are the most repeated.\n",
        "   - **Purpose**:  \n",
        "     This helps identify the main focus or theme of the content. Keywords that are repeated too often might indicate \"keyword stuffing,\" which can hurt SEO rankings.\n",
        "   - **Actionable Steps for Website Owners**:  \n",
        "     - Use this data to ensure no keywords are overused or underused.\n",
        "     - Optimize content by spreading keywords naturally throughout the text.\n",
        "     - Consider introducing synonyms or related terms for more variety.\n",
        "\n",
        "---\n",
        "\n",
        "2. **`Sentiment_Score`**:\n",
        "   - **What it is**:  \n",
        "     A numerical value measuring the sentiment or emotional tone of the content. Scores range between -1 (negative tone) to +1 (positive tone).\n",
        "     - Row 0: Score of 0.147029 indicates a mildly positive tone.\n",
        "     - Row 1: Score of 0.155916 suggests a slightly more positive tone.\n",
        "     - Row 2: Score of 0.199583 reflects the most positive tone among the rows.\n",
        "   - **Purpose**:  \n",
        "     This measures how the content might be perceived by readers. A positive sentiment encourages trust and engagement, while a neutral or negative tone might discourage readers.\n",
        "   - **Actionable Steps for Website Owners**:  \n",
        "     - If sentiment scores are low (neutral or negative), rewrite sections to include more positive language.\n",
        "     - Focus on words that convey benefits, trust, and clarity to engage readers better.\n",
        "\n",
        "---\n",
        "\n",
        "3. **`Citations_Count`**:\n",
        "   - **What it is**:  \n",
        "     The number of references or citations (e.g., phrases like \"source,\" \"study,\" or \"report\") found in the content. For example:\n",
        "     - Row 0: Contains 12 citations, showing strong factual backing.\n",
        "     - Row 1: Contains 11 citations, suggesting credibility.\n",
        "     - Row 2: Contains 7 citations, which is comparatively lower but still acceptable.\n",
        "   - **Purpose**:  \n",
        "     Citations enhance the credibility and authority of the content, especially when discussing technical topics or research-based insights.\n",
        "   - **Actionable Steps for Website Owners**:  \n",
        "     - Ensure at least 5 citations in every article to meet \"Sufficient Citations\" criteria.\n",
        "     - Add hyperlinks or references to external credible sources to improve trustworthiness.\n",
        "\n",
        "---\n",
        "\n",
        "4. **`Sentence_Metadata`**:\n",
        "   - **What it is**:  \n",
        "     A detailed breakdown of individual sentences in the content. For each sentence, it shows:\n",
        "       - The text of the sentence.\n",
        "       - The length of the sentence (in words).\n",
        "       - Whether the sentence is written in passive voice.\n",
        "     - Example from Row 0:  \n",
        "       - Sentence: \"Custom Software Development Services.\"  \n",
        "       - Length: 5 words.  \n",
        "       - Passive Voice: No.\n",
        "   - **Purpose**:  \n",
        "     Identifies structural issues, such as long sentences that are hard to read or sentences written in passive voice, which can feel impersonal.\n",
        "   - **Actionable Steps for Website Owners**:  \n",
        "     - Rewrite long sentences (over 20 words) into shorter ones for better readability.\n",
        "     - Convert passive sentences into active voice for a more engaging tone.\n",
        "\n",
        "---\n",
        "\n",
        "5. **`Sentiment_Flag`**:\n",
        "   - **What it is**:  \n",
        "     A simple label for sentiment:\n",
        "       - \"Positive\" for positive sentiment scores.\n",
        "       - \"Neutral\" for scores close to 0.\n",
        "       - \"Negative\" for negative scores.\n",
        "     - All rows in this example are flagged as \"Positive.\"\n",
        "   - **Purpose**:  \n",
        "     Provides a quick, easy-to-understand summary of the sentiment.\n",
        "   - **Actionable Steps for Website Owners**:  \n",
        "     - Ensure all content has a positive sentiment flag.\n",
        "     - If flagged as \"Neutral\" or \"Negative,\" revise to include optimistic, motivating language.\n",
        "\n",
        "---\n",
        "\n",
        "6. **`Citation_Flag`**:\n",
        "   - **What it is**:  \n",
        "     A label indicating whether the content has enough citations:\n",
        "       - \"Sufficient Citations\" for citation counts >= 5.\n",
        "       - \"Low Citations\" for counts < 5.\n",
        "     - All rows in this example are flagged as \"Sufficient Citations.\"\n",
        "   - **Purpose**:  \n",
        "     Ensures content meets credibility standards.\n",
        "   - **Actionable Steps for Website Owners**:  \n",
        "     - Strive to maintain \"Sufficient Citations\" for all pages.\n",
        "     - Add references if any pages are flagged as \"Low Citations.\"\n",
        "\n",
        "---\n",
        "\n",
        "### **What Does This Output Convey?**\n",
        "1. **Content Focus**:\n",
        "   - The `Keyword_Counts` column highlights the main focus of each webpage. For example:\n",
        "     - Row 0 focuses on \"custom software development.\"\n",
        "     - Row 1 emphasizes \"business intelligence services.\"\n",
        "     - Row 2 targets \"SEO competitor keyword analysis.\"\n",
        "   - This helps ensure the content aligns with the intended message and SEO goals.\n",
        "\n",
        "2. **Readability and Tone**:\n",
        "   - Sentiment scores and sentence metadata help measure how approachable and engaging the content is.\n",
        "   - Shorter, active-voice sentences with a positive tone are more likely to keep readers interested.\n",
        "\n",
        "3. **Credibility**:\n",
        "   - Citation counts and flags indicate whether the content is supported by reliable sources, making it trustworthy.\n",
        "\n",
        "---\n",
        "\n",
        "### **How Is This Beneficial for Website Owners?**\n",
        "1. **Improves SEO**:  \n",
        "   - Optimized keywords and positive sentiment enhance search engine rankings.\n",
        "   - Well-referenced content increases the page's authority.\n",
        "\n",
        "2. **Increases Engagement**:  \n",
        "   - Positive sentiment and readability improvements make content more engaging.\n",
        "   - Readers are more likely to trust and share credible, well-structured content.\n",
        "\n",
        "3. **Enhances Conversion Rates**:  \n",
        "   - Clear, positive, and factual content can convert casual readers into customers or clients.\n",
        "\n",
        "---\n",
        "\n",
        "### **Steps to Take After Getting This Output**\n",
        "1. **Review the Keywords**:  \n",
        "   - Check `Keyword_Counts` for overused or underused keywords.\n",
        "   - Balance keyword usage to prevent keyword stuffing penalties.\n",
        "\n",
        "2. **Edit Sentences**:  \n",
        "   - Use `Sentence_Metadata` to identify long or passive sentences.\n",
        "   - Rewrite them for clarity and engagement.\n",
        "\n",
        "3. **Add More Citations (if needed)**:  \n",
        "   - Ensure `Citation_Flag` remains \"Sufficient Citations.\"\n",
        "   - Add more references if the flag ever shows \"Low Citations.\"\n",
        "\n",
        "4. **Boost Sentiment**:  \n",
        "   - If any content has a negative or neutral sentiment, revise it with more positive language to improve engagement.\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary**\n",
        "This output is a **comprehensive analysis** of webpage content that highlights:\n",
        "- Keyword focus.\n",
        "- Tone and sentiment.\n",
        "- Readability and structure.\n",
        "- Credibility through citations.\n",
        "\n",
        "Using this data, website owners can enhance their content for better audience engagement, improved trustworthiness, and higher search engine rankings."
      ],
      "metadata": {
        "id": "MA4aGNJ2TO5p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Detailed Explanation of the Output:\n",
        "\n",
        "This output is a **content analysis report** designed to help website owners improve their webpage content for readability, SEO (Search Engine Optimization), and user engagement.\n",
        "\n",
        "---\n",
        "\n",
        "### **Understanding the Output Columns**\n",
        "\n",
        "#### 1. **`Summary for Content Analysis`**:\n",
        "   - **What it is**:  \n",
        "     A concise summary for each webpage, including:\n",
        "       - **`Total Issues`**: The number of issues identified in the webpage's content.\n",
        "       - **`Severity Score`**: A score based on the significance of the issues, where higher scores indicate more serious problems.\n",
        "       - **`Top Issues`**: A list of the most common problems in the content.\n",
        "       - **`Top Suggestions`**: Actionable advice to resolve these issues.\n",
        "\n",
        "   - **Example (Row 0)**:\n",
        "     - **`Total Issues`**: 29 (indicating the content has significant room for improvement).\n",
        "     - **`Severity Score`**: 29 (severity of the issues matches the number of issues found).\n",
        "     - **`Top Issues`**:\n",
        "       - **Contains long/complex sentences**: There are 11 sentences that are too long or complicated.\n",
        "       - **Keyword stuffing detected: software**: The keyword \"software\" appears excessively.\n",
        "       - **Keyword stuffing detected: development**: The keyword \"development\" is overused.\n",
        "     - **`Top Suggestions`**:\n",
        "       - Reduce the usage of keywords like \"services\" and \"saas\" to avoid keyword stuffing.\n",
        "       - Simplify long sentences to improve readability.\n",
        "\n",
        "   - **Why It’s Important**:  \n",
        "     This helps website owners prioritize improvements based on the most critical issues.\n",
        "\n",
        "   - **Actions to Take**:\n",
        "     - Rewrite sentences to make them shorter and easier to understand.\n",
        "     - Reduce the repetition of overused keywords to avoid being penalized by search engines.\n",
        "     - Follow the suggestions to ensure your content is engaging and SEO-friendly.\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. **`Issues`**:\n",
        "   - **What it is**:  \n",
        "     A detailed breakdown of all issues detected in the content, with counts for each type.\n",
        "   - **Example (Row 0)**:\n",
        "     ```json\n",
        "     {\n",
        "       \"Contains long/complex sentences.\": 11,\n",
        "       \"Keyword stuffing detected: software\": 4,\n",
        "       \"Keyword stuffing detected: development\": 3\n",
        "     }\n",
        "     ```\n",
        "     - **Contains long/complex sentences**: 11 sentences are too lengthy or complex.\n",
        "     - **Keyword stuffing detected: software**: The keyword \"software\" is overused 4 times.\n",
        "     - **Keyword stuffing detected: development**: The keyword \"development\" appears too frequently (3 times).\n",
        "\n",
        "   - **Why It’s Important**:  \n",
        "     Long or complex sentences make content hard to read, and keyword stuffing can harm SEO rankings.\n",
        "\n",
        "   - **Actions to Take**:\n",
        "     - Focus on simplifying complex sentences to improve readability.\n",
        "     - Limit keyword usage to a natural level to avoid search engine penalties.\n",
        "\n",
        "---\n",
        "\n",
        "#### 3. **`Suggestions`**:\n",
        "   - **What it is**:  \n",
        "     A list of recommendations to address the identified issues.\n",
        "   - **Example (Row 0)**:\n",
        "     ```json\n",
        "     [\n",
        "       \"Reduce usage of keyword 'services'.\",\n",
        "       \"Simplify long sentences for better readability.\",\n",
        "       \"Reduce usage of keyword 'saas'.\"\n",
        "     ]\n",
        "     ```\n",
        "     - Suggestion to reduce the use of specific keywords to make the content more balanced.\n",
        "     - Suggestion to rewrite long sentences for improved clarity.\n",
        "\n",
        "   - **Why It’s Important**:  \n",
        "     These are actionable steps to make content more engaging, readable, and SEO-friendly.\n",
        "\n",
        "   - **Actions to Take**:\n",
        "     - Follow the suggestions directly. For example:\n",
        "       - Identify sentences with keywords like \"services\" or \"saas\" and replace or reduce them.\n",
        "       - Break long sentences into shorter, simpler ones.\n",
        "\n",
        "---\n",
        "\n",
        "#### 4. **`Keyword_Densities`**:\n",
        "   - **What it is**:  \n",
        "     A detailed frequency count of keywords used in the content and their percentage density.\n",
        "   - **Example (Row 0)**:\n",
        "     ```json\n",
        "     {\n",
        "       \"custom\": 3.132530120481928,\n",
        "       \"software\": 3.980582524271845,\n",
        "       \"development\": 2.9125\n",
        "     }\n",
        "     ```\n",
        "     - The keyword \"software\" makes up 3.98% of the content.\n",
        "     - The keyword \"custom\" contributes 3.13%.\n",
        "\n",
        "   - **Why It’s Important**:  \n",
        "     Keyword density helps ensure that the content is optimized for search engines without crossing into keyword stuffing.\n",
        "\n",
        "   - **Actions to Take**:\n",
        "     - Maintain a keyword density of **1-3%** for critical terms.\n",
        "     - Replace repetitive keywords with synonyms to reduce density if it exceeds 3%.\n",
        "\n",
        "---\n",
        "\n",
        "#### 5. **`Severity_Score`**:\n",
        "   - **What it is**:  \n",
        "     A numeric score representing the severity of issues in the content. It is calculated based on the number and type of issues.\n",
        "   - **Example**:\n",
        "     - Row 0: **29** (high severity, indicating significant improvement is needed).\n",
        "     - Row 1: **21** (moderate severity).\n",
        "     - Row 2: **23** (moderate severity).\n",
        "   - **Why It’s Important**:  \n",
        "     Helps prioritize which pages need the most attention.\n",
        "\n",
        "   - **Actions to Take**:\n",
        "     - Focus on pages with higher severity scores first, as they require more work to meet quality standards.\n",
        "\n",
        "---\n",
        "\n",
        "### **What Does This Output Convey?**\n",
        "\n",
        "1. **Content Quality**:  \n",
        "   - It provides a detailed view of where your content excels (e.g., positive sentiment, sufficient citations) and where it needs improvement (e.g., long sentences, keyword stuffing).\n",
        "\n",
        "2. **Readability and Engagement**:  \n",
        "   - Long or passive sentences and excessive keyword usage can hurt readability. The suggestions focus on making the content clear and engaging.\n",
        "\n",
        "3. **SEO Optimization**:  \n",
        "   - Overused keywords and poor readability can negatively affect your search engine rankings. The output gives actionable insights to fix these issues.\n",
        "\n",
        "4. **Credibility**:  \n",
        "   - High citation counts ensure your content is factual and trustworthy.\n",
        "\n",
        "---\n",
        "\n",
        "### **How Is This Beneficial for Website Owners?**\n",
        "\n",
        "1. **Improved SEO Rankings**:  \n",
        "   - Balancing keyword usage and improving readability can help your content rank higher on search engines.\n",
        "\n",
        "2. **Better User Experience**:  \n",
        "   - Clearer, concise content is more likely to engage readers and reduce bounce rates.\n",
        "\n",
        "3. **Increased Trust**:  \n",
        "   - Sufficient citations and a positive tone make your content more credible and appealing to users.\n",
        "\n",
        "---\n",
        "\n",
        "### **Steps to Take After Getting This Output**\n",
        "\n",
        "1. **Simplify Content**:  \n",
        "   - Use the suggestions to rewrite long or complex sentences and remove passive voice.\n",
        "\n",
        "2. **Balance Keyword Usage**:  \n",
        "   - Adjust keyword densities to fall within the recommended range (1-3%).\n",
        "\n",
        "3. **Enhance Sentiment**:  \n",
        "   - Rewrite any content flagged with neutral or negative sentiment to make it more positive and engaging.\n",
        "\n",
        "4. **Validate Citations**:  \n",
        "   - Ensure all facts and references are accurate and add more credible sources if needed.\n",
        "\n",
        "5. **Prioritize Pages**:  \n",
        "   - Start with the pages that have the highest severity scores and fix the critical issues first.\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "This output provides actionable insights into how to improve the quality, readability, and SEO performance of your webpage content. By following the recommendations, website owners can create engaging, trustworthy, and search-engine-optimized content."
      ],
      "metadata": {
        "id": "TTRlFZBhThjV"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM0saoL1kWKr+rFa1XOp9wg",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}