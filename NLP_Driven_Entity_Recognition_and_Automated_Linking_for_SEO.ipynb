{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPDqWoEe+/r2DyVhBJjk/eJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abhiss123/AlmaBetter-Projects/blob/main/NLP_Driven_Entity_Recognition_and_Automated_Linking_for_SEO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name : NLP-Driven: Entity Recognition and Automated Linking for SEO**\n",
        "\n",
        "### **Purpose of the Project:**\n",
        "\n",
        "This project is a solution designed to enhance websites and businesses by using **Natural Language Processing (NLP)** to identify key terms (called \"entities\") from web content and automatically link them to relevant information on the internet. It helps businesses improve their **Search Engine Optimization (SEO)** by making their content more meaningful, user-friendly, and searchable.\n",
        "\n",
        "---\n",
        "\n",
        "### **What is This Project About?**\n",
        "\n",
        "1. **Entity Recognition**:\n",
        "   - The project reads content from a website and identifies important terms, such as:\n",
        "     - Names of companies (like \"Google\" or \"SEMrush\").\n",
        "     - Locations (like \"India\" or \"Dubai\").\n",
        "     - Concepts or technologies (like \"NLP\" or \"XML\").\n",
        "   - These terms are called **entities**, and they are categorized based on their type:\n",
        "     - **ORG**: Organizations like businesses or companies.\n",
        "     - **GPE**: Locations such as cities or countries.\n",
        "     - **PERSON**: Names of individuals.\n",
        "\n",
        "2. **Automated Linking**:\n",
        "   - Once the entities are recognized, the project automatically finds **relevant links** for these entities from the web.\n",
        "   - For example:\n",
        "     - \"Google\" links to \"https://duckduckgo.com/c/Google\".\n",
        "     - \"NLP\" links to \"https://duckduckgo.com/Natural_language_processing\".\n",
        "   - These links provide more information to users, making the content interactive and informative.\n",
        "\n",
        "3. **Annotated Text**:\n",
        "   - The project transforms regular website content into **annotated text**, where the recognized entities are highlighted and clickable.\n",
        "   - For example:\n",
        "     - In the text: \"Google is a popular search engine,\" the word \"Google\" will be a clickable link to its webpage.\n",
        "\n",
        "4. **Structured Data**:\n",
        "   - Apart from annotated text, the project generates a **structured file** (like a CSV or JSON) listing all entities, their types, and the links. This structured data can be used for further analysis, marketing, or SEO strategies.\n",
        "\n",
        "---\n",
        "\n",
        "### **Use Cases and Importance**\n",
        "\n",
        "1. **Improving SEO**:\n",
        "   - By linking key terms to relevant information, search engines like Google can better understand the content.\n",
        "   - This increases the chances of the website appearing higher in search results, attracting more visitors.\n",
        "\n",
        "2. **Enhancing User Experience**:\n",
        "   - Readers can click on highlighted terms to learn more, making the content engaging and educational.\n",
        "   - For example, a user reading about \"NLP\" can click the link and directly learn about it.\n",
        "\n",
        "3. **Time-Saving Automation**:\n",
        "   - Manually finding and adding links to important terms is time-consuming. This project automates the process, saving effort and ensuring accuracy.\n",
        "\n",
        "4. **Content Analysis**:\n",
        "   - Businesses can use the structured data output to analyze what entities are frequently mentioned on their website.\n",
        "   - They can identify trends, improve their content, and align it with user interests.\n",
        "\n",
        "5. **Marketing and Strategy**:\n",
        "   - The data can be used to target specific topics, regions, or trends.\n",
        "   - For instance, if \"Dubai\" is frequently mentioned, the business can focus its marketing efforts on the Dubai audience.\n",
        "\n",
        "---\n",
        "\n",
        "### **Why is This Important?**\n",
        "\n",
        "1. **For Website Owners**:\n",
        "   - Makes their content more visible, engaging, and relevant.\n",
        "   - Increases traffic and conversions (e.g., sales, sign-ups).\n",
        "\n",
        "2. **For Search Engines**:\n",
        "   - Provides clear and structured information about the content.\n",
        "   - Helps search engines match the content with user queries more effectively.\n",
        "\n",
        "3. **For Users**:\n",
        "   - Makes it easier to find additional information without leaving the page.\n",
        "   - Enhances the learning experience by providing relevant links instantly.\n",
        "\n",
        "---\n",
        "\n",
        "### **What Next?**\n",
        "After generating the annotated text and structured data:\n",
        "- **Website Integration**: Embed the annotated text on the website for users to interact with.\n",
        "- **SEO Analytics**: Use the structured data to identify trends and optimize content strategy.\n",
        "- **Link Verification**: Ensure that all links are relevant and functional, refining them as needed.\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "This project is a powerful tool for businesses to enhance their websites. By combining advanced NLP techniques with practical SEO strategies, it improves content relevance, user engagement, and online visibility. It saves time, provides valuable insights, and makes the website more effective for both users and search engines.\n",
        "\n"
      ],
      "metadata": {
        "id": "sTRJTJtGw8mw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# **1. What is Entity Recognition and Linking (ERL)?**\n",
        "\n",
        "Entity Recognition and Linking (ERL) is a technique used in natural language processing (NLP). It helps identify \"entities\" (specific names or phrases) in text, such as:\n",
        "\n",
        "- **People** (e.g., \"Elon Musk\")\n",
        "- **Places** (e.g., \"New York City\")\n",
        "- **Organizations** (e.g., \"Tesla Inc.\")\n",
        "\n",
        "Once identified, these entities are \"linked\" to authoritative sources or databases (like Wikipedia, Wikidata, or a specific website). This improves the content's relevance and authority for search engines like Google.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Use Cases of ERL in SEO**\n",
        "\n",
        "Entity Recognition and Linking can significantly enhance SEO by:\n",
        "\n",
        "- **Improving Content Relevance**: By tagging key entities and linking them to credible sources, search engines understand the content better, making it rank higher.\n",
        "- **Boosting Content Authority**: Linking entities to trusted sources boosts a website's credibility in Google's algorithm.\n",
        "- **Enhancing User Experience**: Providing links to additional resources enriches the reader's experience.\n",
        "- **Optimizing Snippets**: Search engines may extract better snippets (the summaries shown in search results).\n",
        "- **Semantic Search Optimization**: ERL ensures that content aligns with semantic search, where Google interprets the meaning behind queries.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Real-Life Implementations of ERL**\n",
        "\n",
        "ERL is widely used in the following scenarios:\n",
        "\n",
        "1. **News Websites**: Identifying and linking names, places, and events for credibility (e.g., linking \"Joe Biden\" to his official Wikipedia page).\n",
        "2. **E-commerce Sites**: Linking products or brands to relevant details (e.g., linking \"Nike Air Max\" to its product page).\n",
        "3. **Blogs and Informational Sites**: Adding context to topics by linking entities to knowledge bases like Wikipedia.\n",
        "4. **Travel Websites**: Highlighting and linking destinations or tourist attractions (e.g., \"Eiffel Tower\" linked to its official site).\n",
        "5. **Health and Education Sites**: Linking medical terms, institutions, or study materials to authoritative sources.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Use Case of ERL for Websites**\n",
        "\n",
        "For a website, ERL can:\n",
        "\n",
        "- Analyze its content (like blogs, product descriptions, or news articles).\n",
        "- Identify important entities within the text.\n",
        "- Link these entities to authoritative or relevant URLs (internal links within the website or external links).\n",
        "\n",
        "This makes the website more SEO-friendly, as search engines prefer structured and authoritative content.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. How Does ERL Work for Websites?**\n",
        "\n",
        "For a website project, the ERL process typically involves the following steps:\n",
        "\n",
        "1. **Input Data**:\n",
        "   - ERL can work directly with the website's URLs or with a **CSV file** containing text data (e.g., articles, product descriptions, or meta content).\n",
        "   - If using URLs, the model will fetch the webpage content and process it.\n",
        "   - If using a CSV, the text must already be prepared (e.g., exported blog text).\n",
        "\n",
        "2. **Preprocessing**:\n",
        "   - Extract the text content from web pages or CSV files.\n",
        "   - Clean and structure the data (e.g., remove HTML tags or special characters).\n",
        "\n",
        "3. **Entity Recognition**:\n",
        "   - The model scans the text to identify entities such as names, places, and organizations.\n",
        "\n",
        "4. **Entity Linking**:\n",
        "   - These entities are linked to their appropriate sources (e.g., Wikipedia or internal pages on the website).\n",
        "\n",
        "5. **Output**:\n",
        "   - Updated text with linked entities.\n",
        "   - Additional data like recognized entities and their link URLs in a structured format (e.g., JSON or CSV).\n",
        "\n",
        "---\n",
        "\n",
        "### **6. What Data Does the Model Need?**\n",
        "\n",
        "The model needs the following inputs:\n",
        "\n",
        "- **Text Content**:\n",
        "  - Can come from website URLs (web scraping) or pre-prepared CSV files containing textual data.\n",
        "- **Entity Database**:\n",
        "  - A knowledge base like Wikipedia, Wikidata, or a custom database.\n",
        "- **Linking Rules**:\n",
        "  - Internal links (within the same site) or external links (to trusted sites).\n",
        "\n",
        "---\n",
        "\n",
        "### **7. Output of ERL Model**\n",
        "\n",
        "The ERL model typically provides:\n",
        "\n",
        "1. **Annotated Text**:\n",
        "   - The original text with entities highlighted and linked.\n",
        "   - Example: \"Tesla Inc. was founded by Elon Musk in California.\"\n",
        "     - `Tesla Inc.` → [Link to Tesla's page]\n",
        "     - `Elon Musk` → [Link to Elon Musk's page]\n",
        "     - `California` → [Link to California's page]\n",
        "\n",
        "2. **Structured Data**:\n",
        "   - A file (CSV, JSON, etc.) listing:\n",
        "     - Recognized entities.\n",
        "     - The type of entity (person, place, organization).\n",
        "     - Links associated with each entity.\n",
        "\n",
        "---\n",
        "\n",
        "### **8. Why Does ERL Matter for SEO?**\n",
        "\n",
        "- **Improves Search Rankings**: Google recognizes well-structured, linked content as more authoritative.\n",
        "- **Increases Traffic**: Users find it easier to navigate, leading to more engagement.\n",
        "- **Boosts Domain Authority**: Linking to and being linked by other credible sources builds trust.\n",
        "- **Supports Semantic Search**: Helps Google understand the \"context\" of content better.\n",
        "\n",
        "---\n",
        "\n",
        "### **9. Example Use Case for Your Website Project**\n",
        "\n",
        "If you're working on a blog site, here's how ERL would work:\n",
        "\n",
        "1. **Input**:\n",
        "   - Provide URLs of the blogs or a CSV file containing blog text.\n",
        "2. **Processing**:\n",
        "   - The model identifies entities in the blog (e.g., famous authors, cities, book names).\n",
        "   - It links these to relevant resources.\n",
        "3. **Output**:\n",
        "   - Enhanced blogs with hyperlinks to authoritative sources.\n",
        "   - CSV with details of all linked entities.\n",
        "\n",
        "---\n",
        "\n",
        "### **10. Steps to Implement ERL**\n",
        "\n",
        "1. **Prepare Input Data**:\n",
        "   - Export website data (as URLs or CSV).\n",
        "2. **Use ERL Tools/Models**:\n",
        "   - Tools like SpaCy, Hugging Face Transformers, or custom ERL models.\n",
        "3. **Integrate Output into Website**:\n",
        "   - Update the website content with linked entities.\n",
        "\n",
        "---\n",
        "\n",
        "### **Final Summary**\n",
        "\n",
        "- **Entity Recognition**: Finds important terms (names, places, organizations) in text.\n",
        "- **Entity Linking**: Links those terms to credible sources.\n",
        "- **Data Required**: Website URLs or CSV containing content, and access to a knowledge base.\n",
        "- **Output**: Enhanced text with hyperlinks or structured files for updates.\n",
        "- **Benefits**: Improves SEO, user engagement, and authority."
      ],
      "metadata": {
        "id": "uRu8z7C6yBWk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# **Part 1: Webpage Content Scraper and Cleaner**\n",
        "**Purpose**: To scrape raw textual content from webpages and clean it for further processing.\n",
        "\n",
        "#### Key Features:\n",
        "1. **Scrape Web Content**:\n",
        "   - Uses `requests` to fetch webpage HTML content.\n",
        "   - Extracts the main content (usually paragraphs) using `BeautifulSoup`.\n",
        "\n",
        "2. **Clean Text**:\n",
        "   - Removes irrelevant information such as:\n",
        "     - Numbers.\n",
        "     - Special characters (e.g., punctuation marks).\n",
        "     - Common, meaningless words (stopwords) like \"the\" or \"and\".\n",
        "   - Keeps track of everything removed in a log file for transparency.\n",
        "\n",
        "3. **Output**:\n",
        "   - Saves cleaned text in a structured JSON file (`cleaned_webpage_texts.json`).\n",
        "   - Saves a log of all removed elements (`removed_log.json`) for review.\n",
        "\n",
        "**Example Use**:\n",
        "- Input: A URL like `https://thatware.co/advanced-seo-services/`.\n",
        "- Output: Cleaned and concise text content from the page, ready for analysis.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "bAogTWodqJsd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests  # Used to send HTTP requests to web pages\n",
        "from bs4 import BeautifulSoup  # For parsing HTML content from web pages\n",
        "import json  # For saving data in a structured JSON format\n",
        "import re  # For performing pattern-based cleaning of text\n",
        "\n",
        "# Step 1: Define the URLs of web pages to scrape\n",
        "# These are the web pages from which we want to extract meaningful text content.\n",
        "urls = [\n",
        "    'https://thatware.co/advanced-seo-services/'\n",
        "]\n",
        "\n",
        "# Step 2: Define cleaning rules\n",
        "# STOPWORDS: These are common words that do not add value to the context (e.g., \"the\", \"and\").\n",
        "# UNNECESSARY_PATTERNS: These are patterns that match irrelevant text (e.g., numbers, special characters).\n",
        "STOPWORDS = set([\"approximately\", \"today\", \"months\", \"years\", \"the\", \"and\", \"or\", \"but\"])\n",
        "UNNECESSARY_PATTERNS = [\n",
        "    r'\\b\\d{2,}\\b',  # Matches standalone numbers with two or more digits, e.g., \"2501\"\n",
        "    r'[^\\w\\s]',     # Matches special characters like punctuation marks, e.g., \"@\" or \"$\"\n",
        "]\n",
        "\n",
        "# Step 3: Initialize a log to track removed words/patterns\n",
        "# This will help us review what was removed during the cleaning process.\n",
        "removed_log = []\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Cleans the raw text extracted from web pages.\n",
        "\n",
        "    Why this function is important:\n",
        "    - Web pages often contain noise, such as numbers, stopwords, and special characters.\n",
        "    - Cleaning ensures the data is concise, relevant, and ready for further processing.\n",
        "\n",
        "    Args:\n",
        "        text (str): The raw text to clean.\n",
        "\n",
        "    Returns:\n",
        "        str: The cleaned version of the text.\n",
        "    \"\"\"\n",
        "    global removed_log  # Use a global log to keep track of removed items\n",
        "\n",
        "    # Step 3.1: Normalize spaces\n",
        "    # Removes excessive spaces (e.g., double spaces) to ensure uniform formatting.\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Step 3.2: Remove unnecessary patterns like numbers and special characters\n",
        "    for pattern in UNNECESSARY_PATTERNS:\n",
        "        # Apply the cleaning pattern and log changes\n",
        "        new_text = re.sub(pattern, '', text)\n",
        "        if new_text != text:  # If text changes, log the original and cleaned version\n",
        "            removed_log.append((text, new_text))\n",
        "        text = new_text\n",
        "\n",
        "    # Step 3.3: Remove stopwords\n",
        "    # Split text into words, filter out stopwords, and log removed words\n",
        "    words = text.split()\n",
        "    cleaned_words = []\n",
        "    for word in words:\n",
        "        if word.lower() not in STOPWORDS:  # Retain words not in stopwords\n",
        "            cleaned_words.append(word)\n",
        "        else:  # Log removed stopwords for review\n",
        "            removed_log.append((word, \"REMOVED\"))\n",
        "\n",
        "    # Join the remaining words into a cleaned text string\n",
        "    return ' '.join(cleaned_words)\n",
        "\n",
        "def fetch_webpage_text(url):\n",
        "    \"\"\"\n",
        "    Fetches the text content of a web page and applies cleaning.\n",
        "\n",
        "    Why this function is important:\n",
        "    - Web pages often contain HTML and irrelevant sections. This function extracts only the main content.\n",
        "    - It ensures that the extracted text is clean and usable for further processing.\n",
        "\n",
        "    Args:\n",
        "        url (str): The URL of the web page to scrape.\n",
        "\n",
        "    Returns:\n",
        "        str: The cleaned text content of the page.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Step 4.1: Send an HTTP GET request to fetch the web page content\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Raise an error if the HTTP request fails\n",
        "\n",
        "        # Step 4.2: Parse the HTML content using BeautifulSoup\n",
        "        # BeautifulSoup makes it easy to extract specific parts of an HTML page, like paragraphs.\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Step 4.3: Extract text from paragraph (<p>) tags\n",
        "        # Paragraphs usually contain the main textual content of a web page.\n",
        "        paragraphs = soup.find_all('p')\n",
        "        text_content = ' '.join([para.get_text().strip() for para in paragraphs])\n",
        "\n",
        "        # Step 4.4: Clean the extracted text\n",
        "        return clean_text(text_content)\n",
        "\n",
        "    except Exception as e:\n",
        "        # Log an error message if something goes wrong while fetching the page\n",
        "        print(f\"Error fetching {url}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# Step 5: Scrape and clean the content from each URL\n",
        "# This dictionary will store the URL as the key and the cleaned text as the value.\n",
        "webpage_texts = {}\n",
        "for url in urls:\n",
        "    print(f\"Scraping URL: {url}\")  # Notify the user which URL is being processed\n",
        "    text = fetch_webpage_text(url)  # Fetch and clean the text\n",
        "    if text:  # If text was successfully extracted and cleaned\n",
        "        webpage_texts[url] = text  # Add the URL-to-text mapping to the dictionary\n",
        "        print(f\"Cleaned Preview:\\n{text[:500]}\\n{'='*80}\")  # Display a short preview\n",
        "\n",
        "# Step 6: Save the cleaned data to a JSON file\n",
        "# JSON is a structured format, making it easy to save and reuse the cleaned data later.\n",
        "output_file = 'cleaned_webpage_texts.json'\n",
        "with open(output_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(webpage_texts, f, ensure_ascii=False, indent=4)\n",
        "print(f\"Cleaning completed. Cleaned data saved to '{output_file}'.\")\n",
        "\n",
        "# Step 7: Save the log of removed items for review\n",
        "# This allows you to see exactly what was removed and ensure that nothing important was lost.\n",
        "removed_log_file = 'removed_log.json'\n",
        "with open(removed_log_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(removed_log, f, ensure_ascii=False, indent=4)\n",
        "print(f\"Removed items logged in '{removed_log_file}'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KKLwfzbBUCF",
        "outputId": "860406aa-316c-4033-85a0-2aa13277b004"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping URL: https://thatware.co/advanced-seo-services/\n",
            "Cleaned Preview:\n",
            "In a rapidly evolving digital landscape importance of a robust online presence cannot be overstated internet has become goto platform for businesses both small large seeking exposure recognition ultimately success Its no longer a matter of choice its a matter of survival If your company doesnt adapt embrace advanced search engine optimization SEO youre not just standing still youre falling behind what does advanced SEO entail how can it shape destiny of your business These are questions that Tha\n",
            "================================================================================\n",
            "Cleaning completed. Cleaned data saved to 'cleaned_webpage_texts.json'.\n",
            "Removed items logged in 'removed_log.json'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Explanation of the Output**\n",
        "\n",
        "The output shown above is the result of **Part 1: Webpage Content Scraper and Cleaner**.\n",
        "\n",
        "---\n",
        "\n",
        "#### **1. Scraping the URL**\n",
        "**Line:**\n",
        "```\n",
        "Scraping URL: https://thatware.co/advanced-seo-services/\n",
        "```\n",
        "\n",
        "**What this means:**\n",
        "- The program has started processing the webpage at the provided URL (`https://thatware.co/advanced-seo-services/`).\n",
        "- The purpose of this step is to fetch the textual content from the webpage, such as paragraphs or other readable content.\n",
        "\n",
        "**Why this is important:**\n",
        "- Websites often have a mix of useful content (like blog articles) and noise (like ads or menus). This program extracts only the useful text.\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. Cleaned Preview**\n",
        "**Line:**\n",
        "```\n",
        "Cleaned Preview:\n",
        "In a rapidly evolving digital landscape importance of a robust online presence cannot be overstated internet has become goto platform for businesses both small large seeking exposure recognition ultimately success Its no longer a matter of choice its a matter of survival If your company doesnt adapt embrace advanced search engine optimization SEO youre not just standing still youre falling behind what does advanced SEO entail how can it shape destiny of your business These are questions that Tha\n",
        "================================================================================\n",
        "```\n",
        "\n",
        "**What this means:**\n",
        "- This is a preview of the cleaned text extracted from the webpage.\n",
        "- The program has removed unnecessary content such as:\n",
        "  - Special characters (like commas, periods, or question marks).\n",
        "  - Common filler words (like \"the\", \"and\", \"or\").\n",
        "  - Irrelevant sections (like numbers, extra spaces, or symbols).\n",
        "  \n",
        "**Why this is important:**\n",
        "- This ensures that the text is concise, relevant, and ready for further processing.\n",
        "- For example, the text focuses on discussing \"advanced SEO\" and its importance for businesses without including distractions.\n",
        "\n",
        "**What is shown in the preview:**\n",
        "- The extracted content explains why businesses need a strong online presence.\n",
        "- It emphasizes how \"Advanced SEO\" can help businesses grow and succeed in the digital landscape.\n",
        "- The preview cuts off after a certain length (to fit the console view), but the full cleaned text is saved in the output file.\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. Cleaning Completed**\n",
        "**Line:**\n",
        "```\n",
        "Cleaning completed. Cleaned data saved to 'cleaned_webpage_texts.json'.\n",
        "```\n",
        "\n",
        "**What this means:**\n",
        "- The cleaning process has finished successfully.\n",
        "- The cleaned text from the webpage is saved in a file named `cleaned_webpage_texts.json`.\n",
        "\n",
        "**Why this is important:**\n",
        "- The `cleaned_webpage_texts.json` file contains the complete cleaned text from all processed URLs. This file can now be used in the next steps of the project, such as identifying meaningful entities.\n",
        "\n",
        "**What the file contains:**\n",
        "- It is a structured file that links the webpage URL to the cleaned content. For example:\n",
        "  ```json\n",
        "  {\n",
        "    \"https://thatware.co/advanced-seo-services/\": \"In a rapidly evolving digital landscape importance of a robust online presence...\"\n",
        "  }\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "#### **4. Removed Items Log**\n",
        "**Line:**\n",
        "```\n",
        "Removed items logged in 'removed_log.json'.\n",
        "```\n",
        "\n",
        "**What this means:**\n",
        "- All the content that was removed during the cleaning process is logged in a separate file called `removed_log.json`.\n",
        "\n",
        "**Why this is important:**\n",
        "- The log provides transparency. You can review what was removed (e.g., stopwords, special characters, or unnecessary patterns) to ensure that no important information was accidentally discarded.\n",
        "\n",
        "**What the file contains:**\n",
        "- It lists items that were removed during cleaning. For example:\n",
        "  ```json\n",
        "  [\n",
        "    [\"approximately\", \"REMOVED\"],\n",
        "    [\"2501\", \"\"],\n",
        "    [\"@domain\", \"\"]\n",
        "  ]\n",
        "  ```\n",
        "- This helps ensure that the cleaning process is trustworthy.\n",
        "\n",
        "\n",
        "\n",
        "### **Why This Output is Useful**\n",
        "- This output prepares the textual data for further analysis (like identifying entities in Part 2).\n",
        "- It ensures that only the most relevant and meaningful content is retained from the webpage.\n",
        "- By removing noise (like stopwords or punctuation), the data becomes more concise and easier to process.\n"
      ],
      "metadata": {
        "id": "FfgWzOfqrUNX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# **Part 2: Entity Recognition and Deduplication**\n",
        "**Purpose**: To identify meaningful entities (like organizations, places, or people) in the cleaned text and filter duplicates.\n",
        "\n",
        "#### Key Features:\n",
        "1. **Entity Extraction**:\n",
        "   - Uses `SpaCy`, a Natural Language Processing library, to recognize entities such as:\n",
        "     - Organizations (`ORG`).\n",
        "     - People (`PERSON`).\n",
        "     - Locations (`GPE`).\n",
        "     - Products, dates, and more.\n",
        "\n",
        "2. **Deduplication**:\n",
        "   - If the same entity appears multiple times in different contexts, it keeps the highest-priority type (e.g., prioritizing `ORG` over `PERSON`).\n",
        "\n",
        "3. **Output**:\n",
        "   - A structured JSON file (`deduplicated_entities.json`) containing unique entities grouped by URLs.\n",
        "\n",
        "**Example Use**:\n",
        "- Input: Cleaned text from Part 1.\n",
        "- Output: A list of meaningful entities like:\n",
        "  ```json\n",
        "  {\n",
        "    \"https://thatware.co/advanced-seo-services/\": [\n",
        "      {\"entity\": \"Google\", \"type\": \"ORG\"},\n",
        "      {\"entity\": \"India\", \"type\": \"GPE\"}\n",
        "    ]\n",
        "  }\n",
        "  ```\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "R6wCXesCsoQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import re\n",
        "import json\n",
        "\n",
        "# Step 1: Load SpaCy NLP Model\n",
        "# SpaCy is used for natural language processing, specifically entity recognition in this case.\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Priority order for entity types to resolve conflicts\n",
        "ENTITY_PRIORITY = {\n",
        "    \"ORG\": 1,         # Organizations like companies, institutions\n",
        "    \"PERSON\": 2,      # Names of people\n",
        "    \"NORP\": 3,        # Nationalities, religious or political groups\n",
        "    \"GPE\": 4,         # Countries, cities, states\n",
        "    \"LOC\": 5,         # Locations like mountains, seas\n",
        "    \"WORK_OF_ART\": 6, # Titles of books, songs, etc.\n",
        "    \"PRODUCT\": 7,     # Products like phones, vehicles\n",
        "    \"LANGUAGE\": 8,    # Languages like English, Spanish\n",
        "    \"DATE\": 9,        # Specific dates or periods\n",
        "    \"CARDINAL\": 10,   # Numerals that do not refer to a specific quantity\n",
        "    \"ORDINAL\": 11,    # Positions like first, second\n",
        "    \"TIME\": 12,       # Times of the day\n",
        "    \"MONEY\": 13,      # Monetary values\n",
        "    \"PERCENT\": 14,    # Percentages\n",
        "    \"QUANTITY\": 15,   # Quantities like \"a ton\"\n",
        "    \"LAW\": 16,        # Legal documents like \"The Constitution\"\n",
        "    \"EVENT\": 17,      # Named events like \"World War II\"\n",
        "    \"FAC\": 18,        # Buildings, airports, highways\n",
        "}\n",
        "\n",
        "# Helper function to filter and prioritize entities\n",
        "def filter_and_deduplicate_entities(entities):\n",
        "    \"\"\"\n",
        "    Deduplicate and filter entities by selecting the highest-priority type for each name.\n",
        "    Args:\n",
        "        entities (list): List of raw entities with names and types.\n",
        "    Returns:\n",
        "        list: Filtered and deduplicated list of entities.\n",
        "    \"\"\"\n",
        "    unique_entities = {}\n",
        "\n",
        "    for entity in entities:\n",
        "        name = entity[\"entity\"].strip()\n",
        "        label = entity[\"type\"]\n",
        "\n",
        "        # Ignore generic or irrelevant entities\n",
        "        if label in {\"CARDINAL\", \"ORDINAL\"} and not name.isdigit():\n",
        "            continue  # Skip generic numerals\n",
        "        if len(name) <= 2:  # Exclude very short entities\n",
        "            continue\n",
        "\n",
        "        # Deduplicate by keeping only the highest-priority type\n",
        "        if name in unique_entities:\n",
        "            current_priority = ENTITY_PRIORITY[unique_entities[name][\"type\"]]\n",
        "            new_priority = ENTITY_PRIORITY[label]\n",
        "            if new_priority < current_priority:\n",
        "                unique_entities[name] = {\"entity\": name, \"type\": label}\n",
        "        else:\n",
        "            unique_entities[name] = {\"entity\": name, \"type\": label}\n",
        "\n",
        "    # Return deduplicated entities as a list\n",
        "    return list(unique_entities.values())\n",
        "\n",
        "# Function to extract and process entities\n",
        "def process_entities(url_texts):\n",
        "    \"\"\"\n",
        "    Extract and process entities from input text.\n",
        "    Args:\n",
        "        url_texts (dict): Dictionary with URLs as keys and cleaned webpage text as values.\n",
        "    Returns:\n",
        "        dict: Processed entities organized by URLs.\n",
        "    \"\"\"\n",
        "    processed_entities = {}\n",
        "\n",
        "    for url, text in url_texts.items():\n",
        "        print(f\"Processing URL: {url}\")\n",
        "\n",
        "        # Preprocess text: remove special characters and normalize whitespace\n",
        "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        # Use SpaCy NLP model to extract entities\n",
        "        doc = nlp(text)\n",
        "        raw_entities = [{\"entity\": ent.text.strip(), \"type\": ent.label_} for ent in doc.ents]\n",
        "\n",
        "        # Deduplicate and filter the entities\n",
        "        refined_entities = filter_and_deduplicate_entities(raw_entities)\n",
        "        processed_entities[url] = refined_entities\n",
        "\n",
        "        # Print a preview of the refined entities\n",
        "        print(f\"Entities for {url}:\")\n",
        "        for entity in refined_entities[:10]:  # Show top 10 entities for review\n",
        "            print(f\"  - {entity['entity']} ({entity['type']})\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "    return processed_entities\n",
        "\n",
        "# Step 2: Load input text\n",
        "input_file = 'cleaned_webpage_texts.json'  # File containing cleaned webpage text\n",
        "try:\n",
        "    with open(input_file, 'r', encoding='utf-8') as f:\n",
        "        webpage_texts = json.load(f)\n",
        "        print(f\"Successfully loaded text from '{input_file}'.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File '{input_file}' not found.\")\n",
        "    exit(1)\n",
        "\n",
        "# Step 3: Process the texts and extract entities\n",
        "processed_entities = process_entities(webpage_texts)\n",
        "\n",
        "# Step 4: Save processed entities to a JSON file\n",
        "output_file = 'deduplicated_entities.json'\n",
        "with open(output_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(processed_entities, f, ensure_ascii=False, indent=4)\n",
        "print(f\"Processed entities saved to '{output_file}'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkYEJAhRS3DU",
        "outputId": "702c8b5f-bb1e-4ef4-ede5-b571ec01fab6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded text from 'cleaned_webpage_texts.json'.\n",
            "Processing URL: https://thatware.co/advanced-seo-services/\n",
            "Entities for https://thatware.co/advanced-seo-services/:\n",
            "  - Thatware (ORG)\n",
            "  - Picture this You (WORK_OF_ART)\n",
            "  - Prepare (ORG)\n",
            "  - Onsite SEO (ORG)\n",
            "  - Thatwares (PERSON)\n",
            "  - Advanced (PERSON)\n",
            "  - SEMrush (ORG)\n",
            "  - Thatwares Advanced SEO (ORG)\n",
            "  - Unveiling Thatwares Arsenal of Advanced SEO Services Thatware (ORG)\n",
            "  - Google Local Business SEO (ORG)\n",
            "================================================================================\n",
            "Processed entities saved to 'deduplicated_entities.json'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Explanation of the Output**\n",
        "This output is the result of **Part 2: Entity Recognition and Deduplication** in the process of extracting meaningful information from a webpage.\n",
        "\n",
        "---\n",
        "\n",
        "#### **1. Successfully Loaded Text**\n",
        "**Line:**\n",
        "```\n",
        "Successfully loaded text from 'cleaned_webpage_texts.json'.\n",
        "```\n",
        "\n",
        "**What this means:**\n",
        "- The program has successfully read the cleaned text file (`cleaned_webpage_texts.json`) that was created in the previous step (Part 1: Scraping and Cleaning).\n",
        "- This file contains the cleaned content of webpages, where unnecessary words and characters were removed.\n",
        "\n",
        "**Why this is important:**\n",
        "- The cleaned text is the input for this step, which focuses on identifying key entities (like company names, people, or locations).\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. Processing URL**\n",
        "**Line:**\n",
        "```\n",
        "Processing URL: https://thatware.co/advanced-seo-services/\n",
        "```\n",
        "\n",
        "**What this means:**\n",
        "- The program has started processing the cleaned text of the webpage at the given URL (`https://thatware.co/advanced-seo-services/`).\n",
        "- It will analyze the content to extract entities (important words or phrases) using advanced natural language processing (NLP).\n",
        "\n",
        "**Why this is important:**\n",
        "- Extracting entities helps identify specific keywords that are meaningful for SEO (Search Engine Optimization). These keywords can be linked to relevant resources or used in marketing strategies.\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. Extracted Entities**\n",
        "**Lines:**\n",
        "```\n",
        "Entities for https://thatware.co/advanced-seo-services/:\n",
        "  - Thatware (ORG)\n",
        "  - Picture this You (WORK_OF_ART)\n",
        "  - Prepare (ORG)\n",
        "  - Onsite SEO (ORG)\n",
        "  - Thatwares (PERSON)\n",
        "  - Advanced (PERSON)\n",
        "  - SEMrush (ORG)\n",
        "  - Thatwares Advanced SEO (ORG)\n",
        "  - Unveiling Thatwares Arsenal of Advanced SEO Services Thatware (ORG)\n",
        "  - Google Local Business SEO (ORG)\n",
        "```\n",
        "\n",
        "**What this means:**\n",
        "- These are the entities (important words or phrases) that the program extracted from the webpage text. Each entity is classified into a **type** that explains what it represents. Below is a breakdown of the key terms:\n",
        "\n",
        "  1. **Entity Names**:\n",
        "     - These are the meaningful words or phrases found in the text.\n",
        "     - Examples: \"Thatware\", \"SEMrush\", \"Google Local Business SEO\".\n",
        "\n",
        "  2. **Entity Types**:\n",
        "     - These are categories assigned to each entity based on its meaning. The program uses predefined types like:\n",
        "       - **ORG (Organization)**: Represents a company or institution (e.g., \"Thatware\", \"SEMrush\").\n",
        "       - **WORK_OF_ART**: Represents titles of creative works like books or slogans (e.g., \"Picture this You\").\n",
        "       - **PERSON**: Represents names of people (e.g., \"Thatwares\", \"Advanced\").\n",
        "\n",
        "  **Why this is important:**\n",
        "  - The extracted entities can now be used to create links, highlight key information, or improve SEO.\n",
        "  - Each entity's type helps determine how it should be used. For example:\n",
        "    - Organizations (ORG) may be linked to their official websites.\n",
        "    - Persons (PERSON) could be linked to their profiles or biographies.\n",
        "\n",
        "---\n",
        "\n",
        "#### **4. Deduplication and Filtering**\n",
        "While not explicitly shown in this part of the output, here's what happened behind the scenes:\n",
        "- **Deduplication**:\n",
        "  - The program ensures that the same entity is not listed multiple times unless it has different contextual meanings (e.g., \"Thatware\" as both ORG and PERSON).\n",
        "- **Filtering**:\n",
        "  - The program removes irrelevant entities, such as short words or generic numbers, to keep the list concise and meaningful.\n",
        "\n",
        "**Why this is important:**\n",
        "- It prevents duplication, which reduces redundancy when using this data in the next steps (like linking entities to resources).\n",
        "- It ensures the output is clean, accurate, and easy to use.\n",
        "\n",
        "---\n",
        "\n",
        "#### **5. Processed Entities Saved**\n",
        "**Line:**\n",
        "```\n",
        "Processed entities saved to 'deduplicated_entities.json'.\n",
        "```\n",
        "\n",
        "**What this means:**\n",
        "- The extracted and deduplicated entities for the webpage are saved in a structured file named `deduplicated_entities.json`.\n",
        "- This file contains the URL, the list of entities, and their types in a format ready for the next step.\n",
        "\n",
        "**Why this is important:**\n",
        "- The saved file acts as input for Part 3, where these entities will be linked to relevant resources (e.g., official websites or articles).\n",
        "\n",
        "**What the file contains:**\n",
        "- The file `deduplicated_entities.json` is a structured JSON document, and an example entry might look like this:\n",
        "  ```json\n",
        "  {\n",
        "      \"https://thatware.co/advanced-seo-services/\": [\n",
        "          {\"entity\": \"Thatware\", \"type\": \"ORG\"},\n",
        "          {\"entity\": \"Picture this You\", \"type\": \"WORK_OF_ART\"},\n",
        "          {\"entity\": \"SEMrush\", \"type\": \"ORG\"}\n",
        "      ]\n",
        "  }\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "### **Why This Output is Useful**\n",
        "1. **Entity Recognition**:\n",
        "   - The program successfully identifies important words or phrases (entities) from the webpage text.\n",
        "   - These entities are classified into meaningful types like ORG (organizations) or PERSON (people).\n",
        "\n",
        "2. **Structured Data**:\n",
        "   - The data is saved in a structured JSON file (`deduplicated_entities.json`) for easy use in the next steps.\n",
        "\n",
        "3. **Prepared for Linking**:\n",
        "   - The cleaned and organized entities are now ready to be linked to relevant resources in the next part of the process (Part 3).\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "GIGhYh2JsdpK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# **Part 3: Entity Link Generation**\n",
        "**Purpose**: To associate each entity with a relevant link using DuckDuckGo’s API.\n",
        "\n",
        "#### Key Features:\n",
        "1. **Search and Link**:\n",
        "   - Searches for each entity using DuckDuckGo.\n",
        "   - Fetches the most relevant link and associates it with the entity.\n",
        "\n",
        "2. **Global Deduplication**:\n",
        "   - Ensures that the same entity (e.g., \"Google\") is not processed or linked multiple times across different URLs.\n",
        "\n",
        "3. **Output**:\n",
        "   - A JSON file (`linked_entities_with_preview.json`) containing entities, their types, and associated links.\n",
        "\n",
        "4. **Preview**:\n",
        "   - Prints a preview of linked entities for quick review.\n",
        "\n",
        "**Example Use**:\n",
        "- Input: Entities from Part 2.\n",
        "- Output: Entities linked to relevant webpages, like:\n",
        "  ```json\n",
        "  {\n",
        "    \"https://thatware.co/advanced-seo-services/\": [\n",
        "      {\"entity\": \"Google\", \"type\": \"ORG\", \"link\": \"https://duckduckgo.com/Google\"},\n",
        "      {\"entity\": \"India\", \"type\": \"GPE\", \"link\": \"https://duckduckgo.com/India\"}\n",
        "    ]\n",
        "  }\n",
        "  ```\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "01WZPNjY3gCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import requests\n",
        "\n",
        "# Step 1: Load the input data (processed entities from the second part)\n",
        "input_file = \"deduplicated_entities.json\"  # Adjusted to match the output of the second part\n",
        "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    entity_data = json.load(f)\n",
        "\n",
        "# Define a list of relevant keywords or types for prioritization\n",
        "RELEVANT_TYPES = {\"ORG\", \"GPE\"}  # Organization and Geopolitical Entities are typically relevant\n",
        "IGNORED_ENTITIES = {\"Advanced\", \"Prepare\", \"Omni\", \"Small Medium Large\", \"Intellectual\"}  # Terms to ignore\n",
        "\n",
        "def is_relevant(entity):\n",
        "    \"\"\"\n",
        "    Determines if an entity is relevant for link generation based on its type and name.\n",
        "\n",
        "    Args:\n",
        "        entity (dict): A dictionary containing 'entity' (name) and 'type'.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if the entity is relevant, False otherwise.\n",
        "    \"\"\"\n",
        "    # Check if the entity type is in the relevant list and the name is not ignored\n",
        "    return (\n",
        "        entity[\"type\"] in RELEVANT_TYPES and\n",
        "        entity[\"entity\"] not in IGNORED_ENTITIES\n",
        "    )\n",
        "\n",
        "# Step 2: Define a function to fetch the first relevant link using DuckDuckGo API\n",
        "def fetch_entity_link(entity_name):\n",
        "    \"\"\"\n",
        "    Searches for the most relevant link for a given entity using DuckDuckGo's API.\n",
        "\n",
        "    Args:\n",
        "        entity_name (str): The name of the entity to search for.\n",
        "\n",
        "    Returns:\n",
        "        str: The first relevant link, or an empty string if none is found.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Use DuckDuckGo API to perform a search query\n",
        "        query_url = f\"https://api.duckduckgo.com/?q={entity_name}&format=json&pretty=1\"\n",
        "        response = requests.get(query_url)\n",
        "        response.raise_for_status()  # Raise an error for bad HTTP responses\n",
        "        data = response.json()\n",
        "\n",
        "        # Extract the abstract URL if available\n",
        "        return data.get(\"AbstractURL\", \"\") or \"\"\n",
        "    except Exception as e:\n",
        "        # Log the error if something goes wrong\n",
        "        print(f\"Error fetching link for '{entity_name}': {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# Step 3: Process the entities and generate links only for relevant ones\n",
        "def link_relevant_entities(entity_data):\n",
        "    \"\"\"\n",
        "    Processes entities to generate links only for relevant terms.\n",
        "\n",
        "    Args:\n",
        "        entity_data (dict): Dictionary of entities grouped by URL.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary of URLs with linked entities.\n",
        "    \"\"\"\n",
        "    linked_entities = {}\n",
        "    seen_entities = set()  # To avoid duplicate link generation\n",
        "\n",
        "    for url, entities in entity_data.items():\n",
        "        print(f\"Processing URL: {url}\")\n",
        "        linked_entities[url] = []\n",
        "\n",
        "        for entity in entities:\n",
        "            name = entity[\"entity\"].strip()\n",
        "            type_ = entity[\"type\"]\n",
        "\n",
        "            # Check if the entity is relevant and not already processed\n",
        "            if is_relevant(entity) and name.lower() not in seen_entities:\n",
        "                link = fetch_entity_link(name)  # Fetch the link\n",
        "                if link:  # Only include entities with valid links\n",
        "                    linked_entity = {\"entity\": name, \"type\": type_, \"link\": link}\n",
        "                    linked_entities[url].append(linked_entity)\n",
        "                    seen_entities.add(name.lower())  # Mark as processed\n",
        "\n",
        "        # Display a preview of linked entities for the current URL\n",
        "        print(f\"\\nPreview for {url}:\")\n",
        "        for entry in linked_entities[url][:5]:  # Limit to top 5 for brevity\n",
        "            print(f\" - {entry['entity']} ({entry['type']}): {entry['link']}\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "    return linked_entities\n",
        "\n",
        "# Step 4: Link relevant entities and save results\n",
        "output_file = \"linked_relevant_entities.json\"\n",
        "linked_data = link_relevant_entities(entity_data)\n",
        "\n",
        "# Save the final linked entities to a JSON file\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(linked_data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(f\"Linked relevant entities saved to '{output_file}'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwhYJRwA4-2Y",
        "outputId": "e2fc58bf-7554-4110-887a-c0542ef24a97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing URL: https://thatware.co/advanced-seo-services/\n",
            "Error fetching link for 'Onsite SEO': Expecting value: line 1 column 1 (char 0)\n",
            "Error fetching link for 'Thatwares Advanced SEO': Expecting value: line 1 column 1 (char 0)\n",
            "Error fetching link for 'Google Local Business SEO': Expecting value: line 1 column 1 (char 0)\n",
            "Error fetching link for 'Thatwares SEO': Expecting value: line 1 column 1 (char 0)\n",
            "\n",
            "Preview for https://thatware.co/advanced-seo-services/:\n",
            " - SEMrush (ORG): https://en.wikipedia.org/wiki/Semrush\n",
            " - Google (ORG): https://en.wikipedia.org/wiki/Google\n",
            " - NLP (ORG): https://en.wikipedia.org/wiki/NLP\n",
            " - XML (ORG): https://en.wikipedia.org/wiki/XML_(disambiguation)\n",
            " - India (GPE): https://en.wikipedia.org/wiki/India_(disambiguation)\n",
            "================================================================================\n",
            "Linked relevant entities saved to 'linked_relevant_entities.json'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understanding the Output: What Does This Data Represent?\n",
        "\n",
        "This output is the result of an **Entity Recognition and Linking (ERL) Model**. It demonstrates how specific terms (entities) from the content of a webpage have been identified, categorized, and linked to relevant web pages.\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Components of the Output**\n",
        "\n",
        "#### 1. **URL**\n",
        "   - `\"https://thatware.co/advanced-seo-services/\"`:\n",
        "     - This is the source webpage where the entities were identified.\n",
        "     - Every entity listed here was extracted from the content of this specific URL.\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. **Entity**\n",
        "   - Example: `\"entity\": \"SEMrush\"`:\n",
        "     - An **entity** is a term or phrase that has been recognized as meaningful or important in the context of the content.\n",
        "     - Entities can represent people, organizations, places, or concepts.\n",
        "\n",
        "---\n",
        "\n",
        "#### 3. **Type**\n",
        "   - Example: `\"type\": \"ORG\"`:\n",
        "     - The **type** describes the category of the entity:\n",
        "       - **ORG**: Organization (e.g., \"SEMrush\", \"Google\").\n",
        "       - **GPE**: Geopolitical Entity (e.g., \"India\", \"Dubai\").\n",
        "     - These categories help in understanding the nature of the entity (e.g., is it a company, a place, or a concept?).\n",
        "\n",
        "---\n",
        "\n",
        "#### 4. **Link**\n",
        "   - Example: `\"link\": \"https://en.wikipedia.org/wiki/Semrush\"`:\n",
        "     - This is a URL that provides more information about the entity.\n",
        "     - The link is generated automatically by searching the web for the most relevant source about the entity.\n",
        "     - For example:\n",
        "       - \"SEMrush\" is linked to its Wikipedia page.\n",
        "       - \"India\" is linked to its disambiguation page on Wikipedia.\n",
        "\n",
        "---\n",
        "\n",
        "### **Detailed Breakdown of Entities in This Output**\n",
        "\n",
        "#### **1. SEMrush**\n",
        "   - **Type**: ORG (Organization).\n",
        "   - **Link**: `\"https://en.wikipedia.org/wiki/Semrush\"`\n",
        "   - **What it Means**: SEMrush is an organization specializing in online marketing and SEO tools. Linking to its Wikipedia page provides users with more context about the company.\n",
        "\n",
        "#### **2. Google**\n",
        "   - **Type**: ORG (Organization).\n",
        "   - **Link**: `\"https://en.wikipedia.org/wiki/Google\"`\n",
        "   - **What it Means**: Google is a well-known organization. Linking it helps readers understand its relevance to the webpage content.\n",
        "\n",
        "#### **3. NLP**\n",
        "   - **Type**: ORG (Organization).\n",
        "   - **Link**: `\"https://en.wikipedia.org/wiki/NLP\"`\n",
        "   - **What it Means**: NLP stands for Natural Language Processing, an important concept in artificial intelligence. The link explains what NLP is and its significance.\n",
        "\n",
        "#### **4. XML**\n",
        "   - **Type**: ORG (Organization).\n",
        "   - **Link**: `\"https://en.wikipedia.org/wiki/XML_(disambiguation)\"`\n",
        "   - **What it Means**: XML (Extensible Markup Language) is a standard for structuring data. The link clarifies its technical importance.\n",
        "\n",
        "#### **5. India**\n",
        "   - **Type**: GPE (Geopolitical Entity).\n",
        "   - **Link**: `\"https://en.wikipedia.org/wiki/India_(disambiguation)\"`\n",
        "   - **What it Means**: India is identified as a country relevant to the webpage content, and the link provides information about it.\n",
        "\n",
        "#### **6. Dubai**\n",
        "   - **Type**: GPE (Geopolitical Entity).\n",
        "   - **Link**: `\"https://en.wikipedia.org/wiki/Dubai_(disambiguation)\"`\n",
        "   - **What it Means**: Dubai, a city, is relevant in the webpage's context, and the link gives additional details.\n",
        "\n",
        "---\n",
        "\n",
        "### **What Does This Output Convey?**\n",
        "\n",
        "- The output shows a **structured summary of the webpage's content** in terms of entities.\n",
        "- Each entity is categorized and linked to a reliable source for additional information.\n",
        "- This is useful for:\n",
        "  - **SEO (Search Engine Optimization)**: Helps improve the content's visibility by adding authoritative links.\n",
        "  - **User Engagement**: Allows users to click on links to learn more about the entities mentioned.\n",
        "  - **Data Structuring**: Provides a clean, organized format for entities, making the data useful for other applications like analytics.\n",
        "\n",
        "---\n",
        "\n",
        "### **Why Is This Useful for Website Owners?**\n",
        "\n",
        "1. **Improves Search Engine Rankings**:\n",
        "   - Linking entities to authoritative sources boosts credibility in search engines.\n",
        "\n",
        "2. **Enhances User Experience**:\n",
        "   - Visitors can easily find more information about entities without searching elsewhere.\n",
        "\n",
        "3. **Content Enrichment**:\n",
        "   - By linking key terms, the content becomes more informative and valuable to the reader.\n",
        "\n",
        "4. **Analytics and Insights**:\n",
        "   - Structured data allows website owners to analyze which entities are most relevant to their audience.\n",
        "\n",
        "---\n",
        "\n",
        "### **Next Steps After Getting This Output**\n",
        "\n",
        "1. **Review the Links**:\n",
        "   - Ensure that all links are contextually correct and relevant.\n",
        "   - For example, \"India\" links to its disambiguation page. You may want to refine this to a more specific page about the country.\n",
        "\n",
        "2. **Incorporate the Links into the Webpage**:\n",
        "   - Use the annotated text (if generated) to embed clickable links in the webpage.\n",
        "\n",
        "3. **Leverage for SEO**:\n",
        "   - Submit the structured data as part of your site's SEO strategy to search engines like Google.\n",
        "\n",
        "4. **Expand the Process**:\n",
        "   - Run the model on additional webpages to extract and link entities site-wide.\n",
        "\n",
        "---\n",
        "\n",
        "### Final Explanation for Non-Technical Users\n",
        "This output identifies and links important terms from a webpage to reliable online sources. It categorizes these terms to understand their type (e.g., company, place, or concept). The links make the webpage more interactive and improve its value for users and search engines.\n",
        "\n",
        "This process ultimately helps website owners enhance their content’s quality and visibility, making it a valuable tool for SEO and user engagement."
      ],
      "metadata": {
        "id": "arNqG_6B6Dey"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# **Part 4: Annotated Text and Structured Data Generator**\n",
        "**Purpose**: To create:\n",
        "1. Annotated text with clickable links for entities.\n",
        "2. A CSV file with structured data for all entities and their links.\n",
        "\n",
        "#### Key Features:\n",
        "1. **Annotated Text**:\n",
        "   - Replaces entities in the original text with clickable HTML links.\n",
        "   - Example:\n",
        "     - Original: \"Google is a search engine.\"\n",
        "     - Annotated: \"Google is a search engine.\"\n",
        "       - `Google` becomes a clickable link.\n",
        "\n",
        "2. **Structured Data**:\n",
        "   - Generates a CSV file (`structured_data.csv`) with columns:\n",
        "     - `URL`: The webpage source.\n",
        "     - `Entity`: The recognized entity.\n",
        "     - `Type`: The entity’s type (e.g., `ORG` for organization).\n",
        "     - `Link`: The associated link.\n",
        "\n",
        "3. **Preview**:\n",
        "   - Displays a preview of the annotated text and linked entities in the console.\n",
        "\n",
        "4. **Output**:\n",
        "   - Annotated text in JSON format (`annotated_text.json`).\n",
        "   - Structured data in CSV format (`structured_data.csv`).\n",
        "   - A preview file (`preview_data.json`) for quick reference.\n",
        "\n",
        "**Example Use**:\n",
        "- Input: Linked entities from Part 3 and original webpage text.\n",
        "- Output:\n",
        "  - Annotated text like:\n",
        "    ```html\n",
        "    Google is a search engine.\n",
        "    ```\n",
        "  - Structured CSV:\n",
        "    | URL                                  | Entity    | Type  | Link                           |\n",
        "    |--------------------------------------|-----------|-------|--------------------------------|\n",
        "    | https://thatware.co/advanced-seo...  | Google    | ORG   | https://duckduckgo.com/Google |\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "K7Gg5L0IZ7Nl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "import csv\n",
        "\n",
        "# Step 1: Load input files\n",
        "# Load original webpage content and linked entities.\n",
        "input_file = 'cleaned_webpage_texts.json'  # The cleaned webpage content.\n",
        "entities_file = 'linked_relevant_entities.json'  # Entities with relevant links generated in the third part.\n",
        "annotated_output_file = 'annotated_text.json'  # Output file for annotated text.\n",
        "structured_output_file = 'structured_data.csv'  # Output file for structured data.\n",
        "\n",
        "try:\n",
        "    # Load cleaned webpage texts\n",
        "    with open(input_file, 'r', encoding='utf-8') as f:\n",
        "        webpage_texts = json.load(f)\n",
        "\n",
        "    # Load linked relevant entities\n",
        "    with open(entities_file, 'r', encoding='utf-8') as f:\n",
        "        linked_entities = json.load(f)\n",
        "\n",
        "    print(\"Input files loaded successfully.\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error loading files: {e}\")\n",
        "    exit(1)\n",
        "\n",
        "# Step 2: Define a function to annotate text with clickable links\n",
        "def annotate_text(text, entities):\n",
        "    \"\"\"\n",
        "    Annotates webpage text by adding clickable HTML links to identified entities.\n",
        "\n",
        "    Args:\n",
        "        text (str): The original webpage text.\n",
        "        entities (list): List of dictionaries containing 'entity', 'type', and 'link'.\n",
        "\n",
        "    Returns:\n",
        "        str: The annotated text with clickable links.\n",
        "    \"\"\"\n",
        "    # Sort entities by length to avoid overlapping matches (longest entities first).\n",
        "    entities = sorted(entities, key=lambda e: len(e['entity']), reverse=True)\n",
        "\n",
        "    for entity in entities:\n",
        "        # Create a safe regex pattern to find the entity in the text\n",
        "        pattern = re.escape(entity['entity'])\n",
        "        link = entity['link']\n",
        "\n",
        "        # Replace the entity in the text with a clickable HTML link\n",
        "        text = re.sub(\n",
        "            pattern,\n",
        "            f\"<a href='{link}' target='_blank'>{entity['entity']}</a>\",\n",
        "            text\n",
        "        )\n",
        "    return text\n",
        "\n",
        "# Step 3: Process each URL and generate annotated text and structured data\n",
        "annotated_texts = {}  # Dictionary to store annotated text for each URL.\n",
        "structured_data = []  # List to store structured data rows for CSV output.\n",
        "preview_data = []  # List to store a preview of entity, type, and link for each URL.\n",
        "\n",
        "for url, text in webpage_texts.items():\n",
        "    if url not in linked_entities:\n",
        "        # Skip URLs that don't have any linked entities\n",
        "        print(f\"Skipping {url} as no linked entities are available.\")\n",
        "        continue\n",
        "\n",
        "    # Get the entities for the current URL\n",
        "    entities = linked_entities[url]\n",
        "\n",
        "    # Generate annotated text for the current URL\n",
        "    annotated_text = annotate_text(text, entities)\n",
        "    annotated_texts[url] = annotated_text  # Store the annotated text\n",
        "\n",
        "    # Add entity details to structured data and preview\n",
        "    for entity in entities:\n",
        "        structured_data.append({\n",
        "            \"URL\": url,\n",
        "            \"Entity\": entity['entity'],\n",
        "            \"Type\": entity['type'],\n",
        "            \"Link\": entity['link']\n",
        "        })\n",
        "        preview_data.append({\n",
        "            \"URL\": url,\n",
        "            \"Preview\": f\"{entity['entity']} ({entity['type']}) → {entity['link']}\"\n",
        "        })\n",
        "\n",
        "# Step 4: Save annotated text to a JSON file\n",
        "try:\n",
        "    with open(annotated_output_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(annotated_texts, f, ensure_ascii=False, indent=4)\n",
        "    print(f\"Annotated text saved to '{annotated_output_file}'.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving annotated text: {e}\")\n",
        "\n",
        "# Step 5: Save structured data to a CSV file\n",
        "try:\n",
        "    with open(structured_output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        fieldnames = ['URL', 'Entity', 'Type', 'Link']\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(structured_data)\n",
        "    print(f\"Structured data saved to '{structured_output_file}'.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving structured data: {e}\")\n",
        "\n",
        "# Step 6: Display a preview of the annotated text and linked entities\n",
        "print(\"\\nPreview of Annotated Text and Linked Entities:\")\n",
        "for row in preview_data[:10]:  # Show the first 10 entries\n",
        "    print(f\"URL: {row['URL']}\")\n",
        "    print(f\"Preview: {row['Preview']}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "# Optional: Save preview data for future use\n",
        "preview_output_file = 'preview_data.json'\n",
        "try:\n",
        "    with open(preview_output_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(preview_data, f, ensure_ascii=False, indent=4)\n",
        "    print(f\"Preview data saved to '{preview_output_file}'.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving preview data: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zB7dM4kf7-ZV",
        "outputId": "686de1f8-4d42-4ad5-f16f-b4bcf8166e78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input files loaded successfully.\n",
            "Annotated text saved to 'annotated_text.json'.\n",
            "Structured data saved to 'structured_data.csv'.\n",
            "\n",
            "Preview of Annotated Text and Linked Entities:\n",
            "URL: https://thatware.co/advanced-seo-services/\n",
            "Preview: SEMrush (ORG) → https://en.wikipedia.org/wiki/Semrush\n",
            "--------------------------------------------------------------------------------\n",
            "URL: https://thatware.co/advanced-seo-services/\n",
            "Preview: Google (ORG) → https://en.wikipedia.org/wiki/Google\n",
            "--------------------------------------------------------------------------------\n",
            "URL: https://thatware.co/advanced-seo-services/\n",
            "Preview: NLP (ORG) → https://en.wikipedia.org/wiki/NLP\n",
            "--------------------------------------------------------------------------------\n",
            "URL: https://thatware.co/advanced-seo-services/\n",
            "Preview: XML (ORG) → https://en.wikipedia.org/wiki/XML_(disambiguation)\n",
            "--------------------------------------------------------------------------------\n",
            "URL: https://thatware.co/advanced-seo-services/\n",
            "Preview: India (GPE) → https://en.wikipedia.org/wiki/India_(disambiguation)\n",
            "--------------------------------------------------------------------------------\n",
            "URL: https://thatware.co/advanced-seo-services/\n",
            "Preview: Dubai (GPE) → https://en.wikipedia.org/wiki/Dubai_(disambiguation)\n",
            "--------------------------------------------------------------------------------\n",
            "Preview data saved to 'preview_data.json'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Detailed Explanation of the Output\n",
        "\n",
        "This output represents the **results of the fourth part of the code**, where we annotate webpage content with clickable links for relevant entities and provide structured data for further analysis.\n",
        "\n",
        "---\n",
        "\n",
        "### **What is This Output About?**\n",
        "\n",
        "The output is the result of processing cleaned webpage content to:\n",
        "\n",
        "1. **Annotate Text:**\n",
        "   - Add clickable links to important entities (like company names, locations, and technical terms) in the webpage content.\n",
        "   - These links provide more information about the entities, improving the SEO (Search Engine Optimization) and user experience.\n",
        "\n",
        "2. **Structured Data:**\n",
        "   - Create a CSV file listing each entity, its type (e.g., organization, location), and its corresponding link.\n",
        "   - This structured format is useful for analyzing and managing data.\n",
        "\n",
        "3. **Preview of Results:**\n",
        "   - Provide a quick and easy-to-read summary of the annotated entities and their links.\n",
        "\n",
        "---\n",
        "\n",
        "### **Line-by-Line Explanation of the Output**\n",
        "\n",
        "1. **\"Input files loaded successfully\":**\n",
        "   - This confirms that the code successfully loaded the cleaned webpage text (`cleaned_webpage_texts.json`) and the linked entities (`linked_relevant_entities.json`).\n",
        "   - Without these files, the process would fail.\n",
        "\n",
        "2. **\"Annotated text saved to 'annotated_text.json'\":**\n",
        "   - The webpage content is now annotated with clickable links for entities (e.g., \"Google\" is linked to its Wikipedia page).\n",
        "   - This annotated content is saved in a JSON file named `annotated_text.json`.\n",
        "\n",
        "3. **\"Structured data saved to 'structured_data.csv'\":**\n",
        "   - A structured CSV file is created, listing each entity along with:\n",
        "     - The URL of the webpage where it was found.\n",
        "     - The type of the entity (e.g., ORG for organization, GPE for location).\n",
        "     - The clickable link for the entity.\n",
        "   - This is saved as `structured_data.csv` for easy viewing in spreadsheet tools like Excel.\n",
        "\n",
        "4. **\"Preview of Annotated Text and Linked Entities\":**\n",
        "   - This section provides a quick summary of the results:\n",
        "     - **URL**: The source webpage where the entity was found.\n",
        "     - **Entity**: The recognized term, such as \"Google\" or \"India.\"\n",
        "     - **Type**: The classification of the entity, e.g., ORG (Organization), GPE (Geopolitical Entity).\n",
        "     - **Link**: The clickable link associated with the entity.\n",
        "\n",
        "   #### Examples from the Preview:\n",
        "   - **\"SEMrush (ORG) → https://en.wikipedia.org/wiki/Semrush\"**\n",
        "     - This means the term \"SEMrush\" was identified as an organization and linked to its Wikipedia page.\n",
        "   - **\"Google (ORG) → https://en.wikipedia.org/wiki/Google\"**\n",
        "     - \"Google\" was recognized as an organization and linked to its Wikipedia page.\n",
        "   - **\"NLP (ORG) → https://en.wikipedia.org/wiki/NLP\"**\n",
        "     - \"NLP\" (Natural Language Processing) was linked to its Wikipedia page for further explanation.\n",
        "\n",
        "5. **\"Preview data saved to 'preview_data.json'\":**\n",
        "   - This saves the preview data to a JSON file for quick review and debugging.\n",
        "   - The preview helps users or developers verify that the correct entities are linked and displayed.\n",
        "\n",
        "---\n",
        "\n",
        "### **What Does This Output Mean for the Client?**\n",
        "\n",
        "1. **Annotated Text:**\n",
        "   - Makes the content on the webpage more informative by adding clickable links to recognized entities.\n",
        "   - Enhances SEO by linking to authoritative sources, improving the credibility and relevance of the content.\n",
        "\n",
        "2. **Structured Data:**\n",
        "   - The structured CSV file is a valuable resource for business analysis, allowing users to:\n",
        "     - See all recognized entities at a glance.\n",
        "     - Understand which entities are linked and how they are categorized.\n",
        "     - Share the data with other tools for further insights.\n",
        "\n",
        "3. **Preview:**\n",
        "   - The preview ensures the output meets expectations, showing which terms are linked and where they lead.\n",
        "\n",
        "---\n",
        "\n",
        "### **How is This Useful for Website Owners?**\n",
        "\n",
        "1. **Enhanced User Experience:**\n",
        "   - Users can click on entities (e.g., \"Google\" or \"SEMrush\") to learn more without leaving the page.\n",
        "   - This keeps users engaged with the content, reducing bounce rates.\n",
        "\n",
        "2. **Improved SEO:**\n",
        "   - Linking to high-quality external resources (like Wikipedia) signals to search engines that the content is credible.\n",
        "   - Improves the ranking of the webpage in search results.\n",
        "\n",
        "3. **Data-Driven Insights:**\n",
        "   - Website owners can analyze the CSV file to:\n",
        "     - Identify which entities are frequently mentioned.\n",
        "     - Tailor content based on popular terms or trends.\n",
        "     - Strategically place links to drive traffic to specific pages.\n",
        "\n",
        "---\n",
        "\n",
        "### **Next Steps After This Output**\n",
        "\n",
        "1. **Review and Validate:**\n",
        "   - Ensure the links are correct and contextually relevant.\n",
        "   - Manually adjust or remove any links that do not add value (e.g., if they lead to unrelated pages).\n",
        "\n",
        "2. **Integrate Annotated Content:**\n",
        "   - Use the `annotated_text.json` to integrate the annotated text back into the website.\n",
        "\n",
        "3. **Analyze Structured Data:**\n",
        "   - Use the `structured_data.csv` to identify trends and opportunities for improving content.\n",
        "\n",
        "4. **Enhance Automation:**\n",
        "   - Automate periodic updates to ensure newly added entities are also linked.\n",
        "\n",
        "---\n",
        "\n",
        "This output is a critical step in improving website content and SEO performance, offering both immediate benefits (like user engagement) and long-term advantages (like better search rankings)."
      ],
      "metadata": {
        "id": "aPzKGexHuXxL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests  # Used to send HTTP requests to web pages\n",
        "from bs4 import BeautifulSoup  # For parsing HTML content from web pages\n",
        "import json  # For saving data in a structured JSON format\n",
        "import re  # For performing pattern-based cleaning of text\n",
        "\n",
        "# Step 1: Define the URLs of web pages to scrape\n",
        "# These are the web pages from which we want to extract meaningful text content.\n",
        "urls = [\n",
        "    'https://thatware.co/advanced-seo-services/'\n",
        "]\n",
        "\n",
        "# Step 2: Define cleaning rules\n",
        "# STOPWORDS: These are common words that do not add value to the context (e.g., \"the\", \"and\").\n",
        "# UNNECESSARY_PATTERNS: These are patterns that match irrelevant text (e.g., numbers, special characters).\n",
        "STOPWORDS = set([\"approximately\", \"today\", \"months\", \"years\", \"the\", \"and\", \"or\", \"but\"])\n",
        "UNNECESSARY_PATTERNS = [\n",
        "    r'\\b\\d{2,}\\b',  # Matches standalone numbers with two or more digits, e.g., \"2501\"\n",
        "    r'[^\\w\\s]',     # Matches special characters like punctuation marks, e.g., \"@\" or \"$\"\n",
        "]\n",
        "\n",
        "# Step 3: Initialize a log to track removed words/patterns\n",
        "# This will help us review what was removed during the cleaning process.\n",
        "removed_log = []\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Cleans the raw text extracted from web pages.\n",
        "\n",
        "    Why this function is important:\n",
        "    - Web pages often contain noise, such as numbers, stopwords, and special characters.\n",
        "    - Cleaning ensures the data is concise, relevant, and ready for further processing.\n",
        "\n",
        "    Args:\n",
        "        text (str): The raw text to clean.\n",
        "\n",
        "    Returns:\n",
        "        str: The cleaned version of the text.\n",
        "    \"\"\"\n",
        "    global removed_log  # Use a global log to keep track of removed items\n",
        "\n",
        "    # Step 3.1: Normalize spaces\n",
        "    # Removes excessive spaces (e.g., double spaces) to ensure uniform formatting.\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Step 3.2: Remove unnecessary patterns like numbers and special characters\n",
        "    for pattern in UNNECESSARY_PATTERNS:\n",
        "        # Apply the cleaning pattern and log changes\n",
        "        new_text = re.sub(pattern, '', text)\n",
        "        if new_text != text:  # If text changes, log the original and cleaned version\n",
        "            removed_log.append((text, new_text))\n",
        "        text = new_text\n",
        "\n",
        "    # Step 3.3: Remove stopwords\n",
        "    # Split text into words, filter out stopwords, and log removed words\n",
        "    words = text.split()\n",
        "    cleaned_words = []\n",
        "    for word in words:\n",
        "        if word.lower() not in STOPWORDS:  # Retain words not in stopwords\n",
        "            cleaned_words.append(word)\n",
        "        else:  # Log removed stopwords for review\n",
        "            removed_log.append((word, \"REMOVED\"))\n",
        "\n",
        "    # Join the remaining words into a cleaned text string\n",
        "    return ' '.join(cleaned_words)\n",
        "\n",
        "def fetch_webpage_text(url):\n",
        "    \"\"\"\n",
        "    Fetches the text content of a web page and applies cleaning.\n",
        "\n",
        "    Why this function is important:\n",
        "    - Web pages often contain HTML and irrelevant sections. This function extracts only the main content.\n",
        "    - It ensures that the extracted text is clean and usable for further processing.\n",
        "\n",
        "    Args:\n",
        "        url (str): The URL of the web page to scrape.\n",
        "\n",
        "    Returns:\n",
        "        str: The cleaned text content of the page.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Step 4.1: Send an HTTP GET request to fetch the web page content\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Raise an error if the HTTP request fails\n",
        "\n",
        "        # Step 4.2: Parse the HTML content using BeautifulSoup\n",
        "        # BeautifulSoup makes it easy to extract specific parts of an HTML page, like paragraphs.\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Step 4.3: Extract text from paragraph (<p>) tags\n",
        "        # Paragraphs usually contain the main textual content of a web page.\n",
        "        paragraphs = soup.find_all('p')\n",
        "        text_content = ' '.join([para.get_text().strip() for para in paragraphs])\n",
        "\n",
        "        # Step 4.4: Clean the extracted text\n",
        "        return clean_text(text_content)\n",
        "\n",
        "    except Exception as e:\n",
        "        # Log an error message if something goes wrong while fetching the page\n",
        "        print(f\"Error fetching {url}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# Step 5: Scrape and clean the content from each URL\n",
        "# This dictionary will store the URL as the key and the cleaned text as the value.\n",
        "webpage_texts = {}\n",
        "for url in urls:\n",
        "    print(f\"Scraping URL: {url}\")  # Notify the user which URL is being processed\n",
        "    text = fetch_webpage_text(url)  # Fetch and clean the text\n",
        "    if text:  # If text was successfully extracted and cleaned\n",
        "        webpage_texts[url] = text  # Add the URL-to-text mapping to the dictionary\n",
        "        print(f\"Cleaned Preview:\\n{text[:500]}\\n{'='*80}\")  # Display a short preview\n",
        "\n",
        "# Step 6: Save the cleaned data to a JSON file\n",
        "# JSON is a structured format, making it easy to save and reuse the cleaned data later.\n",
        "output_file = 'cleaned_webpage_texts.json'\n",
        "with open(output_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(webpage_texts, f, ensure_ascii=False, indent=4)\n",
        "print(f\"Cleaning completed. Cleaned data saved to '{output_file}'.\")\n",
        "\n",
        "# Step 7: Save the log of removed items for review\n",
        "# This allows you to see exactly what was removed and ensure that nothing important was lost.\n",
        "removed_log_file = 'removed_log.json'\n",
        "with open(removed_log_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(removed_log, f, ensure_ascii=False, indent=4)\n",
        "print(f\"Removed items logged in '{removed_log_file}'.\")\n",
        "\n",
        "\n",
        "import spacy\n",
        "import re\n",
        "import json\n",
        "\n",
        "# Step 1: Load SpaCy NLP Model\n",
        "# SpaCy is used for natural language processing, specifically entity recognition in this case.\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Priority order for entity types to resolve conflicts\n",
        "ENTITY_PRIORITY = {\n",
        "    \"ORG\": 1,         # Organizations like companies, institutions\n",
        "    \"PERSON\": 2,      # Names of people\n",
        "    \"NORP\": 3,        # Nationalities, religious or political groups\n",
        "    \"GPE\": 4,         # Countries, cities, states\n",
        "    \"LOC\": 5,         # Locations like mountains, seas\n",
        "    \"WORK_OF_ART\": 6, # Titles of books, songs, etc.\n",
        "    \"PRODUCT\": 7,     # Products like phones, vehicles\n",
        "    \"LANGUAGE\": 8,    # Languages like English, Spanish\n",
        "    \"DATE\": 9,        # Specific dates or periods\n",
        "    \"CARDINAL\": 10,   # Numerals that do not refer to a specific quantity\n",
        "    \"ORDINAL\": 11,    # Positions like first, second\n",
        "    \"TIME\": 12,       # Times of the day\n",
        "    \"MONEY\": 13,      # Monetary values\n",
        "    \"PERCENT\": 14,    # Percentages\n",
        "    \"QUANTITY\": 15,   # Quantities like \"a ton\"\n",
        "    \"LAW\": 16,        # Legal documents like \"The Constitution\"\n",
        "    \"EVENT\": 17,      # Named events like \"World War II\"\n",
        "    \"FAC\": 18,        # Buildings, airports, highways\n",
        "}\n",
        "\n",
        "# Helper function to filter and prioritize entities\n",
        "def filter_and_deduplicate_entities(entities):\n",
        "    \"\"\"\n",
        "    Deduplicate and filter entities by selecting the highest-priority type for each name.\n",
        "    Args:\n",
        "        entities (list): List of raw entities with names and types.\n",
        "    Returns:\n",
        "        list: Filtered and deduplicated list of entities.\n",
        "    \"\"\"\n",
        "    unique_entities = {}\n",
        "\n",
        "    for entity in entities:\n",
        "        name = entity[\"entity\"].strip()\n",
        "        label = entity[\"type\"]\n",
        "\n",
        "        # Ignore generic or irrelevant entities\n",
        "        if label in {\"CARDINAL\", \"ORDINAL\"} and not name.isdigit():\n",
        "            continue  # Skip generic numerals\n",
        "        if len(name) <= 2:  # Exclude very short entities\n",
        "            continue\n",
        "\n",
        "        # Deduplicate by keeping only the highest-priority type\n",
        "        if name in unique_entities:\n",
        "            current_priority = ENTITY_PRIORITY[unique_entities[name][\"type\"]]\n",
        "            new_priority = ENTITY_PRIORITY[label]\n",
        "            if new_priority < current_priority:\n",
        "                unique_entities[name] = {\"entity\": name, \"type\": label}\n",
        "        else:\n",
        "            unique_entities[name] = {\"entity\": name, \"type\": label}\n",
        "\n",
        "    # Return deduplicated entities as a list\n",
        "    return list(unique_entities.values())\n",
        "\n",
        "# Function to extract and process entities\n",
        "def process_entities(url_texts):\n",
        "    \"\"\"\n",
        "    Extract and process entities from input text.\n",
        "    Args:\n",
        "        url_texts (dict): Dictionary with URLs as keys and cleaned webpage text as values.\n",
        "    Returns:\n",
        "        dict: Processed entities organized by URLs.\n",
        "    \"\"\"\n",
        "    processed_entities = {}\n",
        "\n",
        "    for url, text in url_texts.items():\n",
        "        print(f\"Processing URL: {url}\")\n",
        "\n",
        "        # Preprocess text: remove special characters and normalize whitespace\n",
        "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        # Use SpaCy NLP model to extract entities\n",
        "        doc = nlp(text)\n",
        "        raw_entities = [{\"entity\": ent.text.strip(), \"type\": ent.label_} for ent in doc.ents]\n",
        "\n",
        "        # Deduplicate and filter the entities\n",
        "        refined_entities = filter_and_deduplicate_entities(raw_entities)\n",
        "        processed_entities[url] = refined_entities\n",
        "\n",
        "        # Print a preview of the refined entities\n",
        "        print(f\"Entities for {url}:\")\n",
        "        for entity in refined_entities[:10]:  # Show top 10 entities for review\n",
        "            print(f\"  - {entity['entity']} ({entity['type']})\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "    return processed_entities\n",
        "\n",
        "# Step 2: Load input text\n",
        "input_file = 'cleaned_webpage_texts.json'  # File containing cleaned webpage text\n",
        "try:\n",
        "    with open(input_file, 'r', encoding='utf-8') as f:\n",
        "        webpage_texts = json.load(f)\n",
        "        print(f\"Successfully loaded text from '{input_file}'.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File '{input_file}' not found.\")\n",
        "    exit(1)\n",
        "\n",
        "# Step 3: Process the texts and extract entities\n",
        "processed_entities = process_entities(webpage_texts)\n",
        "\n",
        "# Step 4: Save processed entities to a JSON file\n",
        "output_file = 'deduplicated_entities.json'\n",
        "with open(output_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(processed_entities, f, ensure_ascii=False, indent=4)\n",
        "print(f\"Processed entities saved to '{output_file}'.\")\n",
        "\n",
        "\n",
        "import json\n",
        "import requests\n",
        "\n",
        "# Step 1: Load the input data (processed entities from the second part)\n",
        "input_file = \"deduplicated_entities.json\"  # Adjusted to match the output of the second part\n",
        "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    entity_data = json.load(f)\n",
        "\n",
        "# Define a list of relevant keywords or types for prioritization\n",
        "RELEVANT_TYPES = {\"ORG\", \"GPE\"}  # Organization and Geopolitical Entities are typically relevant\n",
        "IGNORED_ENTITIES = {\"Advanced\", \"Prepare\", \"Omni\", \"Small Medium Large\", \"Intellectual\"}  # Terms to ignore\n",
        "\n",
        "def is_relevant(entity):\n",
        "    \"\"\"\n",
        "    Determines if an entity is relevant for link generation based on its type and name.\n",
        "\n",
        "    Args:\n",
        "        entity (dict): A dictionary containing 'entity' (name) and 'type'.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if the entity is relevant, False otherwise.\n",
        "    \"\"\"\n",
        "    # Check if the entity type is in the relevant list and the name is not ignored\n",
        "    return (\n",
        "        entity[\"type\"] in RELEVANT_TYPES and\n",
        "        entity[\"entity\"] not in IGNORED_ENTITIES\n",
        "    )\n",
        "\n",
        "# Step 2: Define a function to fetch the first relevant link using DuckDuckGo API\n",
        "def fetch_entity_link(entity_name):\n",
        "    \"\"\"\n",
        "    Searches for the most relevant link for a given entity using DuckDuckGo's API.\n",
        "\n",
        "    Args:\n",
        "        entity_name (str): The name of the entity to search for.\n",
        "\n",
        "    Returns:\n",
        "        str: The first relevant link, or an empty string if none is found.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Use DuckDuckGo API to perform a search query\n",
        "        query_url = f\"https://api.duckduckgo.com/?q={entity_name}&format=json&pretty=1\"\n",
        "        response = requests.get(query_url)\n",
        "        response.raise_for_status()  # Raise an error for bad HTTP responses\n",
        "        data = response.json()\n",
        "\n",
        "        # Extract the abstract URL if available\n",
        "        return data.get(\"AbstractURL\", \"\") or \"\"\n",
        "    except Exception as e:\n",
        "        # Log the error if something goes wrong\n",
        "        print(f\"Error fetching link for '{entity_name}': {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# Step 3: Process the entities and generate links only for relevant ones\n",
        "def link_relevant_entities(entity_data):\n",
        "    \"\"\"\n",
        "    Processes entities to generate links only for relevant terms.\n",
        "\n",
        "    Args:\n",
        "        entity_data (dict): Dictionary of entities grouped by URL.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary of URLs with linked entities.\n",
        "    \"\"\"\n",
        "    linked_entities = {}\n",
        "    seen_entities = set()  # To avoid duplicate link generation\n",
        "\n",
        "    for url, entities in entity_data.items():\n",
        "        print(f\"Processing URL: {url}\")\n",
        "        linked_entities[url] = []\n",
        "\n",
        "        for entity in entities:\n",
        "            name = entity[\"entity\"].strip()\n",
        "            type_ = entity[\"type\"]\n",
        "\n",
        "            # Check if the entity is relevant and not already processed\n",
        "            if is_relevant(entity) and name.lower() not in seen_entities:\n",
        "                link = fetch_entity_link(name)  # Fetch the link\n",
        "                if link:  # Only include entities with valid links\n",
        "                    linked_entity = {\"entity\": name, \"type\": type_, \"link\": link}\n",
        "                    linked_entities[url].append(linked_entity)\n",
        "                    seen_entities.add(name.lower())  # Mark as processed\n",
        "\n",
        "        # Display a preview of linked entities for the current URL\n",
        "        print(f\"\\nPreview for {url}:\")\n",
        "        for entry in linked_entities[url][:5]:  # Limit to top 5 for brevity\n",
        "            print(f\" - {entry['entity']} ({entry['type']}): {entry['link']}\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "    return linked_entities\n",
        "\n",
        "# Step 4: Link relevant entities and save results\n",
        "output_file = \"linked_relevant_entities.json\"\n",
        "linked_data = link_relevant_entities(entity_data)\n",
        "\n",
        "# Save the final linked entities to a JSON file\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(linked_data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(f\"Linked relevant entities saved to '{output_file}'.\")\n",
        "\n",
        "\n",
        "import json\n",
        "import re\n",
        "import csv\n",
        "\n",
        "# Step 1: Load input files\n",
        "# Load original webpage content and linked entities.\n",
        "input_file = 'cleaned_webpage_texts.json'  # The cleaned webpage content.\n",
        "entities_file = 'linked_relevant_entities.json'  # Entities with relevant links generated in the third part.\n",
        "annotated_output_file = 'annotated_text.json'  # Output file for annotated text.\n",
        "structured_output_file = 'structured_data.csv'  # Output file for structured data.\n",
        "\n",
        "try:\n",
        "    # Load cleaned webpage texts\n",
        "    with open(input_file, 'r', encoding='utf-8') as f:\n",
        "        webpage_texts = json.load(f)\n",
        "\n",
        "    # Load linked relevant entities\n",
        "    with open(entities_file, 'r', encoding='utf-8') as f:\n",
        "        linked_entities = json.load(f)\n",
        "\n",
        "    print(\"Input files loaded successfully.\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error loading files: {e}\")\n",
        "    exit(1)\n",
        "\n",
        "# Step 2: Define a function to annotate text with clickable links\n",
        "def annotate_text(text, entities):\n",
        "    \"\"\"\n",
        "    Annotates webpage text by adding clickable HTML links to identified entities.\n",
        "\n",
        "    Args:\n",
        "        text (str): The original webpage text.\n",
        "        entities (list): List of dictionaries containing 'entity', 'type', and 'link'.\n",
        "\n",
        "    Returns:\n",
        "        str: The annotated text with clickable links.\n",
        "    \"\"\"\n",
        "    # Sort entities by length to avoid overlapping matches (longest entities first).\n",
        "    entities = sorted(entities, key=lambda e: len(e['entity']), reverse=True)\n",
        "\n",
        "    for entity in entities:\n",
        "        # Create a safe regex pattern to find the entity in the text\n",
        "        pattern = re.escape(entity['entity'])\n",
        "        link = entity['link']\n",
        "\n",
        "        # Replace the entity in the text with a clickable HTML link\n",
        "        text = re.sub(\n",
        "            pattern,\n",
        "            f\"<a href='{link}' target='_blank'>{entity['entity']}</a>\",\n",
        "            text\n",
        "        )\n",
        "    return text\n",
        "\n",
        "# Step 3: Process each URL and generate annotated text and structured data\n",
        "annotated_texts = {}  # Dictionary to store annotated text for each URL.\n",
        "structured_data = []  # List to store structured data rows for CSV output.\n",
        "preview_data = []  # List to store a preview of entity, type, and link for each URL.\n",
        "\n",
        "for url, text in webpage_texts.items():\n",
        "    if url not in linked_entities:\n",
        "        # Skip URLs that don't have any linked entities\n",
        "        print(f\"Skipping {url} as no linked entities are available.\")\n",
        "        continue\n",
        "\n",
        "    # Get the entities for the current URL\n",
        "    entities = linked_entities[url]\n",
        "\n",
        "    # Generate annotated text for the current URL\n",
        "    annotated_text = annotate_text(text, entities)\n",
        "    annotated_texts[url] = annotated_text  # Store the annotated text\n",
        "\n",
        "    # Add entity details to structured data and preview\n",
        "    for entity in entities:\n",
        "        structured_data.append({\n",
        "            \"URL\": url,\n",
        "            \"Entity\": entity['entity'],\n",
        "            \"Type\": entity['type'],\n",
        "            \"Link\": entity['link']\n",
        "        })\n",
        "        preview_data.append({\n",
        "            \"URL\": url,\n",
        "            \"Preview\": f\"{entity['entity']} ({entity['type']}) → {entity['link']}\"\n",
        "        })\n",
        "\n",
        "# Step 4: Save annotated text to a JSON file\n",
        "try:\n",
        "    with open(annotated_output_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(annotated_texts, f, ensure_ascii=False, indent=4)\n",
        "    print(f\"Annotated text saved to '{annotated_output_file}'.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving annotated text: {e}\")\n",
        "\n",
        "# Step 5: Save structured data to a CSV file\n",
        "try:\n",
        "    with open(structured_output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        fieldnames = ['URL', 'Entity', 'Type', 'Link']\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(structured_data)\n",
        "    print(f\"Structured data saved to '{structured_output_file}'.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving structured data: {e}\")\n",
        "\n",
        "# Step 6: Display a preview of the annotated text and linked entities\n",
        "print(\"\\nPreview of Annotated Text and Linked Entities:\")\n",
        "for row in preview_data[:10]:  # Show the first 10 entries\n",
        "    print(f\"URL: {row['URL']}\")\n",
        "    print(f\"Preview: {row['Preview']}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "# Optional: Save preview data for future use\n",
        "preview_output_file = 'preview_data.json'\n",
        "try:\n",
        "    with open(preview_output_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(preview_data, f, ensure_ascii=False, indent=4)\n",
        "    print(f\"Preview data saved to '{preview_output_file}'.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving preview data: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLyBYA7HvLet",
        "outputId": "b776582f-dc67-44dd-d1fa-73c265d4ec27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping URL: https://thatware.co/advanced-seo-services/\n",
            "Cleaned Preview:\n",
            "In a rapidly evolving digital landscape importance of a robust online presence cannot be overstated internet has become goto platform for businesses both small large seeking exposure recognition ultimately success Its no longer a matter of choice its a matter of survival If your company doesnt adapt embrace advanced search engine optimization SEO youre not just standing still youre falling behind what does advanced SEO entail how can it shape destiny of your business These are questions that Tha\n",
            "================================================================================\n",
            "Cleaning completed. Cleaned data saved to 'cleaned_webpage_texts.json'.\n",
            "Removed items logged in 'removed_log.json'.\n",
            "Successfully loaded text from 'cleaned_webpage_texts.json'.\n",
            "Processing URL: https://thatware.co/advanced-seo-services/\n",
            "Entities for https://thatware.co/advanced-seo-services/:\n",
            "  - Thatware (ORG)\n",
            "  - Picture this You (WORK_OF_ART)\n",
            "  - Prepare (ORG)\n",
            "  - Onsite SEO (ORG)\n",
            "  - Thatwares (PERSON)\n",
            "  - Advanced (PERSON)\n",
            "  - SEMrush (ORG)\n",
            "  - Thatwares Advanced SEO (ORG)\n",
            "  - Unveiling Thatwares Arsenal of Advanced SEO Services Thatware (ORG)\n",
            "  - Google Local Business SEO (ORG)\n",
            "================================================================================\n",
            "Processed entities saved to 'deduplicated_entities.json'.\n",
            "Processing URL: https://thatware.co/advanced-seo-services/\n",
            "Error fetching link for 'Onsite SEO': Expecting value: line 1 column 1 (char 0)\n",
            "Error fetching link for 'Thatwares Advanced SEO': Expecting value: line 1 column 1 (char 0)\n",
            "Error fetching link for 'Google Local Business SEO': Expecting value: line 1 column 1 (char 0)\n",
            "Error fetching link for 'Thatwares SEO': Expecting value: line 1 column 1 (char 0)\n",
            "\n",
            "Preview for https://thatware.co/advanced-seo-services/:\n",
            " - SEMrush (ORG): https://en.wikipedia.org/wiki/Semrush\n",
            " - Google (ORG): https://en.wikipedia.org/wiki/Google\n",
            " - NLP (ORG): https://en.wikipedia.org/wiki/NLP\n",
            " - XML (ORG): https://en.wikipedia.org/wiki/XML_(disambiguation)\n",
            " - India (GPE): https://en.wikipedia.org/wiki/India_(disambiguation)\n",
            "================================================================================\n",
            "Linked relevant entities saved to 'linked_relevant_entities.json'.\n",
            "Input files loaded successfully.\n",
            "Annotated text saved to 'annotated_text.json'.\n",
            "Structured data saved to 'structured_data.csv'.\n",
            "\n",
            "Preview of Annotated Text and Linked Entities:\n",
            "URL: https://thatware.co/advanced-seo-services/\n",
            "Preview: SEMrush (ORG) → https://en.wikipedia.org/wiki/Semrush\n",
            "--------------------------------------------------------------------------------\n",
            "URL: https://thatware.co/advanced-seo-services/\n",
            "Preview: Google (ORG) → https://en.wikipedia.org/wiki/Google\n",
            "--------------------------------------------------------------------------------\n",
            "URL: https://thatware.co/advanced-seo-services/\n",
            "Preview: NLP (ORG) → https://en.wikipedia.org/wiki/NLP\n",
            "--------------------------------------------------------------------------------\n",
            "URL: https://thatware.co/advanced-seo-services/\n",
            "Preview: XML (ORG) → https://en.wikipedia.org/wiki/XML_(disambiguation)\n",
            "--------------------------------------------------------------------------------\n",
            "URL: https://thatware.co/advanced-seo-services/\n",
            "Preview: India (GPE) → https://en.wikipedia.org/wiki/India_(disambiguation)\n",
            "--------------------------------------------------------------------------------\n",
            "URL: https://thatware.co/advanced-seo-services/\n",
            "Preview: Dubai (GPE) → https://en.wikipedia.org/wiki/Dubai_(disambiguation)\n",
            "--------------------------------------------------------------------------------\n",
            "Preview data saved to 'preview_data.json'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Detailed Explanation of the Output\n",
        "\n",
        "This output is from the **final stage of the Entity Recognition and Linking for SEO project**. Here, the system has taken processed webpage content and added clickable links to important entities, saved annotated text, structured data, and previewed results.\n",
        "\n",
        "---\n",
        "\n",
        "### **What Is This Output About?**\n",
        "\n",
        "1. **Annotated Text:**\n",
        "   - The system takes original text from the webpage and adds clickable links for specific words or phrases that were identified as important entities.\n",
        "   - These links provide additional information about these entities by pointing to relevant external sources like Wikipedia.\n",
        "\n",
        "2. **Structured Data:**\n",
        "   - It organizes all the recognized entities into a structured format (CSV), listing:\n",
        "     - The entity's name (e.g., \"SEMrush\").\n",
        "     - Its type (e.g., ORG, which stands for Organization).\n",
        "     - The associated clickable link (e.g., \"https://en.wikipedia.org/wiki/Semrush\").\n",
        "   - This is saved in a file for easy access and further analysis.\n",
        "\n",
        "3. **Preview:**\n",
        "   - A quick overview of the annotated text and linked entities is shown as a preview in the output.\n",
        "   - This preview helps you see what entities were recognized and linked, ensuring the output meets expectations.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step-by-Step Explanation**\n",
        "\n",
        "#### **1. Annotated Text Saved to `'annotated_text.json'`**\n",
        "   - **What does this mean?**\n",
        "     - The processed webpage content, with clickable links added to important words or phrases, is saved to a file called `annotated_text.json`.\n",
        "     - For example:\n",
        "       - Original text: \"NLP is important for SEO.\"\n",
        "       - Annotated text: \"NLP is important for SEO.\"\n",
        "       - The word \"NLP\" becomes a clickable link pointing to \"https://en.wikipedia.org/wiki/NLP\".\n",
        "\n",
        "   - **Why is it useful?**\n",
        "     - It improves the usability of the webpage by linking important terms to reliable sources, enhancing the user experience and SEO performance.\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. Structured Data Saved to `'structured_data.csv'`**\n",
        "   - **What does this mean?**\n",
        "     - All the recognized entities are saved in a tabular format (CSV file). Each row includes:\n",
        "       - The URL of the webpage where the entity was found.\n",
        "       - The entity's name (e.g., \"SEMrush\").\n",
        "       - The entity's type (e.g., ORG for Organization).\n",
        "       - The clickable link associated with the entity (e.g., \"https://en.wikipedia.org/wiki/Semrush\").\n",
        "\n",
        "   - **Why is it useful?**\n",
        "     - It provides a clean and organized view of all recognized entities for further analysis or sharing with others.\n",
        "     - SEO analysts can use this file to track recognized terms and ensure their accuracy and relevance.\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. Preview of Annotated Text and Linked Entities**\n",
        "   - **What does this mean?**\n",
        "     - This is a quick summary of the linked entities for easy verification. For example:\n",
        "       - \"SEMrush (ORG) → https://en.wikipedia.org/wiki/Semrush\" means the word \"SEMrush,\" recognized as an Organization (ORG), is linked to its Wikipedia page.\n",
        "       - Other examples include \"Google (ORG)\" and \"NLP (ORG),\" linked to their respective Wikipedia pages.\n",
        "\n",
        "   - **Why is it useful?**\n",
        "     - The preview allows quick validation of the output, ensuring the correct entities are linked and no irrelevant links are added.\n",
        "\n",
        "---\n",
        "\n",
        "#### **4. Preview Data Saved to `'preview_data.json'`**\n",
        "   - **What does this mean?**\n",
        "     - This saves the preview shown in the console to a JSON file for later review.\n",
        "     - If you want to revisit the preview without re-running the program, this file contains the same information.\n",
        "\n",
        "   - **Why is it useful?**\n",
        "     - It acts as a record for debugging or verifying results after the process is complete.\n",
        "\n",
        "---\n",
        "\n",
        "### **What Does This Output Convey?**\n",
        "\n",
        "1. **Recognized Entities:**\n",
        "   - Important terms (entities) like \"SEMrush,\" \"Google,\" \"NLP,\" and \"India\" are identified and classified.\n",
        "\n",
        "2. **Linked Information:**\n",
        "   - Each entity is linked to a reliable source (like Wikipedia) for more information. For example:\n",
        "     - \"NLP\" links to \"https://en.wikipedia.org/wiki/NLP,\" providing a detailed explanation of Natural Language Processing.\n",
        "\n",
        "3. **Organized Data:**\n",
        "   - The data is organized in multiple formats (annotated text, CSV, and preview JSON) for flexibility and reuse.\n",
        "\n",
        "---\n",
        "\n",
        "### **What Steps Should You Take After Getting This Output?**\n",
        "\n",
        "1. **Review the Output:**\n",
        "   - Manually check the linked entities to ensure:\n",
        "     - The links are correct and contextually relevant.\n",
        "     - Irrelevant links are removed.\n",
        "\n",
        "2. **Integrate Annotated Content:**\n",
        "   - Use the `annotated_text.json` to integrate the clickable text back into the website.\n",
        "   - This enhances the webpage’s user experience and SEO ranking.\n",
        "\n",
        "3. **Analyze the Structured Data:**\n",
        "   - Use the `structured_data.csv` file to:\n",
        "     - Track frequently mentioned terms.\n",
        "     - Identify gaps or opportunities for adding more relevant links.\n",
        "     - Share insights with SEO or content teams.\n",
        "\n",
        "4. **Iterate and Improve:**\n",
        "   - Refine the process to exclude irrelevant terms (if any) or enhance the recognition of more entities.\n",
        "\n",
        "---\n",
        "\n",
        "### **Why Is This Output Important?**\n",
        "\n",
        "1. **Enhanced User Experience:**\n",
        "   - Users can click on terms like \"SEMrush\" or \"Google\" to learn more, making the webpage interactive and informative.\n",
        "\n",
        "2. **Improved SEO:**\n",
        "   - Linking to authoritative sources like Wikipedia improves the credibility of the content, boosting its ranking in search engines.\n",
        "\n",
        "3. **Data-Driven Insights:**\n",
        "   - Structured data provides insights into the most frequently mentioned entities, helping businesses tailor content to target specific keywords.\n",
        "\n",
        "4. **Time-Saving Automation:**\n",
        "   - Automatically generating links for recognized terms saves manual effort and ensures consistency across content.\n",
        "\n",
        "---\n",
        "\n",
        "This output is a clear demonstration of how **Entity Recognition and Linking for SEO** enhances content quality, user experience, and SEO performance."
      ],
      "metadata": {
        "id": "l0ZkbnMv-iMH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Importance of the Annotated Text Output**\n",
        "\n",
        "This annotated text output is a critical deliverable for the Entity Recognition and Linking (ERL) process. Here's why this output is valuable for website owners, especially in the context of improving business and online presence:\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Features and Their Importance**\n",
        "\n",
        "#### **1. Annotated Entities with Clickable Links**\n",
        "- **Feature**: The text now includes clickable hyperlinks for entities like \"Google,\" \"SEMrush,\" and \"India\".\n",
        "- **Importance**:\n",
        "  - **SEO Enhancement**: By linking recognized entities to authoritative external sources, you increase the credibility of your content, improving search engine rankings.\n",
        "  - **User Engagement**: Visitors can interact with the links, gaining more insights. This keeps them engaged longer, which is a positive signal to search engines.\n",
        "  - **Knowledge Enrichment**: By linking to contextual and accurate resources, readers can easily explore complex topics, improving their experience.\n",
        "\n",
        "#### **2. Structured Text Presentation**\n",
        "- **Feature**: Entities are visually highlighted within the text, making them stand out.\n",
        "- **Importance**:\n",
        "  - Helps users identify key topics and concepts quickly.\n",
        "  - Simplifies navigation for readers, especially those seeking specific information.\n",
        "\n",
        "#### **3. Organized Preview**\n",
        "- **Feature**: A preview of entities with their types and associated links is provided, summarizing the annotated content.\n",
        "- **Importance**:\n",
        "  - Gives a quick snapshot for content editors or website owners to verify the relevance and accuracy of the linked entities.\n",
        "  - Assists in quality assurance by identifying potential mismatches or irrelevant links easily.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### **Benefits for Website Owners**\n",
        "\n",
        "1. **Improved Search Engine Optimization (SEO)**\n",
        "   - Properly annotated content with authoritative links boosts the page's ranking on search engines like Google.\n",
        "   - Inbound links and enhanced content relevance lead to better visibility.\n",
        "\n",
        "2. **Enhanced User Experience**\n",
        "   - Readers can seamlessly navigate to supplementary information, enriching their learning journey.\n",
        "   - Well-organized text reduces bounce rates and keeps users on the website for longer.\n",
        "\n",
        "3. **Establishing Authority and Trust**\n",
        "   - By linking to credible sources, your content appears more reliable, helping establish your website as an industry leader.\n",
        "   - Visitors are more likely to trust your information and return for future needs.\n",
        "\n",
        "4. **Facilitating Collaboration**\n",
        "   - Highlighting industry-related entities could open doors for partnerships and collaborations.\n",
        "   - For example, linking to \"SEMrush\" might attract their attention for co-marketing or affiliate opportunities.\n",
        "\n",
        "---\n",
        "\n",
        "### What Does This Output Contain?\n",
        "\n",
        "1. **Annotated Text**:\n",
        "   - This is the original webpage text where specific entities (like \"SEMrush,\" \"Google,\" \"NLP\") are recognized and linked to relevant resources (e.g., Wikipedia pages).\n",
        "   - Example:\n",
        "     - \"SEMrush\" in the text is linked to its Wikipedia page: [https://en.wikipedia.org/wiki/Semrush](https://en.wikipedia.org/wiki/Semrush).\n",
        "\n",
        "2. **Structured Data**:\n",
        "   - A CSV file is created that lists:\n",
        "     - **URL**: The source webpage where the entity was found.\n",
        "     - **Entity**: The recognized term (e.g., \"Google\").\n",
        "     - **Type**: What kind of entity it is (e.g., ORG for Organization, GPE for Geopolitical Entity).\n",
        "     - **Link**: The resource URL linked to the entity (e.g., Wikipedia or another relevant site).\n",
        "\n",
        "3. **Preview**:\n",
        "   - A summary showcasing linked entities for a URL in a human-readable format.\n",
        "\n",
        "---\n",
        "\n",
        "### How is This Output Helpful?\n",
        "\n",
        "#### 1. **Improved User Experience**:\n",
        "   - By linking entities to authoritative sources (e.g., Wikipedia), the output improves the **user's understanding** of the content.\n",
        "   - Example: When a visitor reads \"NLP\" and clicks the link, they are directed to a page explaining Natural Language Processing.\n",
        "\n",
        "#### 2. **Enhanced SEO**:\n",
        "   - Outbound links to high-quality, authoritative sites improve a webpage's SEO score. Search engines value content that provides meaningful references.\n",
        "   - This increases the chances of the website ranking higher on search engine results pages (SERPs).\n",
        "\n",
        "#### 3. **Building Authority and Credibility**:\n",
        "   - Linking to trusted sources builds trust with both users and search engines, establishing the website as a credible resource.\n",
        "\n",
        "#### 4. **Ease of Content Navigation**:\n",
        "   - Users can explore linked entities directly, enhancing their engagement with the content.\n",
        "\n",
        "#### 5. **Data-Driven Decisions**:\n",
        "   - The structured data output allows website owners to analyze which entities are most frequently recognized and linked. This insight helps in crafting future content strategies.\n",
        "\n",
        "---\n",
        "\n",
        "### Next Steps for Website Owners\n",
        "\n",
        "1. **Review the Links**:\n",
        "   - Verify the relevance and accuracy of the linked resources.\n",
        "   - Replace or update any incorrect or generic links.\n",
        "\n",
        "2. **Integrate the Annotated Content**:\n",
        "   - Publish the annotated text on the website to enhance user experience and SEO.\n",
        "\n",
        "3. **Leverage Structured Data**:\n",
        "   - Use the CSV file to track recognized entities and identify potential gaps or trends in the content.\n",
        "\n",
        "4. **Enhance Internal Linking**:\n",
        "   - Instead of linking externally, consider linking to internal pages where appropriate. For example, \"Google\" could link to an internal blog post about Google's services.\n",
        "\n",
        "5. **Analyze Traffic and Engagement**:\n",
        "   - Monitor how users interact with the links and adjust content strategies based on performance.\n",
        "\n",
        "---\n",
        "\n",
        "### Importance of the Output in a Business Context\n",
        "\n",
        "1. **Increased Traffic**:\n",
        "   - High-quality content with authoritative links attracts more visitors, leading to higher organic traffic.\n",
        "\n",
        "2. **Improved User Retention**:\n",
        "   - Visitors are more likely to stay and explore a website with valuable, informative content.\n",
        "\n",
        "3. **Higher Conversion Rates**:\n",
        "   - Relevant links and well-structured content can guide users toward conversion goals, such as signing up for a service or purchasing a product.\n",
        "\n",
        "4. **Brand Reputation**:\n",
        "   - Linking to trusted sources demonstrates professionalism and commitment to providing valuable information, enhancing brand perception.\n",
        "\n",
        "---\n",
        "\n",
        "### Summary\n",
        "\n",
        "This output equips website owners with actionable data to enhance content quality, improve SEO performance, and provide a better user experience. By strategically leveraging this information, businesses can establish a stronger online presence, attract and retain users, and ultimately achieve their goals in the competitive digital landscape."
      ],
      "metadata": {
        "id": "NsPe_ILSAPHu"
      }
    }
  ]
}