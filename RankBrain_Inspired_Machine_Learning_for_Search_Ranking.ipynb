{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNWLJEpLAY1yIBINrX5PSHy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abhiss123/AlmaBetter-Projects/blob/main/RankBrain_Inspired_Machine_Learning_for_Search_Ranking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name: RankBrain-Inspired Machine Learning for Search Ranking**\n",
        "\n",
        "### **Purpose of the Project:**\n",
        "**\"RankBrain-Inspired Machine Learning for Search Ranking\"** aims to build a machine learning model that can **analyze search queries** and **rank web pages** based on their relevance to those queries. This is similar to how **Google's RankBrain** works, where the system tries to understand what a user is searching for and provides the most relevant results.\n",
        "\n",
        "Here’s a simple breakdown of what this project is about and why it’s important:\n",
        "\n",
        "### 1. **Understanding User Search Queries**:\n",
        "- When someone types a query into Google, they are trying to find **specific information**.\n",
        "- The goal of this project is to create a system that takes these **search queries** and finds the most relevant **web pages** from a website.\n",
        "- It works by analyzing both the **content on the website** and the **words used in the query** to understand which pages match best.\n",
        "\n",
        "### 2. **Ranking Pages Based on Relevance**:\n",
        "- Not all web pages are equally relevant to every search. Some pages might provide the exact information a user is looking for, while others may be less helpful.\n",
        "- This system ranks web pages based on **how closely they match the search query**. The more relevant the content of a page, the higher it will rank.\n",
        "\n",
        "### 3. **How RankBrain-Inspired Machine Learning Works**:\n",
        "- **RankBrain** is part of Google's search algorithm that uses **artificial intelligence (AI)** to better understand and match search queries with relevant web pages.\n",
        "- In this project, we're using **machine learning techniques** like:\n",
        "  - **TF-IDF** (Term Frequency-Inverse Document Frequency): This method looks at how important each word is in a document or web page.\n",
        "  - **Cosine Similarity**: This technique compares how similar the search query is to the content of the web pages.\n",
        "- By using these methods, the system can automatically figure out which web pages are most relevant to the user’s search.\n",
        "\n",
        "### 4. **Practical Use of the System**:\n",
        "- Imagine you own a website with many pages (like a business website offering various services).\n",
        "- If a user searches for \"SEO services,\" the system will check all the pages on the website and rank them based on which ones are most relevant to SEO services.\n",
        "- The page about \"Advanced SEO Services\" will likely be ranked higher than a page about \"Web Design Services\" because it is more related to what the user is searching for.\n",
        "\n",
        "### 5. **Why This Is Useful**:\n",
        "- **Improving User Experience**: This system helps users quickly find the information they need, which leads to a better experience on the website.\n",
        "- **SEO (Search Engine Optimization)**: By knowing which pages are most relevant for certain queries, website owners can optimize their content to improve how well their site ranks in search engines like Google.\n",
        "- **Content Strategy**: Website owners can see which pages are less relevant and update them to make them more useful to users searching for specific topics.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JUxAxpdxU56l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Understanding RankBrain and Neural Matching:**\n",
        "1. **RankBrain** is a machine learning-based algorithm that helps Google understand queries that it hasn’t seen before or that are complex. Instead of just matching keywords, RankBrain looks at the *intent* behind a search query and finds relevant pages, even if those pages don't have the exact search terms. It adjusts search results based on what users seem to prefer by learning over time.\n",
        "   \n",
        "2. **Neural Matching** is an AI system that focuses on understanding the broader concepts behind search queries. It uses deep learning to match queries to pages, even when they use different words but have similar meanings. For example, if someone searches for “why my TV looks strange,” Neural Matching might understand that this could relate to “motion smoothing” and show results accordingly.\n",
        "\n",
        "### **Use Cases for RankBrain and Neural Matching:**\n",
        "- **RankBrain:** Imagine someone types a query like, “best way to fix a laptop screen without replacing it.” If websites don’t use that exact phrase but offer relevant content (like “laptop screen repair tips”), RankBrain understands this and ranks those pages higher.\n",
        "- **Neural Matching:** If a person searches for “movie about a kid’s adventure in space,” Neural Matching understands this concept and may show results for “sci-fi movies about space travel for children,” even if none of the exact words match the query.\n",
        "\n",
        "### **Real-Life Implementation (In the Context of Websites):**\n",
        "- **RankBrain** helps websites get ranked even when they don’t include the exact keywords. For example, if a website sells “running shoes” but the search query is “footwear for jogging,” RankBrain could still rank that site because it understands the relationship between “running shoes” and “footwear for jogging.”\n",
        "- **Neural Matching** enhances how well Google can match the *idea* behind a search to your content. For example, if you write about \"best foods for a healthy gut,\" and someone searches for \"foods that improve digestion,\" Neural Matching connects those concepts, potentially ranking your page higher, even if you don’t use the exact same words.\n",
        "\n",
        "### **How to Optimize Website Content for RankBrain and Neural Matching:**\n",
        "1. **Focus on User Intent:** Instead of stuffing your content with exact keywords, write content that answers real questions people might have. This is what RankBrain looks for—it tries to understand what users *mean* when they type something into Google.\n",
        "2. **Write Naturally:** Create high-quality content that explains things clearly, even when people use different ways to phrase their queries. This helps Neural Matching because it understands the broader concepts and can match your content with queries more easily.\n",
        "3. **Use Synonyms and Related Terms:** Since Neural Matching connects related ideas, you should use a variety of terms that relate to your main topic. For example, if your website is about fitness, include terms like “exercise,” “workout,” and “physical activity” throughout your content.\n",
        "\n",
        "### **What Kind of Data Does RankBrain and Neural Matching Use?**\n",
        "Both RankBrain and Neural Matching do not directly require **URLs or CSV data from your website** to operate. These systems are already built into Google’s algorithm. What they need from your website is **high-quality content** that answers users' queries. Google crawls and processes the text content on your website itself, so you don’t need to worry about providing them with your data in CSV format or URLs for RankBrain or Neural Matching to work.\n",
        "\n",
        "However, if you’re building or optimizing content for a website, you will need to focus on the **text content** of the pages. Tools that analyze how well your content matches search queries (such as SEO tools) can process your content by crawling your website or using CSV files with the relevant data. These tools help you align your website’s content with what Google’s algorithms (like RankBrain and Neural Matching) prefer.\n",
        "\n",
        "### **How Does Google Use RankBrain and Neural Matching to Influence Search Results?**\n",
        "RankBrain and Neural Matching make search results smarter by focusing on **concepts, intent, and relevance** rather than just looking at the literal words someone types. This means that well-written, informative content has a better chance of ranking, even if it doesn't match the exact search terms users type. **Google adapts search results over time** by learning from user behavior (for instance, which results people click on most often), and RankBrain helps adjust those rankings to reflect what users find most helpful.\n",
        "\n"
      ],
      "metadata": {
        "id": "gXN5XKL9Y_u1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Can We Write a Code for RankBrain and Neural Matching?**\n",
        "- **RankBrain** and **Neural Matching** are proprietary AI systems developed by Google, specifically designed to improve search results. These algorithms are deeply integrated into Google’s entire search engine infrastructure, and they aren’t publicly available for coding or direct use by developers. They are not open-source, and Google hasn’t released them for external use.\n",
        "- **In simple terms:** You can’t recreate Google’s RankBrain or Neural Matching exactly because Google hasn’t provided the code or framework for those. These systems are part of how Google’s search engine works behind the scenes.\n",
        "\n",
        "### **How Do These Systems Work?**\n",
        "Google’s **RankBrain** uses **machine learning** to understand new search queries and adapt over time based on user behavior. It identifies patterns and improves search results by figuring out what people actually mean, even if they use unfamiliar or new phrases. **Neural Matching**, on the other hand, uses **deep learning** to understand the relationship between different words and concepts, matching queries with pages that may not use the exact same words but are about the same thing.\n",
        "\n",
        "For example, if someone searches “how to fix my fridge making noise,” Google’s Neural Matching might find a webpage about “common refrigerator problems,” even if that page doesn’t have the exact phrase \"fix fridge making noise.\"\n",
        "\n",
        "### **What Kind of Data Do These Models Need?**\n",
        "Google’s RankBrain and Neural Matching models use **huge amounts of data**, including:\n",
        "- **Search Queries:** What people type into Google, whether it’s a specific question, keyword, or phrase.\n",
        "- **User Behavior:** How users interact with search results (like which links they click, how long they stay on a page, etc.).\n",
        "- **Content from Web Pages:** The text on web pages that Google has crawled (analyzed), including the page’s topic, keywords, and relevance.\n",
        "- **Contextual Data:** Data that helps understand the context of words and sentences. For example, the phrase “jaguar” might mean the animal in one context or the car in another.\n",
        "\n",
        "### **Can We Write a Similar Code or Model?**\n",
        "While We can’t replicate **Google’s RankBrain or Neural Matching** exactly, We can build a Model of **machine learning** or **natural language processing (NLP)** model that mimics certain aspects of how these systems work.\n",
        "\n",
        "1. **For RankBrain-like Systems:** We can build a model that understands search queries and ranks content based on relevance. This involves using **machine learning** techniques to teach your system how to predict the best results for a query. We’ll need:\n",
        "   - **Text Data (content of web pages)**: We’d collect text data from the websites or documents We want to rank.\n",
        "   - **Search Query Data**: Collect search queries and train your model to understand them.\n",
        "   - **User Interaction Data**: Data showing how users engage with content (e.g., clicks, time spent on a page).\n",
        "\n",
        "2. **For Neural Matching-like Systems:** We could use **deep learning** models that understand the broader meanings of words and phrases. This would require:\n",
        "   - **A large text corpus** (e.g., thousands of articles or documents) to train the model on how different words and phrases relate to each other.\n",
        "   - **Natural Language Processing (NLP) techniques** like word embeddings (e.g., Word2Vec, BERT, or GloVe) to understand the relationships between words.\n",
        "   - **Conceptual Mapping**: Our model would be trained to understand how different terms, even if not identical, relate to the same concept (e.g., “jogging shoes” = “running footwear”).\n",
        "\n",
        "### **What Kind of Data to Feed into Such Models:**\n",
        "- **Text Data**: We would use web page content or articles to train the model. For example, if We want to create a search engine for a shopping site, We’d collect descriptions of products, categories, and other details.\n",
        "- **User Query Data**: Collect a set of common search queries related to our domain, so the model knows what people are looking for.\n",
        "- **Interaction Data (Optional)**: If we have access to user interaction data (like what links they clicked on), this can help improve the model by showing which search results are most relevant to users.\n",
        "\n",
        "### **How to Build a Model Like Google’s RankBrain**\n",
        "We can’t build the exact RankBrain or Neural Matching models, but here’s what we can do:\n",
        "- Use Python and machine learning libraries like **scikit-learn** or **TensorFlow**.\n",
        "- Use **Natural Language Processing (NLP)** tools such as **spaCy** or **NLTK** to process and analyze text data.\n",
        "- Train our model on web page content and search queries to predict which pages are most relevant for a given query.\n",
        "\n",
        "**Here’s an example workflow:**\n",
        "1. **Collect Data:** Gather text from your website, including page titles, headings, and body text. Also collect common search queries users might type.\n",
        "2. **Process the Data:** Use **Natural Language Processing (NLP)** to analyze the words and phrases in both the content and the search queries.\n",
        "3. **Build a Model:** Create a machine learning model that looks at the search queries and ranks the relevant pages.\n",
        "4. **Train the Model:** Teach the model by showing it many examples of search queries and the best pages that match. Over time, it will learn to predict what content is most relevant.\n"
      ],
      "metadata": {
        "id": "wI9vu6C7Z1AC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJphWJLW1BV5",
        "outputId": "32d91c6e-b01f-43a0-f928-961414e1c9d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 1: Loading and Preprocessing the Data**\n"
      ],
      "metadata": {
        "id": "lkj_MB02ecL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries to handle data and text processing\n",
        "import pandas as pd  # This is for loading and handling CSV data\n",
        "\n",
        "# Step 1: Load the datasets containing user behavior and search queries\n",
        "# The datasets contain the search queries, user interactions, and page content information\n",
        "# These CSV files store the data needed for the model to understand the queries and website pages.\n",
        "pagewise_user_flow_data = pd.read_csv('/content/drive/MyDrive/Rank Brain Datasets/Pagewise User flow data.csv')\n",
        "user_flow_data = pd.read_csv('/content/drive/MyDrive/Rank Brain Datasets/User Flow Data.csv')\n",
        "search_query_data = pd.read_csv('/content/drive/MyDrive/Rank Brain Datasets/Search query.csv')\n",
        "\n",
        "# Step 2: Preprocess the data by selecting the necessary columns\n",
        "# We'll focus on 'Keyword' and 'URL' from the search query data and 'Page path' from the pagewise data.\n",
        "# These columns contain the information we need to match search queries with page content.\n",
        "search_queries = search_query_data[['Keyword', 'URL']]  # Extract search queries and their associated URLs\n",
        "pagewise_data = pagewise_user_flow_data[['Page path and screen class']]  # Extract page paths from the user flow data\n",
        "\n",
        "# Step 3: Handle missing data (NaN values) that might exist in the pagewise data\n",
        "# We will fill the missing values with an empty string to avoid any errors during text processing.\n",
        "# Missing data can cause problems when analyzing text, so we fill them with empty values.\n",
        "pagewise_data = pagewise_data.fillna('')\n",
        "\n",
        "# Step 4: Combine both search queries and page content into one dataset for TF-IDF vectorization\n",
        "# We will create a single dataset containing both search queries and page content so that the same vocabulary is used.\n",
        "# This ensures that the words used in the search queries and the website pages are processed together, creating a shared understanding of language.\n",
        "combined_text = pd.concat([search_queries['Keyword'], pagewise_data['Page path and screen class']], axis=0)\n",
        "\n",
        "# Display the combined text to see how the search queries and page paths are combined.\n",
        "print(\"Combined text data for queries and pages:\")\n",
        "print(combined_text.head(20))  # Show the first few rows of the combined text data to ensure it looks correct.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZguSvaAeWFE",
        "outputId": "6e1ef71f-fb3f-424b-fdc5-687ed212f506"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined text data for queries and pages:\n",
            "0                        webpage click depth visualizer\n",
            "1                                               textise\n",
            "2                                    escort seo company\n",
            "3                                              thatware\n",
            "4     online reputation management with rapid url in...\n",
            "5                                click depth visualizer\n",
            "6                                         ai seo agency\n",
            "7                                        seo madagascar\n",
            "8                                           seo tunisia\n",
            "9                                        seo costa rica\n",
            "10                             keyword clustering tools\n",
            "11                              keyword clustering tool\n",
            "12                                             edge seo\n",
            "13                                 history replacestate\n",
            "14                                        alternate tag\n",
            "15                     how to measure brand recognition\n",
            "16                       360 international seo services\n",
            "17                                         history.push\n",
            "18                                         zombie pages\n",
            "0                                                     /\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 2: Vectorizing the Data and Comparing Queries with Pages**\n"
      ],
      "metadata": {
        "id": "nxlUeL3PemvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer  # This will help us convert text into numbers\n",
        "\n",
        "# Step 5: Convert the text data into numerical form using TF-IDF (Term Frequency-Inverse Document Frequency)\n",
        "# TF-IDF measures the importance of each word in the entire dataset.\n",
        "# By fitting the vectorizer on both queries and pages together, we ensure the same vocabulary is used.\n",
        "vectorizer = TfidfVectorizer(stop_words='english')  # The 'stop_words' argument removes common words like 'the', 'and'\n",
        "combined_vectors = vectorizer.fit_transform(combined_text)  # Fit the TF-IDF on combined text\n",
        "\n",
        "# Step 6: Split the vectors back into search query vectors and page vectors\n",
        "# After creating the combined TF-IDF matrix, we split it into two parts: one for queries and one for pages.\n",
        "query_vectors = combined_vectors[:len(search_queries)]  # This gets the first part (search queries)\n",
        "page_vectors = combined_vectors[len(search_queries):]  # This gets the second part (page content)\n",
        "\n",
        "# Step 7: Function to calculate similarity between search queries and web pages\n",
        "# Cosine similarity helps measure how similar the query is to the page content\n",
        "from sklearn.metrics.pairwise import cosine_similarity  # This will help measure the similarity between search queries and pages\n",
        "\n",
        "def find_relevant_pages(query_vector, page_vectors):\n",
        "    # Calculate the similarity between a query and all pages\n",
        "    similarities = cosine_similarity(query_vector, page_vectors)  # Compare the query with every page\n",
        "    return similarities  # Return the similarity scores\n",
        "\n",
        "# Example: Check the similarity between the first search query and the pages\n",
        "sample_query_vector = query_vectors[0]  # Get the vector for the first search query\n",
        "similarity_scores = find_relevant_pages(sample_query_vector, page_vectors)  # Get similarity scores\n",
        "print(f\"Query Vectors :  {query_vectors}\")\n",
        "\n",
        "print()  # This will print a blank line to create a gap\n",
        "\n",
        "print(f\"Page Vectors :  {page_vectors}\")\n",
        "\n",
        "print(f\"Similarity scores for the first search query: {similarity_scores[0][:5]}\")  # Display the first few similarity scores\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsqJIl4YeiIV",
        "outputId": "9c36bfa8-0b4e-4bd0-f757-2ff8ca2ada05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query Vectors :    (0, 1197)\t0.532850087395224\n",
            "  (0, 262)\t0.4813274215587225\n",
            "  (0, 344)\t0.4813274215587225\n",
            "  (0, 1183)\t0.5027112599436836\n",
            "  (1, 1107)\t1.0\n",
            "  (2, 418)\t0.810616978520908\n",
            "  (2, 1001)\t0.1639530477245356\n",
            "  (2, 276)\t0.5621561280244751\n",
            "  (3, 1110)\t1.0\n",
            "  (4, 818)\t0.3747968134425224\n",
            "  (4, 954)\t0.41759605798857885\n",
            "  (4, 722)\t0.3998327665611599\n",
            "  (4, 929)\t0.44263201110721634\n",
            "  (4, 1157)\t0.36527858581655415\n",
            "  (4, 591)\t0.44263201110721634\n",
            "  (5, 262)\t0.5688039805746846\n",
            "  (5, 344)\t0.5688039805746846\n",
            "  (5, 1183)\t0.5940741227867001\n",
            "  (6, 1001)\t0.17348085600214944\n",
            "  (6, 58)\t0.6651538642052938\n",
            "  (6, 54)\t0.7262745552017689\n",
            "  (7, 1001)\t0.19824289805868175\n",
            "  (7, 711)\t0.9801529234610766\n",
            "  (8, 1001)\t0.20961998186473432\n",
            "  (8, 1139)\t0.9777829325586679\n",
            "  :\t:\n",
            "  (9, 304)\t0.6991195161751151\n",
            "  (9, 962)\t0.6991195161751151\n",
            "  (10, 637)\t0.47558241816137226\n",
            "  (10, 267)\t0.5896608236426717\n",
            "  (10, 1124)\t0.652779807130113\n",
            "  (11, 637)\t0.48712800594656375\n",
            "  (11, 267)\t0.6039758625147452\n",
            "  (11, 1123)\t0.6308085789858078\n",
            "  (12, 1001)\t0.20961998186473432\n",
            "  (12, 391)\t0.9777829325586679\n",
            "  (13, 557)\t0.7071067811865475\n",
            "  (13, 951)\t0.7071067811865475\n",
            "  (14, 68)\t0.7410834354552656\n",
            "  (14, 1087)\t0.6714129442405927\n",
            "  (15, 744)\t0.6018533004655682\n",
            "  (15, 189)\t0.5249240034875573\n",
            "  (15, 936)\t0.6018533004655682\n",
            "  (16, 1001)\t0.15840042943057323\n",
            "  (16, 31)\t0.6631406143570872\n",
            "  (16, 609)\t0.7074375686978012\n",
            "  (16, 1005)\t0.18624155267087647\n",
            "  (17, 557)\t0.6862366763391348\n",
            "  (17, 918)\t0.7273783225028208\n",
            "  (18, 1229)\t0.7666261321349791\n",
            "  (18, 838)\t0.6420937420094995\n",
            "\n",
            "Page Vectors :    (1, 1005)\t1.0\n",
            "  (2, 1001)\t0.16412868859402133\n",
            "  (2, 31)\t0.687121870686386\n",
            "  (2, 833)\t0.707760770603209\n",
            "  (3, 292)\t1.0\n",
            "  (4, 1001)\t0.2419441068423264\n",
            "  (4, 1005)\t0.2844692169072248\n",
            "  (4, 48)\t0.9276531214826258\n",
            "  (5, 1001)\t0.16412868859402133\n",
            "  (5, 410)\t0.687121870686386\n",
            "  (5, 899)\t0.707760770603209\n",
            "  (6, 174)\t1.0\n",
            "  (8, 58)\t1.0\n",
            "  (9, 1005)\t0.20367779912427864\n",
            "  (9, 359)\t0.7470083427999767\n",
            "  (9, 731)\t0.6328458658560728\n",
            "  (10, 4)\t0.39973348131068714\n",
            "  (10, 1113)\t0.4583157055783814\n",
            "  (10, 668)\t0.4583157055783814\n",
            "  (10, 401)\t0.4583157055783814\n",
            "  (10, 784)\t0.4583157055783814\n",
            "  (11, 1107)\t1.0\n",
            "  (12, 731)\t0.4630767217922527\n",
            "  (12, 357)\t0.6267216087443215\n",
            "  (12, 995)\t0.6267216087443215\n",
            "  :\t:\n",
            "  (928, 407)\t0.4194959874436908\n",
            "  (928, 108)\t0.4644000394730175\n",
            "  (929, 823)\t0.29823530183822294\n",
            "  (929, 335)\t0.37459193541436414\n",
            "  (929, 430)\t0.3431393538675496\n",
            "  (929, 273)\t0.3383384279881565\n",
            "  (929, 567)\t0.3832424800174832\n",
            "  (929, 407)\t0.4194959874436908\n",
            "  (929, 497)\t0.4644000394730175\n",
            "  (930, 823)\t0.3367510162956794\n",
            "  (930, 335)\t0.42296875711708753\n",
            "  (930, 430)\t0.38745421965043325\n",
            "  (930, 273)\t0.3820332763245394\n",
            "  (930, 567)\t0.43273647967929324\n",
            "  (930, 407)\t0.4736719604718414\n",
            "  (931, 1110)\t0.5574277511017001\n",
            "  (931, 823)\t0.4104399265012786\n",
            "  (931, 335)\t0.5155241029207309\n",
            "  (931, 569)\t0.505023037437998\n",
            "  (933, 1001)\t0.20352947744705846\n",
            "  (933, 1005)\t0.2393026712761932\n",
            "  (933, 1028)\t0.9493734688362664\n",
            "  (934, 1001)\t0.19306762676278721\n",
            "  (934, 1005)\t0.22700200187615507\n",
            "  (934, 506)\t0.9545653370201556\n",
            "Similarity scores for the first search query: [0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 3: Ranking and Displaying the Most Relevant Pages**\n"
      ],
      "metadata": {
        "id": "quQf_C2ZewXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Function to rank the most relevant pages for a given search query\n",
        "# This function will return the top web pages that match the search query\n",
        "def rank_pages_for_query(search_query):\n",
        "    # Convert the input query into a TF-IDF vector\n",
        "    query_vector = vectorizer.transform([search_query])  # Transform the query into a vector (using the fitted vectorizer)\n",
        "    # Calculate how similar this query is to each page (using cosine similarity)\n",
        "    similarities = find_relevant_pages(query_vector, page_vectors)\n",
        "    # Sort the pages by similarity (higher similarity means more relevance)\n",
        "    top_results = similarities.argsort()[0][::-1]  # Sort the similarity scores in descending order\n",
        "    top_pages = pagewise_data.iloc[top_results]  # Retrieve the pages corresponding to the best matches\n",
        "    return top_pages  # Return the top pages\n",
        "\n",
        "# Step 9: Test the model with a sample search query\n",
        "# You can input any query to see which pages are the most relevant to that query\n",
        "sample_query = \"SEO services\"  # You can change this to any other query to test\n",
        "top_pages = rank_pages_for_query(sample_query)  # Get the top pages for the query\n",
        "\n",
        "# Step 10: Display the top-ranking pages for the sample query\n",
        "print(f\"Top pages for the query '{sample_query}':\")\n",
        "print(top_pages.head())  # Show the top 5 relevant pages\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLXosb8EesD2",
        "outputId": "06933a73-97f6-404e-f94b-5636ba4d3c32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top pages for the query 'SEO services':\n",
            "    Page path and screen class\n",
            "1                   /services/\n",
            "4      /advanced-seo-services/\n",
            "33       /seo-services-canada/\n",
            "869    /seo-services-in-india/\n",
            "872        /seo-services-italy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries to handle data and text processing\n",
        "import pandas as pd  # This is for loading and handling CSV data\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer  # This will help us convert text into numbers\n",
        "from sklearn.metrics.pairwise import cosine_similarity  # This will help measure the similarity between search queries and pages\n",
        "\n",
        "# Step 1: Load the datasets containing user behavior and search queries\n",
        "# The datasets contain the search queries, user interactions, and page content information\n",
        "pagewise_user_flow_data = pd.read_csv('/content/drive/MyDrive/Rank Brain Datasets/Pagewise User flow data.csv')\n",
        "user_flow_data = pd.read_csv('/content/drive/MyDrive/Rank Brain Datasets/User Flow Data.csv')\n",
        "search_query_data = pd.read_csv('/content/drive/MyDrive/Rank Brain Datasets/Search query.csv')\n",
        "\n",
        "# Step 2: Preprocess the data by selecting the necessary columns\n",
        "# We'll focus on 'Keyword' and 'URL' from the search query data and 'Page path' from the pagewise data\n",
        "search_queries = search_query_data[['Keyword', 'URL']]  # Extract search queries and their associated URLs\n",
        "pagewise_data = pagewise_user_flow_data[['Page path and screen class']]  # Extract page paths from the user flow data\n",
        "\n",
        "# Step 3: Handle missing data (NaN values) that might exist in the pagewise data\n",
        "# We will fill the missing values with an empty string to avoid any errors during text processing\n",
        "pagewise_data = pagewise_data.fillna('')\n",
        "\n",
        "# Step 4: Combine both search queries and page content into one dataset for TF-IDF vectorization\n",
        "# We will create a single dataset containing both search queries and page content so that the same vocabulary is used.\n",
        "combined_text = pd.concat([search_queries['Keyword'], pagewise_data['Page path and screen class']], axis=0)\n",
        "\n",
        "# Step 5: Convert the text data into numerical form using TF-IDF (Term Frequency-Inverse Document Frequency)\n",
        "# TF-IDF measures the importance of each word in the entire dataset.\n",
        "# By fitting the vectorizer on both queries and pages together, we ensure the same vocabulary is used.\n",
        "vectorizer = TfidfVectorizer(stop_words='english')  # The 'stop_words' argument removes common words like 'the', 'and'\n",
        "combined_vectors = vectorizer.fit_transform(combined_text)  # Fit the TF-IDF on combined text\n",
        "\n",
        "# Step 6: Split the vectors back into search query vectors and page vectors\n",
        "# After creating the combined TF-IDF matrix, we split it into two parts: one for queries and one for pages\n",
        "query_vectors = combined_vectors[:len(search_queries)]  # This gets the first part (search queries)\n",
        "page_vectors = combined_vectors[len(search_queries):]  # This gets the second part (page content)\n",
        "\n",
        "# Step 7: Function to calculate similarity between search queries and web pages\n",
        "# Cosine similarity helps measure how similar the query is to the page content\n",
        "def find_relevant_pages(query_vector, page_vectors):\n",
        "    # Calculate the similarity between a query and all pages\n",
        "    similarities = cosine_similarity(query_vector, page_vectors)  # Compare the query with every page\n",
        "    return similarities  # Return the similarity scores\n",
        "\n",
        "# Step 8: Function to rank the most relevant pages for a given search query\n",
        "# This function will return the top web pages that match the search query\n",
        "def rank_pages_for_query(search_query):\n",
        "    # Convert the input query into a TF-IDF vector\n",
        "    query_vector = vectorizer.transform([search_query])  # Transform the query into a vector (using the fitted vectorizer)\n",
        "    # Calculate how similar this query is to each page (using cosine similarity)\n",
        "    similarities = find_relevant_pages(query_vector, page_vectors)\n",
        "    # Sort the pages by similarity (higher similarity means more relevance)\n",
        "    top_results = similarities.argsort()[0][::-1]  # Sort the similarity scores in descending order\n",
        "    top_pages = pagewise_data.iloc[top_results]  # Retrieve the pages corresponding to the best matches\n",
        "    return top_pages  # Return the top pages\n",
        "\n",
        "# Step 9: Test the model with a sample search query\n",
        "# You can input any query to see which pages are the most relevant to that query\n",
        "sample_query = \"SEO services\"  # You can change this to any other query to test\n",
        "top_pages = rank_pages_for_query(sample_query)  # Get the top pages for the query\n",
        "\n",
        "# Step 10: Display the top-ranking pages for the sample query\n",
        "print(f\"Top pages for the query '{sample_query}':\")\n",
        "print(top_pages)  # This will show the top relevant pages for the search query\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJZBPORCNHec",
        "outputId": "a40850e5-a8e1-4a50-af23-8eda4e4cfd73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top pages for the query 'SEO services':\n",
            "              Page path and screen class\n",
            "1                             /services/\n",
            "4                /advanced-seo-services/\n",
            "33                 /seo-services-canada/\n",
            "869              /seo-services-in-india/\n",
            "872                  /seo-services-italy\n",
            "..                                   ...\n",
            "221  /crawl-compare-with-screaming-frog/\n",
            "222  /gephi-report-the-definitive-guide/\n",
            "223                    /google-discover/\n",
            "224        /regex-google-search-console/\n",
            "0                                      /\n",
            "\n",
            "[935 rows x 1 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What Is This Output?\n",
        "\n",
        "The output you see is a **list of web pages** on your website that are ranked as **most relevant** to the search query `\"SEO services\"`. The model has analyzed the **content** of all the pages on your website and compared them to the search query **“SEO services”** to determine which pages match this search query best.\n",
        "\n",
        "Here’s a simplified breakdown of what the different parts of the output mean:\n",
        "\n",
        "1. **Page Paths**: These are the **URLs** (web addresses) or paths of your website's pages.\n",
        "   - Example: `/services/`, `/advanced-seo-services/`, `/seo-services-in-india/`, etc.\n",
        "   - These represent the specific sections or pages of your website where the **content is most relevant** to the query `\"SEO services\"`.\n",
        "\n",
        "2. **Ranking of Pages**: The **order of the pages** shown is based on how well they match the search query. The top pages (like `/services/` and `/advanced-seo-services/`) are more relevant to the search query, while the pages towards the bottom (like `/regex-google-search-console/`) are less relevant.\n",
        "\n",
        "3. **Number of Results (935 rows)**: The output lists **935 pages** from your website that are analyzed in response to the query `\"SEO services\"`. These are ranked from most relevant to least relevant based on the content on each page.\n",
        "\n",
        "### Breaking Down the Output:\n",
        "\n",
        "- **Top-Ranked Pages**:\n",
        "   - `/services/` and `/advanced-seo-services/` are at the top of the list. This means that these pages are the most relevant to the query `\"SEO services\"`.\n",
        "   - These are pages that likely contain content related to SEO services, which is why they were ranked at the top by the model.\n",
        "\n",
        "- **Lower-Ranked Pages**:\n",
        "   - Pages like `/regex-google-search-console/` and `/gephi-report-the-definitive-guide/` are ranked lower because they are likely less relevant to the search query `\"SEO services\"`.\n",
        "   - These pages may contain content related to **technical tools** like Google Search Console and are not specifically about SEO services.\n",
        "\n",
        "### What Does This Output Mean?\n",
        "\n",
        "- This output helps you **identify the pages on your website that are most relevant to a given search query**.\n",
        "- For example, if a user searches for “SEO services” on Google, the pages at the top of this list are the ones that are most likely to show up in search results (assuming other SEO factors like backlinks and site speed are good).\n",
        "- **Relevance to the query** is an important factor in how search engines like Google rank pages.\n",
        "\n",
        "### What Steps Should You Take as a Website Owner?\n",
        "\n",
        "1. **Focus on the Top-Ranked Pages**:\n",
        "   - The pages that are ranked at the top (e.g., `/services/` and `/advanced-seo-services/`) are the most relevant to the search query `\"SEO services\"`.\n",
        "   - These pages are already performing well for this query, so you should focus on **optimizing** them further. You can do this by:\n",
        "     - **Improving the content**: Make sure the content is comprehensive and answers common questions about SEO services.\n",
        "     - **Enhancing user experience**: Ensure that these pages load quickly, are mobile-friendly, and have a clear structure.\n",
        "\n",
        "2. **Optimize Lower-Ranked Pages**:\n",
        "   - Pages that are ranked lower (like `/regex-google-search-console/` or `/gephi-report-the-definitive-guide/`) are less relevant for the query `\"SEO services\"`.\n",
        "   - If you want these pages to perform better for SEO service-related queries, consider **adjusting the content** to make it more relevant. For example:\n",
        "     - **Add more relevant content** about SEO services.\n",
        "     - **Link these pages** to more relevant pages on your site that discuss SEO services.\n",
        "\n",
        "3. **Identify Content Gaps**:\n",
        "   - Look at the pages that are **not appearing** in the top results but are important for the keyword `\"SEO services\"`. If you have key SEO service pages that aren’t in the top results, it might mean that the content on those pages needs improvement to be more relevant.\n",
        "   - For example, if you have a page called `/affordable-seo-services/` that is not in the top results, you might want to review the content on that page and make it more comprehensive.\n",
        "\n",
        "4. **SEO Strategy Recommendations for Your Client**:\n",
        "   - **Content optimization**: Tell your client to focus on optimizing the top pages further (those ranked higher), while updating or repurposing the lower-ranked pages.\n",
        "   - **Keyword optimization**: Ensure that the **target keyword** (in this case, “SEO services”) is naturally included in the title, headings, and body content of the top pages.\n",
        "   - **Internal linking**: Add internal links from less relevant pages (lower-ranked pages) to more relevant pages. This helps pass **SEO value** from one page to another.\n",
        "\n",
        "### **What Should You Tell Your Client?**\n",
        "\n",
        "1. **Identify the most important pages**:\n",
        "   - You should tell your client that the pages like `/services/` and `/advanced-seo-services/` are currently the most relevant to users searching for `\"SEO services\"`. These pages should be prioritized for further SEO optimization.\n",
        "\n",
        "2. **Improve lower-ranking pages**:\n",
        "   - Pages that rank lower (like `/gephi-report-the-definitive-guide/`) may need **content adjustments** to make them more relevant for SEO service-related keywords. This could involve adding content about SEO services or updating existing content.\n",
        "\n",
        "3. **Increase keyword relevance**:\n",
        "   - Advise your client to ensure that the keyword `\"SEO services\"` (and related keywords like “best SEO services” or “affordable SEO services”) appear in the **title**, **headings**, and **meta descriptions** of these top-ranked pages.\n",
        "\n",
        "4. **Monitor and adjust SEO strategy**:\n",
        "   - Tell your client that this output should be used as part of an ongoing SEO strategy. By regularly checking which pages are relevant to important keywords, your client can continually update and improve their website content.\n",
        "\n",
        "---\n",
        "\n",
        "### Summary:\n",
        "\n",
        "- The output provides a **ranking of web pages** from your website, starting with the most relevant pages for the query `\"SEO services\"`.\n",
        "- The top results (like `/services/` and `/advanced-seo-services/`) are performing well for this keyword, so they should be optimized further to maintain or improve their ranking.\n",
        "- Lower-ranked pages (like `/gephi-report-the-definitive-guide/`) may not be as relevant to the query, so they either need content adjustments or should focus on different keywords.\n",
        "- The key takeaway for your client is to **focus on improving the content of the most relevant pages**, while also identifying opportunities to optimize lower-ranked pages for better performance in search results.\n",
        "\n"
      ],
      "metadata": {
        "id": "GTUkl1jKSup5"
      }
    }
  ]
}